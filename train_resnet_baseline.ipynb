{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce53351b-477a-4285-9137-cd65a347c009",
   "metadata": {},
   "source": [
    "# 图像预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c81a94c-a2e9-4770-a28f-90e50d10a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 5 classes: ['./data\\\\T0', './data\\\\T1', './data\\\\T2', './data\\\\T3', './data\\\\Tis']\n",
      "T0\n",
      "len(files): 200\n",
      "boundary: 10\n",
      "T1\n",
      "len(files): 76\n",
      "boundary: 3\n",
      "T2\n",
      "len(files): 87\n",
      "boundary: 4\n",
      "T3\n",
      "len(files): 148\n",
      "boundary: 7\n",
      "Tis\n",
      "len(files): 99\n",
      "boundary: 4\n",
      "Totally 577 files for training\n",
      "Totally 33 files for val\n"
     ]
    }
   ],
   "source": [
    "!python resnet_baseline/split_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50a0a7-99fc-499c-9014-2c57a59f9fa3",
   "metadata": {},
   "source": [
    "# 统计训练集均值和标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3dbb670-a7c3-4346-bd65-e44dae621839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 577 files for training\n",
      "(577, 128, 128, 3)\n",
      "[0.4304976  0.38631701 0.42988439]\n",
      "[0.42719202 0.4007811  0.42732545]\n"
     ]
    }
   ],
   "source": [
    "!python resnet_baseline/statistic_mean_std.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7585e39-8d4a-46d6-9cba-e56060bb3edc",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a25cc2-075b-4d24-917f-48b0216fa74c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mode...\n",
      "train transform\n",
      "finding classes from ./train:\t['T0', 'T1', 'T2', 'T3', 'Tis']\n",
      "mapping classes from ./train to indexes:\t{'T0': 0, 'T1': 1, 'T2': 2, 'T3': 3, 'Tis': 4}\n",
      "eval transform\n",
      "finding classes from ./val:\t['T0', 'T1', 'T2', 'T3', 'Tis']\n",
      "mapping classes from ./val to indexes:\t{'T0': 0, 'T1': 1, 'T2': 2, 'T3': 3, 'Tis': 4}\n",
      "number of params (M): 11.18\n",
      "Epoch 0\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:44 loss: 2.2942 (2.2942) acc1: 0.0000 (0.0000) acc5: 100.0000 (100.0000) time: 4.9609 data: 0.0250 max mem: 56\n",
      "Test: [8/9] eta: 0:00:00 loss: 1.7591 (1.7283) acc1: 0.0000 (15.1515) acc5: 100.0000 (100.0000) time: 0.5649 data: 0.0127 max mem: 57\n",
      "Test: Total time: 0:00:05 (0.5651 s / it)\n",
      "* Acc@1 15.152 Acc@5 100.000 loss 1.728\n",
      "Accuracy of the network on the 33 test image: 15.2%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 1, Step: 0, Loss: 1.4423246383666992, Lr:0.0001\n",
      "Epoch 1, Step: 1, Loss: 1.432084321975708, Lr:0.0001\n",
      "Epoch 1, Step: 2, Loss: 2.090200424194336, Lr:0.0001\n",
      "Epoch 1, Step: 3, Loss: 1.3993043899536133, Lr:0.0001\n",
      "Epoch 1, Step: 4, Loss: 2.3926401138305664, Lr:0.0001\n",
      "Epoch 1, Step: 5, Loss: 1.9740655422210693, Lr:0.0001\n",
      "Epoch 1, Step: 6, Loss: 1.8284060955047607, Lr:0.0001\n",
      "Epoch 1, Step: 7, Loss: 1.5720043182373047, Lr:0.0001\n",
      "Epoch 1, Step: 8, Loss: 2.424863338470459, Lr:0.0001\n",
      "Epoch 1, Step: 9, Loss: 1.441664695739746, Lr:0.0001\n",
      "Epoch 1, Step: 10, Loss: 1.582291841506958, Lr:0.0001\n",
      "Epoch 1, Step: 11, Loss: 0.9333653450012207, Lr:0.0001\n",
      "Epoch 1, Step: 12, Loss: 1.1977038383483887, Lr:0.0001\n",
      "Epoch 1, Step: 13, Loss: 1.5885672569274902, Lr:0.0001\n",
      "Epoch 1, Step: 14, Loss: 1.162635087966919, Lr:0.0001\n",
      "Epoch 1, Step: 15, Loss: 1.5413471460342407, Lr:0.0001\n",
      "Epoch 1, Step: 16, Loss: 1.1630632877349854, Lr:0.0001\n",
      "Epoch 1, Step: 17, Loss: 1.5052670240402222, Lr:0.0001\n",
      "Epoch 1, Step: 18, Loss: 1.59898042678833, Lr:0.0001\n",
      "Epoch 1, Step: 19, Loss: 1.8373899459838867, Lr:0.0001\n",
      "Epoch 1, Step: 20, Loss: 1.0530301332473755, Lr:0.0001\n",
      "Epoch 1, Step: 21, Loss: 1.2266167402267456, Lr:0.0001\n",
      "Epoch 1, Step: 22, Loss: 1.4300261735916138, Lr:0.0001\n",
      "Epoch 1, Step: 23, Loss: 0.7758579254150391, Lr:0.0001\n",
      "Epoch 1, Step: 24, Loss: 1.4822903871536255, Lr:0.0001\n",
      "Epoch 1, Step: 25, Loss: 2.05745530128479, Lr:0.0001\n",
      "Epoch 1, Step: 26, Loss: 1.1672927141189575, Lr:0.0001\n",
      "Epoch 1, Step: 27, Loss: 1.4984920024871826, Lr:0.0001\n",
      "Epoch 1, Step: 28, Loss: 1.7380411624908447, Lr:0.0001\n",
      "Epoch 1, Step: 29, Loss: 1.0934542417526245, Lr:0.0001\n",
      "Epoch 1, Step: 30, Loss: 2.7357444763183594, Lr:0.0001\n",
      "Epoch 1, Step: 31, Loss: 1.7511155605316162, Lr:0.0001\n",
      "Epoch 1, Step: 32, Loss: 1.4690165519714355, Lr:0.0001\n",
      "Epoch 1, Step: 33, Loss: 1.407130241394043, Lr:0.0001\n",
      "Epoch 1, Step: 34, Loss: 1.2906652688980103, Lr:0.0001\n",
      "Epoch 1, Step: 35, Loss: 1.8304818868637085, Lr:0.0001\n",
      "Epoch 1, Step: 36, Loss: 1.1247543096542358, Lr:0.0001\n",
      "Epoch 1, Step: 37, Loss: 1.5698564052581787, Lr:0.0001\n",
      "Epoch 1, Step: 38, Loss: 1.1282579898834229, Lr:0.0001\n",
      "Epoch 1, Step: 39, Loss: 1.2936376333236694, Lr:0.0001\n",
      "Epoch 1, Step: 40, Loss: 1.4772067070007324, Lr:0.0001\n",
      "Epoch 1, Step: 41, Loss: 1.5110918283462524, Lr:0.0001\n",
      "Epoch 1, Step: 42, Loss: 1.5845584869384766, Lr:0.0001\n",
      "Epoch 1, Step: 43, Loss: 2.218812942504883, Lr:0.0001\n",
      "Epoch 1, Step: 44, Loss: 2.3751237392425537, Lr:0.0001\n",
      "Epoch 1, Step: 45, Loss: 1.0518674850463867, Lr:0.0001\n",
      "Epoch 1, Step: 46, Loss: 1.3757867813110352, Lr:0.0001\n",
      "Epoch 1, Step: 47, Loss: 0.9830523729324341, Lr:0.0001\n",
      "Epoch 1, Step: 48, Loss: 1.4897377490997314, Lr:0.0001\n",
      "Epoch 1, Step: 49, Loss: 0.807290256023407, Lr:0.0001\n",
      "Epoch 1, Step: 50, Loss: 0.5966272354125977, Lr:0.0001\n",
      "Epoch 1, Step: 51, Loss: 1.756897211074829, Lr:0.0001\n",
      "Epoch 1, Step: 52, Loss: 1.5513079166412354, Lr:0.0001\n",
      "Epoch 1, Step: 53, Loss: 1.799870252609253, Lr:0.0001\n",
      "Epoch 1, Step: 54, Loss: 0.8789302110671997, Lr:0.0001\n",
      "Epoch 1, Step: 55, Loss: 2.1569151878356934, Lr:0.0001\n",
      "Epoch 1, Step: 56, Loss: 1.022117018699646, Lr:0.0001\n",
      "Epoch 1, Step: 57, Loss: 1.8473225831985474, Lr:0.0001\n",
      "Epoch 1, Step: 58, Loss: 1.7919477224349976, Lr:0.0001\n",
      "Epoch 1, Step: 59, Loss: 1.6896615028381348, Lr:0.0001\n",
      "Epoch 1, Step: 60, Loss: 1.4318749904632568, Lr:0.0001\n",
      "Epoch 1, Step: 61, Loss: 1.0550179481506348, Lr:0.0001\n",
      "Epoch 1, Step: 62, Loss: 1.3560587167739868, Lr:0.0001\n",
      "Epoch 1, Step: 63, Loss: 1.0796360969543457, Lr:0.0001\n",
      "Epoch 1, Step: 64, Loss: 1.7325268983840942, Lr:0.0001\n",
      "Epoch 1, Step: 65, Loss: 3.222505569458008, Lr:0.0001\n",
      "Epoch 1, Step: 66, Loss: 1.4675995111465454, Lr:0.0001\n",
      "Epoch 1, Step: 67, Loss: 2.132652997970581, Lr:0.0001\n",
      "Epoch 1, Step: 68, Loss: 0.9277942776679993, Lr:0.0001\n",
      "Epoch 1, Step: 69, Loss: 1.260951280593872, Lr:0.0001\n",
      "Epoch 1, Step: 70, Loss: 1.2927711009979248, Lr:0.0001\n",
      "Epoch 1, Step: 71, Loss: 1.4277350902557373, Lr:0.0001\n",
      "Epoch 1, Step: 72, Loss: 0.7817714810371399, Lr:0.0001\n",
      "Epoch 1, Step: 73, Loss: 1.2947113513946533, Lr:0.0001\n",
      "Epoch 1, Step: 74, Loss: 1.2815042734146118, Lr:0.0001\n",
      "Epoch 1, Step: 75, Loss: 1.0405185222625732, Lr:0.0001\n",
      "Epoch 1, Step: 76, Loss: 2.2153244018554688, Lr:0.0001\n",
      "Epoch 1, Step: 77, Loss: 0.7778359055519104, Lr:0.0001\n",
      "Epoch 1, Step: 78, Loss: 1.1061874628067017, Lr:0.0001\n",
      "Epoch 1, Step: 79, Loss: 2.224287509918213, Lr:0.0001\n",
      "Epoch 1, Step: 80, Loss: 0.9820873737335205, Lr:0.0001\n",
      "Epoch 1, Step: 81, Loss: 0.8046781420707703, Lr:0.0001\n",
      "Epoch 1, Step: 82, Loss: 1.465573787689209, Lr:0.0001\n",
      "Epoch 1, Step: 83, Loss: 1.593477725982666, Lr:0.0001\n",
      "Epoch 1, Step: 84, Loss: 0.38066959381103516, Lr:0.0001\n",
      "Epoch 1, Step: 85, Loss: 1.6988720893859863, Lr:0.0001\n",
      "Epoch 1, Step: 86, Loss: 1.2772228717803955, Lr:0.0001\n",
      "Epoch 1, Step: 87, Loss: 1.5299125909805298, Lr:0.0001\n",
      "Epoch 1, Step: 88, Loss: 2.8875973224639893, Lr:0.0001\n",
      "Epoch 1, Step: 89, Loss: 1.2951080799102783, Lr:0.0001\n",
      "Epoch 1, Step: 90, Loss: 1.69044029712677, Lr:0.0001\n",
      "Epoch 1, Step: 91, Loss: 2.274418592453003, Lr:0.0001\n",
      "Epoch 1, Step: 92, Loss: 0.8161870837211609, Lr:0.0001\n",
      "Epoch 1, Step: 93, Loss: 1.606439471244812, Lr:0.0001\n",
      "Epoch 1, Step: 94, Loss: 1.5079460144042969, Lr:0.0001\n",
      "Epoch 1, Step: 95, Loss: 1.2821444272994995, Lr:0.0001\n",
      "Epoch 1, Step: 96, Loss: 1.9437764883041382, Lr:0.0001\n",
      "Epoch 1, Step: 97, Loss: 1.5257312059402466, Lr:0.0001\n",
      "Epoch 1, Step: 98, Loss: 0.8660935163497925, Lr:0.0001\n",
      "Epoch 1, Step: 99, Loss: 1.6530840396881104, Lr:0.0001\n",
      "Epoch 1, Step: 100, Loss: 1.4513018131256104, Lr:0.0001\n",
      "Epoch 1, Step: 101, Loss: 1.195123314857483, Lr:0.0001\n",
      "Epoch 1, Step: 102, Loss: 1.7715364694595337, Lr:0.0001\n",
      "Epoch 1, Step: 103, Loss: 1.2689461708068848, Lr:0.0001\n",
      "Epoch 1, Step: 104, Loss: 0.5306576490402222, Lr:0.0001\n",
      "Epoch 1, Step: 105, Loss: 0.8780874013900757, Lr:0.0001\n",
      "Epoch 1, Step: 106, Loss: 1.5368750095367432, Lr:0.0001\n",
      "Epoch 1, Step: 107, Loss: 2.2300264835357666, Lr:0.0001\n",
      "Epoch 1, Step: 108, Loss: 0.5094349980354309, Lr:0.0001\n",
      "Epoch 1, Step: 109, Loss: 1.7393298149108887, Lr:0.0001\n",
      "Epoch 1, Step: 110, Loss: 1.4878432750701904, Lr:0.0001\n",
      "Epoch 1, Step: 111, Loss: 1.5505505800247192, Lr:0.0001\n",
      "Epoch 1, Step: 112, Loss: 0.8042151927947998, Lr:0.0001\n",
      "Epoch 1, Step: 113, Loss: 1.6345829963684082, Lr:0.0001\n",
      "Epoch 1, Step: 114, Loss: 1.4151146411895752, Lr:0.0001\n",
      "Epoch 1, Step: 115, Loss: 1.3724509477615356, Lr:0.0001\n",
      "Epoch 1, Step: 116, Loss: 1.4205398559570312, Lr:0.0001\n",
      "Epoch 1, Step: 117, Loss: 1.5429288148880005, Lr:0.0001\n",
      "Epoch 1, Step: 118, Loss: 0.47315895557403564, Lr:0.0001\n",
      "Epoch 1, Step: 119, Loss: 1.4434523582458496, Lr:0.0001\n",
      "Epoch 1, Step: 120, Loss: 1.1534397602081299, Lr:0.0001\n",
      "Epoch 1, Step: 121, Loss: 1.04985511302948, Lr:0.0001\n",
      "Epoch 1, Step: 122, Loss: 2.4920928478240967, Lr:0.0001\n",
      "Epoch 1, Step: 123, Loss: 1.3145931959152222, Lr:0.0001\n",
      "Epoch 1, Step: 124, Loss: 0.9793301224708557, Lr:0.0001\n",
      "Epoch 1, Step: 125, Loss: 1.388728141784668, Lr:0.0001\n",
      "Epoch 1, Step: 126, Loss: 1.8488316535949707, Lr:0.0001\n",
      "Epoch 1, Step: 127, Loss: 1.1804567575454712, Lr:0.0001\n",
      "Epoch 1, Step: 128, Loss: 1.6591520309448242, Lr:0.0001\n",
      "Epoch 1, Step: 129, Loss: 0.882695198059082, Lr:0.0001\n",
      "Epoch 1, Step: 130, Loss: 1.2764192819595337, Lr:0.0001\n",
      "Epoch 1, Step: 131, Loss: 0.7468971610069275, Lr:0.0001\n",
      "Epoch 1, Step: 132, Loss: 1.5256171226501465, Lr:0.0001\n",
      "Epoch 1, Step: 133, Loss: 2.24470853805542, Lr:0.0001\n",
      "Epoch 1, Step: 134, Loss: 1.8828175067901611, Lr:0.0001\n",
      "Epoch 1, Step: 135, Loss: 0.9440097808837891, Lr:0.0001\n",
      "Epoch 1, Step: 136, Loss: 2.191232204437256, Lr:0.0001\n",
      "Epoch 1, Step: 137, Loss: 0.7782497406005859, Lr:0.0001\n",
      "Epoch 1, Step: 138, Loss: 1.1936931610107422, Lr:0.0001\n",
      "Epoch 1, Step: 139, Loss: 1.8455448150634766, Lr:0.0001\n",
      "Epoch 1, Step: 140, Loss: 1.5342042446136475, Lr:0.0001\n",
      "Epoch 1, Step: 141, Loss: 1.9506027698516846, Lr:0.0001\n",
      "Epoch 1, Step: 142, Loss: 1.244307041168213, Lr:0.0001\n",
      "Epoch 1, Step: 143, Loss: 1.1672776937484741, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 1\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.6420 (0.6420) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0090 data: 0.0050 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.8506 (1.2770) acc1: 75.0000 (54.5455) acc5: 100.0000 (100.0000) time: 0.0068 data: 0.0033 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0068 s / it)\n",
      "* Acc@1 54.545 Acc@5 100.000 loss 1.277\n",
      "Accuracy of the network on the 33 test image: 54.5%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 2, Step: 0, Loss: 1.9899752140045166, Lr:0.0001\n",
      "Epoch 2, Step: 1, Loss: 1.254375696182251, Lr:0.0001\n",
      "Epoch 2, Step: 2, Loss: 1.162858486175537, Lr:0.0001\n",
      "Epoch 2, Step: 3, Loss: 1.2828569412231445, Lr:0.0001\n",
      "Epoch 2, Step: 4, Loss: 1.2412742376327515, Lr:0.0001\n",
      "Epoch 2, Step: 5, Loss: 1.8739690780639648, Lr:0.0001\n",
      "Epoch 2, Step: 6, Loss: 1.4893231391906738, Lr:0.0001\n",
      "Epoch 2, Step: 7, Loss: 0.59363853931427, Lr:0.0001\n",
      "Epoch 2, Step: 8, Loss: 1.3709014654159546, Lr:0.0001\n",
      "Epoch 2, Step: 9, Loss: 1.109494924545288, Lr:0.0001\n",
      "Epoch 2, Step: 10, Loss: 0.5755321979522705, Lr:0.0001\n",
      "Epoch 2, Step: 11, Loss: 1.3705158233642578, Lr:0.0001\n",
      "Epoch 2, Step: 12, Loss: 0.8232234716415405, Lr:0.0001\n",
      "Epoch 2, Step: 13, Loss: 1.2992734909057617, Lr:0.0001\n",
      "Epoch 2, Step: 14, Loss: 1.8622909784317017, Lr:0.0001\n",
      "Epoch 2, Step: 15, Loss: 1.7083392143249512, Lr:0.0001\n",
      "Epoch 2, Step: 16, Loss: 2.0444726943969727, Lr:0.0001\n",
      "Epoch 2, Step: 17, Loss: 2.105515718460083, Lr:0.0001\n",
      "Epoch 2, Step: 18, Loss: 1.7529590129852295, Lr:0.0001\n",
      "Epoch 2, Step: 19, Loss: 1.4255414009094238, Lr:0.0001\n",
      "Epoch 2, Step: 20, Loss: 0.9823141098022461, Lr:0.0001\n",
      "Epoch 2, Step: 21, Loss: 2.115992307662964, Lr:0.0001\n",
      "Epoch 2, Step: 22, Loss: 0.6204318404197693, Lr:0.0001\n",
      "Epoch 2, Step: 23, Loss: 0.8549929857254028, Lr:0.0001\n",
      "Epoch 2, Step: 24, Loss: 0.8414432406425476, Lr:0.0001\n",
      "Epoch 2, Step: 25, Loss: 0.9295201301574707, Lr:0.0001\n",
      "Epoch 2, Step: 26, Loss: 1.1219393014907837, Lr:0.0001\n",
      "Epoch 2, Step: 27, Loss: 0.8860081434249878, Lr:0.0001\n",
      "Epoch 2, Step: 28, Loss: 1.0767842531204224, Lr:0.0001\n",
      "Epoch 2, Step: 29, Loss: 1.5250067710876465, Lr:0.0001\n",
      "Epoch 2, Step: 30, Loss: 1.2086293697357178, Lr:0.0001\n",
      "Epoch 2, Step: 31, Loss: 0.746878981590271, Lr:0.0001\n",
      "Epoch 2, Step: 32, Loss: 1.731776475906372, Lr:0.0001\n",
      "Epoch 2, Step: 33, Loss: 0.8630558848381042, Lr:0.0001\n",
      "Epoch 2, Step: 34, Loss: 1.1678494215011597, Lr:0.0001\n",
      "Epoch 2, Step: 35, Loss: 1.725396752357483, Lr:0.0001\n",
      "Epoch 2, Step: 36, Loss: 2.527334213256836, Lr:0.0001\n",
      "Epoch 2, Step: 37, Loss: 0.8943555951118469, Lr:0.0001\n",
      "Epoch 2, Step: 38, Loss: 1.1136776208877563, Lr:0.0001\n",
      "Epoch 2, Step: 39, Loss: 0.7326526641845703, Lr:0.0001\n",
      "Epoch 2, Step: 40, Loss: 1.1787407398223877, Lr:0.0001\n",
      "Epoch 2, Step: 41, Loss: 1.3669240474700928, Lr:0.0001\n",
      "Epoch 2, Step: 42, Loss: 1.1551650762557983, Lr:0.0001\n",
      "Epoch 2, Step: 43, Loss: 1.6952497959136963, Lr:0.0001\n",
      "Epoch 2, Step: 44, Loss: 2.3700497150421143, Lr:0.0001\n",
      "Epoch 2, Step: 45, Loss: 1.846811056137085, Lr:0.0001\n",
      "Epoch 2, Step: 46, Loss: 0.6006292104721069, Lr:0.0001\n",
      "Epoch 2, Step: 47, Loss: 1.8973020315170288, Lr:0.0001\n",
      "Epoch 2, Step: 48, Loss: 0.8866519331932068, Lr:0.0001\n",
      "Epoch 2, Step: 49, Loss: 2.474681854248047, Lr:0.0001\n",
      "Epoch 2, Step: 50, Loss: 0.9132940173149109, Lr:0.0001\n",
      "Epoch 2, Step: 51, Loss: 1.0634891986846924, Lr:0.0001\n",
      "Epoch 2, Step: 52, Loss: 3.045382261276245, Lr:0.0001\n",
      "Epoch 2, Step: 53, Loss: 1.5751819610595703, Lr:0.0001\n",
      "Epoch 2, Step: 54, Loss: 0.42793238162994385, Lr:0.0001\n",
      "Epoch 2, Step: 55, Loss: 1.1211285591125488, Lr:0.0001\n",
      "Epoch 2, Step: 56, Loss: 1.0161399841308594, Lr:0.0001\n",
      "Epoch 2, Step: 57, Loss: 1.3714720010757446, Lr:0.0001\n",
      "Epoch 2, Step: 58, Loss: 1.4002271890640259, Lr:0.0001\n",
      "Epoch 2, Step: 59, Loss: 0.8103550672531128, Lr:0.0001\n",
      "Epoch 2, Step: 60, Loss: 1.6597965955734253, Lr:0.0001\n",
      "Epoch 2, Step: 61, Loss: 1.8458569049835205, Lr:0.0001\n",
      "Epoch 2, Step: 62, Loss: 0.6932666301727295, Lr:0.0001\n",
      "Epoch 2, Step: 63, Loss: 1.1030921936035156, Lr:0.0001\n",
      "Epoch 2, Step: 64, Loss: 1.714012861251831, Lr:0.0001\n",
      "Epoch 2, Step: 65, Loss: 1.145838975906372, Lr:0.0001\n",
      "Epoch 2, Step: 66, Loss: 1.3073713779449463, Lr:0.0001\n",
      "Epoch 2, Step: 67, Loss: 1.0186264514923096, Lr:0.0001\n",
      "Epoch 2, Step: 68, Loss: 1.5887192487716675, Lr:0.0001\n",
      "Epoch 2, Step: 69, Loss: 1.5998685359954834, Lr:0.0001\n",
      "Epoch 2, Step: 70, Loss: 1.5578722953796387, Lr:0.0001\n",
      "Epoch 2, Step: 71, Loss: 1.2013037204742432, Lr:0.0001\n",
      "Epoch 2, Step: 72, Loss: 0.7672166228294373, Lr:0.0001\n",
      "Epoch 2, Step: 73, Loss: 1.2573803663253784, Lr:0.0001\n",
      "Epoch 2, Step: 74, Loss: 2.34773588180542, Lr:0.0001\n",
      "Epoch 2, Step: 75, Loss: 0.8555545210838318, Lr:0.0001\n",
      "Epoch 2, Step: 76, Loss: 0.9255794882774353, Lr:0.0001\n",
      "Epoch 2, Step: 77, Loss: 1.188068151473999, Lr:0.0001\n",
      "Epoch 2, Step: 78, Loss: 1.2159523963928223, Lr:0.0001\n",
      "Epoch 2, Step: 79, Loss: 1.4824026823043823, Lr:0.0001\n",
      "Epoch 2, Step: 80, Loss: 0.9510265588760376, Lr:0.0001\n",
      "Epoch 2, Step: 81, Loss: 2.498770236968994, Lr:0.0001\n",
      "Epoch 2, Step: 82, Loss: 1.2099770307540894, Lr:0.0001\n",
      "Epoch 2, Step: 83, Loss: 1.9098848104476929, Lr:0.0001\n",
      "Epoch 2, Step: 84, Loss: 1.6767525672912598, Lr:0.0001\n",
      "Epoch 2, Step: 85, Loss: 2.3919525146484375, Lr:0.0001\n",
      "Epoch 2, Step: 86, Loss: 1.2721081972122192, Lr:0.0001\n",
      "Epoch 2, Step: 87, Loss: 1.9788283109664917, Lr:0.0001\n",
      "Epoch 2, Step: 88, Loss: 0.969910740852356, Lr:0.0001\n",
      "Epoch 2, Step: 89, Loss: 1.3835818767547607, Lr:0.0001\n",
      "Epoch 2, Step: 90, Loss: 1.3082499504089355, Lr:0.0001\n",
      "Epoch 2, Step: 91, Loss: 1.3948724269866943, Lr:0.0001\n",
      "Epoch 2, Step: 92, Loss: 0.8699607253074646, Lr:0.0001\n",
      "Epoch 2, Step: 93, Loss: 1.1493818759918213, Lr:0.0001\n",
      "Epoch 2, Step: 94, Loss: 0.46128034591674805, Lr:0.0001\n",
      "Epoch 2, Step: 95, Loss: 0.9411275386810303, Lr:0.0001\n",
      "Epoch 2, Step: 96, Loss: 1.2129818201065063, Lr:0.0001\n",
      "Epoch 2, Step: 97, Loss: 1.2683165073394775, Lr:0.0001\n",
      "Epoch 2, Step: 98, Loss: 1.6920086145401, Lr:0.0001\n",
      "Epoch 2, Step: 99, Loss: 2.782064199447632, Lr:0.0001\n",
      "Epoch 2, Step: 100, Loss: 0.9109272956848145, Lr:0.0001\n",
      "Epoch 2, Step: 101, Loss: 0.6103181838989258, Lr:0.0001\n",
      "Epoch 2, Step: 102, Loss: 0.515617311000824, Lr:0.0001\n",
      "Epoch 2, Step: 103, Loss: 2.1737606525421143, Lr:0.0001\n",
      "Epoch 2, Step: 104, Loss: 1.125811219215393, Lr:0.0001\n",
      "Epoch 2, Step: 105, Loss: 1.665285348892212, Lr:0.0001\n",
      "Epoch 2, Step: 106, Loss: 2.735248327255249, Lr:0.0001\n",
      "Epoch 2, Step: 107, Loss: 0.978903591632843, Lr:0.0001\n",
      "Epoch 2, Step: 108, Loss: 0.8230881690979004, Lr:0.0001\n",
      "Epoch 2, Step: 109, Loss: 1.1229203939437866, Lr:0.0001\n",
      "Epoch 2, Step: 110, Loss: 1.6871349811553955, Lr:0.0001\n",
      "Epoch 2, Step: 111, Loss: 1.475968599319458, Lr:0.0001\n",
      "Epoch 2, Step: 112, Loss: 1.5338449478149414, Lr:0.0001\n",
      "Epoch 2, Step: 113, Loss: 1.098610520362854, Lr:0.0001\n",
      "Epoch 2, Step: 114, Loss: 0.473322331905365, Lr:0.0001\n",
      "Epoch 2, Step: 115, Loss: 1.393363118171692, Lr:0.0001\n",
      "Epoch 2, Step: 116, Loss: 2.3617987632751465, Lr:0.0001\n",
      "Epoch 2, Step: 117, Loss: 0.792779266834259, Lr:0.0001\n",
      "Epoch 2, Step: 118, Loss: 1.3233489990234375, Lr:0.0001\n",
      "Epoch 2, Step: 119, Loss: 1.3979806900024414, Lr:0.0001\n",
      "Epoch 2, Step: 120, Loss: 1.3897687196731567, Lr:0.0001\n",
      "Epoch 2, Step: 121, Loss: 0.6389693021774292, Lr:0.0001\n",
      "Epoch 2, Step: 122, Loss: 1.2399685382843018, Lr:0.0001\n",
      "Epoch 2, Step: 123, Loss: 0.4319669008255005, Lr:0.0001\n",
      "Epoch 2, Step: 124, Loss: 1.9456944465637207, Lr:0.0001\n",
      "Epoch 2, Step: 125, Loss: 1.8251458406448364, Lr:0.0001\n",
      "Epoch 2, Step: 126, Loss: 1.4369052648544312, Lr:0.0001\n",
      "Epoch 2, Step: 127, Loss: 1.5842666625976562, Lr:0.0001\n",
      "Epoch 2, Step: 128, Loss: 2.424612045288086, Lr:0.0001\n",
      "Epoch 2, Step: 129, Loss: 1.3490421772003174, Lr:0.0001\n",
      "Epoch 2, Step: 130, Loss: 1.1267614364624023, Lr:0.0001\n",
      "Epoch 2, Step: 131, Loss: 1.8646166324615479, Lr:0.0001\n",
      "Epoch 2, Step: 132, Loss: 0.8691104650497437, Lr:0.0001\n",
      "Epoch 2, Step: 133, Loss: 1.2616249322891235, Lr:0.0001\n",
      "Epoch 2, Step: 134, Loss: 1.0627915859222412, Lr:0.0001\n",
      "Epoch 2, Step: 135, Loss: 0.5850284695625305, Lr:0.0001\n",
      "Epoch 2, Step: 136, Loss: 1.115708589553833, Lr:0.0001\n",
      "Epoch 2, Step: 137, Loss: 0.6716173887252808, Lr:0.0001\n",
      "Epoch 2, Step: 138, Loss: 1.3082084655761719, Lr:0.0001\n",
      "Epoch 2, Step: 139, Loss: 1.2305057048797607, Lr:0.0001\n",
      "Epoch 2, Step: 140, Loss: 1.462602972984314, Lr:0.0001\n",
      "Epoch 2, Step: 141, Loss: 1.0985721349716187, Lr:0.0001\n",
      "Epoch 2, Step: 142, Loss: 0.9553902745246887, Lr:0.0001\n",
      "Epoch 2, Step: 143, Loss: 1.4193484783172607, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 2\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.3380 (0.3380) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0110 data: 0.0050 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 1.1194 (1.0798) acc1: 50.0000 (60.6061) acc5: 100.0000 (100.0000) time: 0.0074 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0075 s / it)\n",
      "* Acc@1 60.606 Acc@5 100.000 loss 1.080\n",
      "Accuracy of the network on the 33 test image: 60.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 3, Step: 0, Loss: 1.397911787033081, Lr:0.0001\n",
      "Epoch 3, Step: 1, Loss: 1.9558876752853394, Lr:0.0001\n",
      "Epoch 3, Step: 2, Loss: 0.8212085962295532, Lr:0.0001\n",
      "Epoch 3, Step: 3, Loss: 1.2908412218093872, Lr:0.0001\n",
      "Epoch 3, Step: 4, Loss: 0.5265553593635559, Lr:0.0001\n",
      "Epoch 3, Step: 5, Loss: 1.145486831665039, Lr:0.0001\n",
      "Epoch 3, Step: 6, Loss: 1.6849470138549805, Lr:0.0001\n",
      "Epoch 3, Step: 7, Loss: 1.084510087966919, Lr:0.0001\n",
      "Epoch 3, Step: 8, Loss: 1.6356713771820068, Lr:0.0001\n",
      "Epoch 3, Step: 9, Loss: 1.0917621850967407, Lr:0.0001\n",
      "Epoch 3, Step: 10, Loss: 1.2025763988494873, Lr:0.0001\n",
      "Epoch 3, Step: 11, Loss: 0.9784666895866394, Lr:0.0001\n",
      "Epoch 3, Step: 12, Loss: 0.7843506336212158, Lr:0.0001\n",
      "Epoch 3, Step: 13, Loss: 1.720588207244873, Lr:0.0001\n",
      "Epoch 3, Step: 14, Loss: 0.9768025279045105, Lr:0.0001\n",
      "Epoch 3, Step: 15, Loss: 0.9993788003921509, Lr:0.0001\n",
      "Epoch 3, Step: 16, Loss: 1.6651115417480469, Lr:0.0001\n",
      "Epoch 3, Step: 17, Loss: 0.7322396636009216, Lr:0.0001\n",
      "Epoch 3, Step: 18, Loss: 1.1840267181396484, Lr:0.0001\n",
      "Epoch 3, Step: 19, Loss: 1.4713976383209229, Lr:0.0001\n",
      "Epoch 3, Step: 20, Loss: 1.8761382102966309, Lr:0.0001\n",
      "Epoch 3, Step: 21, Loss: 1.6796579360961914, Lr:0.0001\n",
      "Epoch 3, Step: 22, Loss: 1.4254934787750244, Lr:0.0001\n",
      "Epoch 3, Step: 23, Loss: 1.6442103385925293, Lr:0.0001\n",
      "Epoch 3, Step: 24, Loss: 0.6722835302352905, Lr:0.0001\n",
      "Epoch 3, Step: 25, Loss: 1.725914478302002, Lr:0.0001\n",
      "Epoch 3, Step: 26, Loss: 1.029576063156128, Lr:0.0001\n",
      "Epoch 3, Step: 27, Loss: 1.1103968620300293, Lr:0.0001\n",
      "Epoch 3, Step: 28, Loss: 0.9263572692871094, Lr:0.0001\n",
      "Epoch 3, Step: 29, Loss: 1.258747935295105, Lr:0.0001\n",
      "Epoch 3, Step: 30, Loss: 1.0048282146453857, Lr:0.0001\n",
      "Epoch 3, Step: 31, Loss: 0.5451710820198059, Lr:0.0001\n",
      "Epoch 3, Step: 32, Loss: 1.101306676864624, Lr:0.0001\n",
      "Epoch 3, Step: 33, Loss: 2.5544047355651855, Lr:0.0001\n",
      "Epoch 3, Step: 34, Loss: 0.9647693037986755, Lr:0.0001\n",
      "Epoch 3, Step: 35, Loss: 1.5359771251678467, Lr:0.0001\n",
      "Epoch 3, Step: 36, Loss: 0.7908771634101868, Lr:0.0001\n",
      "Epoch 3, Step: 37, Loss: 2.3117127418518066, Lr:0.0001\n",
      "Epoch 3, Step: 38, Loss: 2.231604814529419, Lr:0.0001\n",
      "Epoch 3, Step: 39, Loss: 0.7793678045272827, Lr:0.0001\n",
      "Epoch 3, Step: 40, Loss: 0.830016016960144, Lr:0.0001\n",
      "Epoch 3, Step: 41, Loss: 0.9371687173843384, Lr:0.0001\n",
      "Epoch 3, Step: 42, Loss: 0.8702642917633057, Lr:0.0001\n",
      "Epoch 3, Step: 43, Loss: 1.0326708555221558, Lr:0.0001\n",
      "Epoch 3, Step: 44, Loss: 0.7712566256523132, Lr:0.0001\n",
      "Epoch 3, Step: 45, Loss: 2.024853467941284, Lr:0.0001\n",
      "Epoch 3, Step: 46, Loss: 1.9161251783370972, Lr:0.0001\n",
      "Epoch 3, Step: 47, Loss: 0.8968589901924133, Lr:0.0001\n",
      "Epoch 3, Step: 48, Loss: 0.8935729265213013, Lr:0.0001\n",
      "Epoch 3, Step: 49, Loss: 1.289818525314331, Lr:0.0001\n",
      "Epoch 3, Step: 50, Loss: 0.9810691475868225, Lr:0.0001\n",
      "Epoch 3, Step: 51, Loss: 0.6584816575050354, Lr:0.0001\n",
      "Epoch 3, Step: 52, Loss: 0.5746715068817139, Lr:0.0001\n",
      "Epoch 3, Step: 53, Loss: 0.8771817088127136, Lr:0.0001\n",
      "Epoch 3, Step: 54, Loss: 2.8836891651153564, Lr:0.0001\n",
      "Epoch 3, Step: 55, Loss: 1.0577542781829834, Lr:0.0001\n",
      "Epoch 3, Step: 56, Loss: 1.3545913696289062, Lr:0.0001\n",
      "Epoch 3, Step: 57, Loss: 1.1181498765945435, Lr:0.0001\n",
      "Epoch 3, Step: 58, Loss: 1.0935264825820923, Lr:0.0001\n",
      "Epoch 3, Step: 59, Loss: 1.2248218059539795, Lr:0.0001\n",
      "Epoch 3, Step: 60, Loss: 0.4664301574230194, Lr:0.0001\n",
      "Epoch 3, Step: 61, Loss: 0.5825461149215698, Lr:0.0001\n",
      "Epoch 3, Step: 62, Loss: 3.2829720973968506, Lr:0.0001\n",
      "Epoch 3, Step: 63, Loss: 1.0223115682601929, Lr:0.0001\n",
      "Epoch 3, Step: 64, Loss: 1.1914316415786743, Lr:0.0001\n",
      "Epoch 3, Step: 65, Loss: 3.107927083969116, Lr:0.0001\n",
      "Epoch 3, Step: 66, Loss: 2.1600263118743896, Lr:0.0001\n",
      "Epoch 3, Step: 67, Loss: 0.7584667801856995, Lr:0.0001\n",
      "Epoch 3, Step: 68, Loss: 0.3945502042770386, Lr:0.0001\n",
      "Epoch 3, Step: 69, Loss: 0.8049609065055847, Lr:0.0001\n",
      "Epoch 3, Step: 70, Loss: 0.9473274946212769, Lr:0.0001\n",
      "Epoch 3, Step: 71, Loss: 0.9474930763244629, Lr:0.0001\n",
      "Epoch 3, Step: 72, Loss: 2.1422505378723145, Lr:0.0001\n",
      "Epoch 3, Step: 73, Loss: 0.9472682476043701, Lr:0.0001\n",
      "Epoch 3, Step: 74, Loss: 1.5242055654525757, Lr:0.0001\n",
      "Epoch 3, Step: 75, Loss: 2.216256856918335, Lr:0.0001\n",
      "Epoch 3, Step: 76, Loss: 1.5976227521896362, Lr:0.0001\n",
      "Epoch 3, Step: 77, Loss: 0.7527118921279907, Lr:0.0001\n",
      "Epoch 3, Step: 78, Loss: 0.9666016101837158, Lr:0.0001\n",
      "Epoch 3, Step: 79, Loss: 1.690887212753296, Lr:0.0001\n",
      "Epoch 3, Step: 80, Loss: 1.6232550144195557, Lr:0.0001\n",
      "Epoch 3, Step: 81, Loss: 1.33713698387146, Lr:0.0001\n",
      "Epoch 3, Step: 82, Loss: 1.918827772140503, Lr:0.0001\n",
      "Epoch 3, Step: 83, Loss: 1.244688630104065, Lr:0.0001\n",
      "Epoch 3, Step: 84, Loss: 2.2027835845947266, Lr:0.0001\n",
      "Epoch 3, Step: 85, Loss: 0.8146229386329651, Lr:0.0001\n",
      "Epoch 3, Step: 86, Loss: 1.3782767057418823, Lr:0.0001\n",
      "Epoch 3, Step: 87, Loss: 1.154991626739502, Lr:0.0001\n",
      "Epoch 3, Step: 88, Loss: 1.6973470449447632, Lr:0.0001\n",
      "Epoch 3, Step: 89, Loss: 0.5894482135772705, Lr:0.0001\n",
      "Epoch 3, Step: 90, Loss: 1.5980935096740723, Lr:0.0001\n",
      "Epoch 3, Step: 91, Loss: 1.0270203351974487, Lr:0.0001\n",
      "Epoch 3, Step: 92, Loss: 1.0513403415679932, Lr:0.0001\n",
      "Epoch 3, Step: 93, Loss: 0.6975069046020508, Lr:0.0001\n",
      "Epoch 3, Step: 94, Loss: 1.7381693124771118, Lr:0.0001\n",
      "Epoch 3, Step: 95, Loss: 1.5903823375701904, Lr:0.0001\n",
      "Epoch 3, Step: 96, Loss: 1.5018903017044067, Lr:0.0001\n",
      "Epoch 3, Step: 97, Loss: 1.1762897968292236, Lr:0.0001\n",
      "Epoch 3, Step: 98, Loss: 1.182769536972046, Lr:0.0001\n",
      "Epoch 3, Step: 99, Loss: 1.7525535821914673, Lr:0.0001\n",
      "Epoch 3, Step: 100, Loss: 0.5310956835746765, Lr:0.0001\n",
      "Epoch 3, Step: 101, Loss: 1.0680162906646729, Lr:0.0001\n",
      "Epoch 3, Step: 102, Loss: 2.2711102962493896, Lr:0.0001\n",
      "Epoch 3, Step: 103, Loss: 1.8782439231872559, Lr:0.0001\n",
      "Epoch 3, Step: 104, Loss: 1.8337444067001343, Lr:0.0001\n",
      "Epoch 3, Step: 105, Loss: 0.8981575965881348, Lr:0.0001\n",
      "Epoch 3, Step: 106, Loss: 1.1058794260025024, Lr:0.0001\n",
      "Epoch 3, Step: 107, Loss: 1.9201253652572632, Lr:0.0001\n",
      "Epoch 3, Step: 108, Loss: 2.103693962097168, Lr:0.0001\n",
      "Epoch 3, Step: 109, Loss: 1.8037282228469849, Lr:0.0001\n",
      "Epoch 3, Step: 110, Loss: 0.5128111243247986, Lr:0.0001\n",
      "Epoch 3, Step: 111, Loss: 1.1827114820480347, Lr:0.0001\n",
      "Epoch 3, Step: 112, Loss: 0.371019572019577, Lr:0.0001\n",
      "Epoch 3, Step: 113, Loss: 0.7108396291732788, Lr:0.0001\n",
      "Epoch 3, Step: 114, Loss: 0.9709547162055969, Lr:0.0001\n",
      "Epoch 3, Step: 115, Loss: 1.0092899799346924, Lr:0.0001\n",
      "Epoch 3, Step: 116, Loss: 1.903564214706421, Lr:0.0001\n",
      "Epoch 3, Step: 117, Loss: 2.0583624839782715, Lr:0.0001\n",
      "Epoch 3, Step: 118, Loss: 0.44831520318984985, Lr:0.0001\n",
      "Epoch 3, Step: 119, Loss: 0.8017874956130981, Lr:0.0001\n",
      "Epoch 3, Step: 120, Loss: 0.9411875605583191, Lr:0.0001\n",
      "Epoch 3, Step: 121, Loss: 1.448613166809082, Lr:0.0001\n",
      "Epoch 3, Step: 122, Loss: 0.38419145345687866, Lr:0.0001\n",
      "Epoch 3, Step: 123, Loss: 1.778631567955017, Lr:0.0001\n",
      "Epoch 3, Step: 124, Loss: 0.9982048273086548, Lr:0.0001\n",
      "Epoch 3, Step: 125, Loss: 1.6749913692474365, Lr:0.0001\n",
      "Epoch 3, Step: 126, Loss: 1.7954033613204956, Lr:0.0001\n",
      "Epoch 3, Step: 127, Loss: 1.3523305654525757, Lr:0.0001\n",
      "Epoch 3, Step: 128, Loss: 1.450108289718628, Lr:0.0001\n",
      "Epoch 3, Step: 129, Loss: 1.9371700286865234, Lr:0.0001\n",
      "Epoch 3, Step: 130, Loss: 1.8031041622161865, Lr:0.0001\n",
      "Epoch 3, Step: 131, Loss: 1.2598581314086914, Lr:0.0001\n",
      "Epoch 3, Step: 132, Loss: 1.2940075397491455, Lr:0.0001\n",
      "Epoch 3, Step: 133, Loss: 1.4715455770492554, Lr:0.0001\n",
      "Epoch 3, Step: 134, Loss: 0.647490382194519, Lr:0.0001\n",
      "Epoch 3, Step: 135, Loss: 2.517576217651367, Lr:0.0001\n",
      "Epoch 3, Step: 136, Loss: 1.5648808479309082, Lr:0.0001\n",
      "Epoch 3, Step: 137, Loss: 0.8131842613220215, Lr:0.0001\n",
      "Epoch 3, Step: 138, Loss: 0.7799826860427856, Lr:0.0001\n",
      "Epoch 3, Step: 139, Loss: 2.6101174354553223, Lr:0.0001\n",
      "Epoch 3, Step: 140, Loss: 0.6187801361083984, Lr:0.0001\n",
      "Epoch 3, Step: 141, Loss: 1.425924301147461, Lr:0.0001\n",
      "Epoch 3, Step: 142, Loss: 0.9965755939483643, Lr:0.0001\n",
      "Epoch 3, Step: 143, Loss: 1.6366277933120728, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 3\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0830 (0.0830) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0080 data: 0.0040 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7199 (0.8671) acc1: 75.0000 (60.6061) acc5: 100.0000 (100.0000) time: 0.0072 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0072 s / it)\n",
      "* Acc@1 60.606 Acc@5 100.000 loss 0.867\n",
      "Accuracy of the network on the 33 test image: 60.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 4, Step: 0, Loss: 0.8282386064529419, Lr:0.0001\n",
      "Epoch 4, Step: 1, Loss: 1.2122514247894287, Lr:0.0001\n",
      "Epoch 4, Step: 2, Loss: 1.8127186298370361, Lr:0.0001\n",
      "Epoch 4, Step: 3, Loss: 1.2414395809173584, Lr:0.0001\n",
      "Epoch 4, Step: 4, Loss: 1.9290851354599, Lr:0.0001\n",
      "Epoch 4, Step: 5, Loss: 0.7132374048233032, Lr:0.0001\n",
      "Epoch 4, Step: 6, Loss: 0.6390133500099182, Lr:0.0001\n",
      "Epoch 4, Step: 7, Loss: 1.4886976480484009, Lr:0.0001\n",
      "Epoch 4, Step: 8, Loss: 2.239064931869507, Lr:0.0001\n",
      "Epoch 4, Step: 9, Loss: 2.259181499481201, Lr:0.0001\n",
      "Epoch 4, Step: 10, Loss: 1.3580964803695679, Lr:0.0001\n",
      "Epoch 4, Step: 11, Loss: 1.0928452014923096, Lr:0.0001\n",
      "Epoch 4, Step: 12, Loss: 1.3950880765914917, Lr:0.0001\n",
      "Epoch 4, Step: 13, Loss: 1.2679613828659058, Lr:0.0001\n",
      "Epoch 4, Step: 14, Loss: 1.236539363861084, Lr:0.0001\n",
      "Epoch 4, Step: 15, Loss: 0.7303409576416016, Lr:0.0001\n",
      "Epoch 4, Step: 16, Loss: 1.3881731033325195, Lr:0.0001\n",
      "Epoch 4, Step: 17, Loss: 0.7196617722511292, Lr:0.0001\n",
      "Epoch 4, Step: 18, Loss: 0.8844068050384521, Lr:0.0001\n",
      "Epoch 4, Step: 19, Loss: 2.0494256019592285, Lr:0.0001\n",
      "Epoch 4, Step: 20, Loss: 0.993834376335144, Lr:0.0001\n",
      "Epoch 4, Step: 21, Loss: 1.5301718711853027, Lr:0.0001\n",
      "Epoch 4, Step: 22, Loss: 0.7552528977394104, Lr:0.0001\n",
      "Epoch 4, Step: 23, Loss: 0.7416883111000061, Lr:0.0001\n",
      "Epoch 4, Step: 24, Loss: 0.964792788028717, Lr:0.0001\n",
      "Epoch 4, Step: 25, Loss: 1.1258354187011719, Lr:0.0001\n",
      "Epoch 4, Step: 26, Loss: 1.1202449798583984, Lr:0.0001\n",
      "Epoch 4, Step: 27, Loss: 0.9780715107917786, Lr:0.0001\n",
      "Epoch 4, Step: 28, Loss: 1.5145237445831299, Lr:0.0001\n",
      "Epoch 4, Step: 29, Loss: 1.6888444423675537, Lr:0.0001\n",
      "Epoch 4, Step: 30, Loss: 1.013007640838623, Lr:0.0001\n",
      "Epoch 4, Step: 31, Loss: 0.88694167137146, Lr:0.0001\n",
      "Epoch 4, Step: 32, Loss: 0.8016119003295898, Lr:0.0001\n",
      "Epoch 4, Step: 33, Loss: 2.1786837577819824, Lr:0.0001\n",
      "Epoch 4, Step: 34, Loss: 0.6925994157791138, Lr:0.0001\n",
      "Epoch 4, Step: 35, Loss: 0.8406305313110352, Lr:0.0001\n",
      "Epoch 4, Step: 36, Loss: 0.9781114459037781, Lr:0.0001\n",
      "Epoch 4, Step: 37, Loss: 0.9221625328063965, Lr:0.0001\n",
      "Epoch 4, Step: 38, Loss: 0.5370854139328003, Lr:0.0001\n",
      "Epoch 4, Step: 39, Loss: 0.9247874617576599, Lr:0.0001\n",
      "Epoch 4, Step: 40, Loss: 0.9693687558174133, Lr:0.0001\n",
      "Epoch 4, Step: 41, Loss: 1.6058495044708252, Lr:0.0001\n",
      "Epoch 4, Step: 42, Loss: 0.540002703666687, Lr:0.0001\n",
      "Epoch 4, Step: 43, Loss: 1.5284171104431152, Lr:0.0001\n",
      "Epoch 4, Step: 44, Loss: 1.8228706121444702, Lr:0.0001\n",
      "Epoch 4, Step: 45, Loss: 0.9743776917457581, Lr:0.0001\n",
      "Epoch 4, Step: 46, Loss: 1.4662251472473145, Lr:0.0001\n",
      "Epoch 4, Step: 47, Loss: 2.499314308166504, Lr:0.0001\n",
      "Epoch 4, Step: 48, Loss: 0.6188082098960876, Lr:0.0001\n",
      "Epoch 4, Step: 49, Loss: 1.6343035697937012, Lr:0.0001\n",
      "Epoch 4, Step: 50, Loss: 0.9046163558959961, Lr:0.0001\n",
      "Epoch 4, Step: 51, Loss: 1.2715438604354858, Lr:0.0001\n",
      "Epoch 4, Step: 52, Loss: 1.2665765285491943, Lr:0.0001\n",
      "Epoch 4, Step: 53, Loss: 0.5203632116317749, Lr:0.0001\n",
      "Epoch 4, Step: 54, Loss: 1.8781983852386475, Lr:0.0001\n",
      "Epoch 4, Step: 55, Loss: 1.540243148803711, Lr:0.0001\n",
      "Epoch 4, Step: 56, Loss: 0.4874575138092041, Lr:0.0001\n",
      "Epoch 4, Step: 57, Loss: 2.209024429321289, Lr:0.0001\n",
      "Epoch 4, Step: 58, Loss: 1.7890170812606812, Lr:0.0001\n",
      "Epoch 4, Step: 59, Loss: 0.9063149690628052, Lr:0.0001\n",
      "Epoch 4, Step: 60, Loss: 1.4269386529922485, Lr:0.0001\n",
      "Epoch 4, Step: 61, Loss: 0.6989121437072754, Lr:0.0001\n",
      "Epoch 4, Step: 62, Loss: 0.7125881910324097, Lr:0.0001\n",
      "Epoch 4, Step: 63, Loss: 0.7790614366531372, Lr:0.0001\n",
      "Epoch 4, Step: 64, Loss: 0.9454166889190674, Lr:0.0001\n",
      "Epoch 4, Step: 65, Loss: 1.5840879678726196, Lr:0.0001\n",
      "Epoch 4, Step: 66, Loss: 1.547602653503418, Lr:0.0001\n",
      "Epoch 4, Step: 67, Loss: 1.6024181842803955, Lr:0.0001\n",
      "Epoch 4, Step: 68, Loss: 1.2620511054992676, Lr:0.0001\n",
      "Epoch 4, Step: 69, Loss: 1.6690196990966797, Lr:0.0001\n",
      "Epoch 4, Step: 70, Loss: 0.8065941333770752, Lr:0.0001\n",
      "Epoch 4, Step: 71, Loss: 1.019738793373108, Lr:0.0001\n",
      "Epoch 4, Step: 72, Loss: 1.7448441982269287, Lr:0.0001\n",
      "Epoch 4, Step: 73, Loss: 1.2447855472564697, Lr:0.0001\n",
      "Epoch 4, Step: 74, Loss: 1.7774107456207275, Lr:0.0001\n",
      "Epoch 4, Step: 75, Loss: 1.5697622299194336, Lr:0.0001\n",
      "Epoch 4, Step: 76, Loss: 1.2955114841461182, Lr:0.0001\n",
      "Epoch 4, Step: 77, Loss: 0.9226438403129578, Lr:0.0001\n",
      "Epoch 4, Step: 78, Loss: 1.2283943891525269, Lr:0.0001\n",
      "Epoch 4, Step: 79, Loss: 1.2379313707351685, Lr:0.0001\n",
      "Epoch 4, Step: 80, Loss: 1.36688232421875, Lr:0.0001\n",
      "Epoch 4, Step: 81, Loss: 1.4717137813568115, Lr:0.0001\n",
      "Epoch 4, Step: 82, Loss: 0.7320051193237305, Lr:0.0001\n",
      "Epoch 4, Step: 83, Loss: 0.6804207563400269, Lr:0.0001\n",
      "Epoch 4, Step: 84, Loss: 1.193997859954834, Lr:0.0001\n",
      "Epoch 4, Step: 85, Loss: 2.248727798461914, Lr:0.0001\n",
      "Epoch 4, Step: 86, Loss: 1.916223168373108, Lr:0.0001\n",
      "Epoch 4, Step: 87, Loss: 1.2036969661712646, Lr:0.0001\n",
      "Epoch 4, Step: 88, Loss: 1.4585829973220825, Lr:0.0001\n",
      "Epoch 4, Step: 89, Loss: 1.4700653553009033, Lr:0.0001\n",
      "Epoch 4, Step: 90, Loss: 0.9991216063499451, Lr:0.0001\n",
      "Epoch 4, Step: 91, Loss: 1.114058256149292, Lr:0.0001\n",
      "Epoch 4, Step: 92, Loss: 1.087465763092041, Lr:0.0001\n",
      "Epoch 4, Step: 93, Loss: 2.0812489986419678, Lr:0.0001\n",
      "Epoch 4, Step: 94, Loss: 1.0727863311767578, Lr:0.0001\n",
      "Epoch 4, Step: 95, Loss: 0.48748666048049927, Lr:0.0001\n",
      "Epoch 4, Step: 96, Loss: 0.5001299381256104, Lr:0.0001\n",
      "Epoch 4, Step: 97, Loss: 0.7240141034126282, Lr:0.0001\n",
      "Epoch 4, Step: 98, Loss: 1.0406054258346558, Lr:0.0001\n",
      "Epoch 4, Step: 99, Loss: 0.9735197424888611, Lr:0.0001\n",
      "Epoch 4, Step: 100, Loss: 2.066274642944336, Lr:0.0001\n",
      "Epoch 4, Step: 101, Loss: 1.67571222782135, Lr:0.0001\n",
      "Epoch 4, Step: 102, Loss: 1.7962393760681152, Lr:0.0001\n",
      "Epoch 4, Step: 103, Loss: 2.1431713104248047, Lr:0.0001\n",
      "Epoch 4, Step: 104, Loss: 0.5148157477378845, Lr:0.0001\n",
      "Epoch 4, Step: 105, Loss: 1.2571531534194946, Lr:0.0001\n",
      "Epoch 4, Step: 106, Loss: 1.3169581890106201, Lr:0.0001\n",
      "Epoch 4, Step: 107, Loss: 1.3854217529296875, Lr:0.0001\n",
      "Epoch 4, Step: 108, Loss: 1.5846669673919678, Lr:0.0001\n",
      "Epoch 4, Step: 109, Loss: 0.877936840057373, Lr:0.0001\n",
      "Epoch 4, Step: 110, Loss: 1.1170599460601807, Lr:0.0001\n",
      "Epoch 4, Step: 111, Loss: 1.917478322982788, Lr:0.0001\n",
      "Epoch 4, Step: 112, Loss: 1.2347397804260254, Lr:0.0001\n",
      "Epoch 4, Step: 113, Loss: 1.6469335556030273, Lr:0.0001\n",
      "Epoch 4, Step: 114, Loss: 0.9723083972930908, Lr:0.0001\n",
      "Epoch 4, Step: 115, Loss: 1.1938706636428833, Lr:0.0001\n",
      "Epoch 4, Step: 116, Loss: 0.4444395899772644, Lr:0.0001\n",
      "Epoch 4, Step: 117, Loss: 1.5383727550506592, Lr:0.0001\n",
      "Epoch 4, Step: 118, Loss: 0.3459928333759308, Lr:0.0001\n",
      "Epoch 4, Step: 119, Loss: 2.1920578479766846, Lr:0.0001\n",
      "Epoch 4, Step: 120, Loss: 0.44576650857925415, Lr:0.0001\n",
      "Epoch 4, Step: 121, Loss: 0.8925566077232361, Lr:0.0001\n",
      "Epoch 4, Step: 122, Loss: 1.9345879554748535, Lr:0.0001\n",
      "Epoch 4, Step: 123, Loss: 1.5185139179229736, Lr:0.0001\n",
      "Epoch 4, Step: 124, Loss: 0.977544903755188, Lr:0.0001\n",
      "Epoch 4, Step: 125, Loss: 1.219736099243164, Lr:0.0001\n",
      "Epoch 4, Step: 126, Loss: 1.8483340740203857, Lr:0.0001\n",
      "Epoch 4, Step: 127, Loss: 2.118767261505127, Lr:0.0001\n",
      "Epoch 4, Step: 128, Loss: 1.7185633182525635, Lr:0.0001\n",
      "Epoch 4, Step: 129, Loss: 1.7072910070419312, Lr:0.0001\n",
      "Epoch 4, Step: 130, Loss: 0.7334424257278442, Lr:0.0001\n",
      "Epoch 4, Step: 131, Loss: 0.9787799715995789, Lr:0.0001\n",
      "Epoch 4, Step: 132, Loss: 0.2883123755455017, Lr:0.0001\n",
      "Epoch 4, Step: 133, Loss: 1.3842604160308838, Lr:0.0001\n",
      "Epoch 4, Step: 134, Loss: 1.1516106128692627, Lr:0.0001\n",
      "Epoch 4, Step: 135, Loss: 0.7542719841003418, Lr:0.0001\n",
      "Epoch 4, Step: 136, Loss: 1.396249771118164, Lr:0.0001\n",
      "Epoch 4, Step: 137, Loss: 0.646986722946167, Lr:0.0001\n",
      "Epoch 4, Step: 138, Loss: 0.8419409990310669, Lr:0.0001\n",
      "Epoch 4, Step: 139, Loss: 1.5860676765441895, Lr:0.0001\n",
      "Epoch 4, Step: 140, Loss: 0.35642868280410767, Lr:0.0001\n",
      "Epoch 4, Step: 141, Loss: 1.4832993745803833, Lr:0.0001\n",
      "Epoch 4, Step: 142, Loss: 1.0636208057403564, Lr:0.0001\n",
      "Epoch 4, Step: 143, Loss: 1.0485706329345703, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 4\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1099 (0.1099) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0080 data: 0.0050 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 1.0584 (0.9056) acc1: 75.0000 (60.6061) acc5: 100.0000 (100.0000) time: 0.0068 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0069 s / it)\n",
      "* Acc@1 60.606 Acc@5 100.000 loss 0.906\n",
      "Accuracy of the network on the 33 test image: 60.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 5, Step: 0, Loss: 0.43832728266716003, Lr:0.0001\n",
      "Epoch 5, Step: 1, Loss: 1.7565535306930542, Lr:0.0001\n",
      "Epoch 5, Step: 2, Loss: 1.3280607461929321, Lr:0.0001\n",
      "Epoch 5, Step: 3, Loss: 1.485280156135559, Lr:0.0001\n",
      "Epoch 5, Step: 4, Loss: 0.7965222597122192, Lr:0.0001\n",
      "Epoch 5, Step: 5, Loss: 2.26247501373291, Lr:0.0001\n",
      "Epoch 5, Step: 6, Loss: 0.8942139148712158, Lr:0.0001\n",
      "Epoch 5, Step: 7, Loss: 2.5470118522644043, Lr:0.0001\n",
      "Epoch 5, Step: 8, Loss: 1.9573272466659546, Lr:0.0001\n",
      "Epoch 5, Step: 9, Loss: 1.118865966796875, Lr:0.0001\n",
      "Epoch 5, Step: 10, Loss: 1.1039519309997559, Lr:0.0001\n",
      "Epoch 5, Step: 11, Loss: 0.7546150088310242, Lr:0.0001\n",
      "Epoch 5, Step: 12, Loss: 1.2046873569488525, Lr:0.0001\n",
      "Epoch 5, Step: 13, Loss: 0.5924436450004578, Lr:0.0001\n",
      "Epoch 5, Step: 14, Loss: 0.7431305646896362, Lr:0.0001\n",
      "Epoch 5, Step: 15, Loss: 1.6926686763763428, Lr:0.0001\n",
      "Epoch 5, Step: 16, Loss: 1.386974573135376, Lr:0.0001\n",
      "Epoch 5, Step: 17, Loss: 0.9057111144065857, Lr:0.0001\n",
      "Epoch 5, Step: 18, Loss: 1.06904137134552, Lr:0.0001\n",
      "Epoch 5, Step: 19, Loss: 1.268409013748169, Lr:0.0001\n",
      "Epoch 5, Step: 20, Loss: 1.4460749626159668, Lr:0.0001\n",
      "Epoch 5, Step: 21, Loss: 1.3713843822479248, Lr:0.0001\n",
      "Epoch 5, Step: 22, Loss: 1.7927165031433105, Lr:0.0001\n",
      "Epoch 5, Step: 23, Loss: 1.4505971670150757, Lr:0.0001\n",
      "Epoch 5, Step: 24, Loss: 1.2352235317230225, Lr:0.0001\n",
      "Epoch 5, Step: 25, Loss: 0.41858890652656555, Lr:0.0001\n",
      "Epoch 5, Step: 26, Loss: 0.5300997495651245, Lr:0.0001\n",
      "Epoch 5, Step: 27, Loss: 0.6152997016906738, Lr:0.0001\n",
      "Epoch 5, Step: 28, Loss: 0.9963778853416443, Lr:0.0001\n",
      "Epoch 5, Step: 29, Loss: 1.0835561752319336, Lr:0.0001\n",
      "Epoch 5, Step: 30, Loss: 0.7812522649765015, Lr:0.0001\n",
      "Epoch 5, Step: 31, Loss: 1.1708041429519653, Lr:0.0001\n",
      "Epoch 5, Step: 32, Loss: 1.3117321729660034, Lr:0.0001\n",
      "Epoch 5, Step: 33, Loss: 1.109300971031189, Lr:0.0001\n",
      "Epoch 5, Step: 34, Loss: 0.7114939093589783, Lr:0.0001\n",
      "Epoch 5, Step: 35, Loss: 0.8600722551345825, Lr:0.0001\n",
      "Epoch 5, Step: 36, Loss: 1.2057852745056152, Lr:0.0001\n",
      "Epoch 5, Step: 37, Loss: 1.096464991569519, Lr:0.0001\n",
      "Epoch 5, Step: 38, Loss: 1.0972938537597656, Lr:0.0001\n",
      "Epoch 5, Step: 39, Loss: 0.43245774507522583, Lr:0.0001\n",
      "Epoch 5, Step: 40, Loss: 0.42689090967178345, Lr:0.0001\n",
      "Epoch 5, Step: 41, Loss: 0.8226423263549805, Lr:0.0001\n",
      "Epoch 5, Step: 42, Loss: 0.8039771914482117, Lr:0.0001\n",
      "Epoch 5, Step: 43, Loss: 1.0714693069458008, Lr:0.0001\n",
      "Epoch 5, Step: 44, Loss: 1.0321931838989258, Lr:0.0001\n",
      "Epoch 5, Step: 45, Loss: 0.9875161647796631, Lr:0.0001\n",
      "Epoch 5, Step: 46, Loss: 1.6949423551559448, Lr:0.0001\n",
      "Epoch 5, Step: 47, Loss: 1.0360521078109741, Lr:0.0001\n",
      "Epoch 5, Step: 48, Loss: 0.8566816449165344, Lr:0.0001\n",
      "Epoch 5, Step: 49, Loss: 0.5339171290397644, Lr:0.0001\n",
      "Epoch 5, Step: 50, Loss: 0.9635822176933289, Lr:0.0001\n",
      "Epoch 5, Step: 51, Loss: 0.8206386566162109, Lr:0.0001\n",
      "Epoch 5, Step: 52, Loss: 1.229434609413147, Lr:0.0001\n",
      "Epoch 5, Step: 53, Loss: 0.5187605023384094, Lr:0.0001\n",
      "Epoch 5, Step: 54, Loss: 0.91542649269104, Lr:0.0001\n",
      "Epoch 5, Step: 55, Loss: 2.1394999027252197, Lr:0.0001\n",
      "Epoch 5, Step: 56, Loss: 1.4830049276351929, Lr:0.0001\n",
      "Epoch 5, Step: 57, Loss: 1.0935921669006348, Lr:0.0001\n",
      "Epoch 5, Step: 58, Loss: 2.0383553504943848, Lr:0.0001\n",
      "Epoch 5, Step: 59, Loss: 2.9207756519317627, Lr:0.0001\n",
      "Epoch 5, Step: 60, Loss: 0.8138881325721741, Lr:0.0001\n",
      "Epoch 5, Step: 61, Loss: 1.100985050201416, Lr:0.0001\n",
      "Epoch 5, Step: 62, Loss: 1.7384544610977173, Lr:0.0001\n",
      "Epoch 5, Step: 63, Loss: 0.8211210370063782, Lr:0.0001\n",
      "Epoch 5, Step: 64, Loss: 1.4640858173370361, Lr:0.0001\n",
      "Epoch 5, Step: 65, Loss: 2.2926106452941895, Lr:0.0001\n",
      "Epoch 5, Step: 66, Loss: 1.3618875741958618, Lr:0.0001\n",
      "Epoch 5, Step: 67, Loss: 0.7858741879463196, Lr:0.0001\n",
      "Epoch 5, Step: 68, Loss: 0.8195379972457886, Lr:0.0001\n",
      "Epoch 5, Step: 69, Loss: 0.4213874042034149, Lr:0.0001\n",
      "Epoch 5, Step: 70, Loss: 0.8436722159385681, Lr:0.0001\n",
      "Epoch 5, Step: 71, Loss: 2.416735887527466, Lr:0.0001\n",
      "Epoch 5, Step: 72, Loss: 0.8515560626983643, Lr:0.0001\n",
      "Epoch 5, Step: 73, Loss: 1.1543948650360107, Lr:0.0001\n",
      "Epoch 5, Step: 74, Loss: 0.97111976146698, Lr:0.0001\n",
      "Epoch 5, Step: 75, Loss: 1.194504737854004, Lr:0.0001\n",
      "Epoch 5, Step: 76, Loss: 0.8714587688446045, Lr:0.0001\n",
      "Epoch 5, Step: 77, Loss: 1.3259458541870117, Lr:0.0001\n",
      "Epoch 5, Step: 78, Loss: 1.393681526184082, Lr:0.0001\n",
      "Epoch 5, Step: 79, Loss: 0.6340394020080566, Lr:0.0001\n",
      "Epoch 5, Step: 80, Loss: 0.596183180809021, Lr:0.0001\n",
      "Epoch 5, Step: 81, Loss: 1.07688570022583, Lr:0.0001\n",
      "Epoch 5, Step: 82, Loss: 1.6221009492874146, Lr:0.0001\n",
      "Epoch 5, Step: 83, Loss: 1.1582242250442505, Lr:0.0001\n",
      "Epoch 5, Step: 84, Loss: 1.5668001174926758, Lr:0.0001\n",
      "Epoch 5, Step: 85, Loss: 0.679722249507904, Lr:0.0001\n",
      "Epoch 5, Step: 86, Loss: 1.6491525173187256, Lr:0.0001\n",
      "Epoch 5, Step: 87, Loss: 0.5415987968444824, Lr:0.0001\n",
      "Epoch 5, Step: 88, Loss: 0.7840730547904968, Lr:0.0001\n",
      "Epoch 5, Step: 89, Loss: 1.0249100923538208, Lr:0.0001\n",
      "Epoch 5, Step: 90, Loss: 1.5830435752868652, Lr:0.0001\n",
      "Epoch 5, Step: 91, Loss: 1.103401780128479, Lr:0.0001\n",
      "Epoch 5, Step: 92, Loss: 1.1287328004837036, Lr:0.0001\n",
      "Epoch 5, Step: 93, Loss: 2.3902320861816406, Lr:0.0001\n",
      "Epoch 5, Step: 94, Loss: 0.6491317749023438, Lr:0.0001\n",
      "Epoch 5, Step: 95, Loss: 0.7171406745910645, Lr:0.0001\n",
      "Epoch 5, Step: 96, Loss: 0.43334901332855225, Lr:0.0001\n",
      "Epoch 5, Step: 97, Loss: 2.2627758979797363, Lr:0.0001\n",
      "Epoch 5, Step: 98, Loss: 0.520667314529419, Lr:0.0001\n",
      "Epoch 5, Step: 99, Loss: 1.0005617141723633, Lr:0.0001\n",
      "Epoch 5, Step: 100, Loss: 0.6971772313117981, Lr:0.0001\n",
      "Epoch 5, Step: 101, Loss: 1.3601051568984985, Lr:0.0001\n",
      "Epoch 5, Step: 102, Loss: 1.3987464904785156, Lr:0.0001\n",
      "Epoch 5, Step: 103, Loss: 1.6597367525100708, Lr:0.0001\n",
      "Epoch 5, Step: 104, Loss: 1.6439769268035889, Lr:0.0001\n",
      "Epoch 5, Step: 105, Loss: 1.0420249700546265, Lr:0.0001\n",
      "Epoch 5, Step: 106, Loss: 1.7824857234954834, Lr:0.0001\n",
      "Epoch 5, Step: 107, Loss: 0.9184509515762329, Lr:0.0001\n",
      "Epoch 5, Step: 108, Loss: 2.1741552352905273, Lr:0.0001\n",
      "Epoch 5, Step: 109, Loss: 0.9798852205276489, Lr:0.0001\n",
      "Epoch 5, Step: 110, Loss: 0.8800797462463379, Lr:0.0001\n",
      "Epoch 5, Step: 111, Loss: 1.3026387691497803, Lr:0.0001\n",
      "Epoch 5, Step: 112, Loss: 1.5926955938339233, Lr:0.0001\n",
      "Epoch 5, Step: 113, Loss: 1.6549298763275146, Lr:0.0001\n",
      "Epoch 5, Step: 114, Loss: 1.0593326091766357, Lr:0.0001\n",
      "Epoch 5, Step: 115, Loss: 0.8157802224159241, Lr:0.0001\n",
      "Epoch 5, Step: 116, Loss: 0.5457199811935425, Lr:0.0001\n",
      "Epoch 5, Step: 117, Loss: 0.9478867650032043, Lr:0.0001\n",
      "Epoch 5, Step: 118, Loss: 0.5673519968986511, Lr:0.0001\n",
      "Epoch 5, Step: 119, Loss: 1.5967204570770264, Lr:0.0001\n",
      "Epoch 5, Step: 120, Loss: 3.5044755935668945, Lr:0.0001\n",
      "Epoch 5, Step: 121, Loss: 1.5742192268371582, Lr:0.0001\n",
      "Epoch 5, Step: 122, Loss: 1.7263281345367432, Lr:0.0001\n",
      "Epoch 5, Step: 123, Loss: 0.5867781043052673, Lr:0.0001\n",
      "Epoch 5, Step: 124, Loss: 1.0131819248199463, Lr:0.0001\n",
      "Epoch 5, Step: 125, Loss: 0.9412893056869507, Lr:0.0001\n",
      "Epoch 5, Step: 126, Loss: 1.3946731090545654, Lr:0.0001\n",
      "Epoch 5, Step: 127, Loss: 0.7812477350234985, Lr:0.0001\n",
      "Epoch 5, Step: 128, Loss: 0.6987133622169495, Lr:0.0001\n",
      "Epoch 5, Step: 129, Loss: 0.9514662027359009, Lr:0.0001\n",
      "Epoch 5, Step: 130, Loss: 1.73530912399292, Lr:0.0001\n",
      "Epoch 5, Step: 131, Loss: 0.7499184608459473, Lr:0.0001\n",
      "Epoch 5, Step: 132, Loss: 1.5643608570098877, Lr:0.0001\n",
      "Epoch 5, Step: 133, Loss: 1.3852438926696777, Lr:0.0001\n",
      "Epoch 5, Step: 134, Loss: 1.0545977354049683, Lr:0.0001\n",
      "Epoch 5, Step: 135, Loss: 0.9499950408935547, Lr:0.0001\n",
      "Epoch 5, Step: 136, Loss: 0.7916641235351562, Lr:0.0001\n",
      "Epoch 5, Step: 137, Loss: 1.1356152296066284, Lr:0.0001\n",
      "Epoch 5, Step: 138, Loss: 1.0587425231933594, Lr:0.0001\n",
      "Epoch 5, Step: 139, Loss: 0.592353880405426, Lr:0.0001\n",
      "Epoch 5, Step: 140, Loss: 1.5975494384765625, Lr:0.0001\n",
      "Epoch 5, Step: 141, Loss: 0.858495831489563, Lr:0.0001\n",
      "Epoch 5, Step: 142, Loss: 1.6369469165802002, Lr:0.0001\n",
      "Epoch 5, Step: 143, Loss: 1.0147333145141602, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 5\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1679 (0.1679) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0110 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7144 (1.1920) acc1: 50.0000 (57.5758) acc5: 100.0000 (100.0000) time: 0.0088 data: 0.0033 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0089 s / it)\n",
      "* Acc@1 57.576 Acc@5 100.000 loss 1.192\n",
      "Accuracy of the network on the 33 test image: 57.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 6, Step: 0, Loss: 0.8201760053634644, Lr:0.0001\n",
      "Epoch 6, Step: 1, Loss: 0.6542555689811707, Lr:0.0001\n",
      "Epoch 6, Step: 2, Loss: 1.1925926208496094, Lr:0.0001\n",
      "Epoch 6, Step: 3, Loss: 0.7943633794784546, Lr:0.0001\n",
      "Epoch 6, Step: 4, Loss: 1.493910551071167, Lr:0.0001\n",
      "Epoch 6, Step: 5, Loss: 0.9927875995635986, Lr:0.0001\n",
      "Epoch 6, Step: 6, Loss: 0.7822508811950684, Lr:0.0001\n",
      "Epoch 6, Step: 7, Loss: 1.2326501607894897, Lr:0.0001\n",
      "Epoch 6, Step: 8, Loss: 0.9608345627784729, Lr:0.0001\n",
      "Epoch 6, Step: 9, Loss: 0.9463607668876648, Lr:0.0001\n",
      "Epoch 6, Step: 10, Loss: 0.9863261580467224, Lr:0.0001\n",
      "Epoch 6, Step: 11, Loss: 0.9769517183303833, Lr:0.0001\n",
      "Epoch 6, Step: 12, Loss: 0.40645480155944824, Lr:0.0001\n",
      "Epoch 6, Step: 13, Loss: 1.1498795747756958, Lr:0.0001\n",
      "Epoch 6, Step: 14, Loss: 1.613724708557129, Lr:0.0001\n",
      "Epoch 6, Step: 15, Loss: 0.28651076555252075, Lr:0.0001\n",
      "Epoch 6, Step: 16, Loss: 1.8067721128463745, Lr:0.0001\n",
      "Epoch 6, Step: 17, Loss: 1.9884568452835083, Lr:0.0001\n",
      "Epoch 6, Step: 18, Loss: 0.3444399833679199, Lr:0.0001\n",
      "Epoch 6, Step: 19, Loss: 1.592281460762024, Lr:0.0001\n",
      "Epoch 6, Step: 20, Loss: 0.5768641233444214, Lr:0.0001\n",
      "Epoch 6, Step: 21, Loss: 1.1419825553894043, Lr:0.0001\n",
      "Epoch 6, Step: 22, Loss: 0.8720512390136719, Lr:0.0001\n",
      "Epoch 6, Step: 23, Loss: 1.3504610061645508, Lr:0.0001\n",
      "Epoch 6, Step: 24, Loss: 1.0757508277893066, Lr:0.0001\n",
      "Epoch 6, Step: 25, Loss: 0.8541442155838013, Lr:0.0001\n",
      "Epoch 6, Step: 26, Loss: 1.2392600774765015, Lr:0.0001\n",
      "Epoch 6, Step: 27, Loss: 0.966970682144165, Lr:0.0001\n",
      "Epoch 6, Step: 28, Loss: 1.2748345136642456, Lr:0.0001\n",
      "Epoch 6, Step: 29, Loss: 0.9525281190872192, Lr:0.0001\n",
      "Epoch 6, Step: 30, Loss: 2.043501377105713, Lr:0.0001\n",
      "Epoch 6, Step: 31, Loss: 0.6654939651489258, Lr:0.0001\n",
      "Epoch 6, Step: 32, Loss: 1.4558322429656982, Lr:0.0001\n",
      "Epoch 6, Step: 33, Loss: 0.8755150437355042, Lr:0.0001\n",
      "Epoch 6, Step: 34, Loss: 0.9930726885795593, Lr:0.0001\n",
      "Epoch 6, Step: 35, Loss: 0.7483958005905151, Lr:0.0001\n",
      "Epoch 6, Step: 36, Loss: 0.6410943269729614, Lr:0.0001\n",
      "Epoch 6, Step: 37, Loss: 0.5167730450630188, Lr:0.0001\n",
      "Epoch 6, Step: 38, Loss: 0.8857464790344238, Lr:0.0001\n",
      "Epoch 6, Step: 39, Loss: 0.5976245403289795, Lr:0.0001\n",
      "Epoch 6, Step: 40, Loss: 1.3196543455123901, Lr:0.0001\n",
      "Epoch 6, Step: 41, Loss: 1.2518943548202515, Lr:0.0001\n",
      "Epoch 6, Step: 42, Loss: 0.8177019357681274, Lr:0.0001\n",
      "Epoch 6, Step: 43, Loss: 1.077335238456726, Lr:0.0001\n",
      "Epoch 6, Step: 44, Loss: 0.6539131999015808, Lr:0.0001\n",
      "Epoch 6, Step: 45, Loss: 0.3949878513813019, Lr:0.0001\n",
      "Epoch 6, Step: 46, Loss: 0.4841741621494293, Lr:0.0001\n",
      "Epoch 6, Step: 47, Loss: 0.5197232365608215, Lr:0.0001\n",
      "Epoch 6, Step: 48, Loss: 1.7910363674163818, Lr:0.0001\n",
      "Epoch 6, Step: 49, Loss: 0.9322400689125061, Lr:0.0001\n",
      "Epoch 6, Step: 50, Loss: 1.0449299812316895, Lr:0.0001\n",
      "Epoch 6, Step: 51, Loss: 1.112971544265747, Lr:0.0001\n",
      "Epoch 6, Step: 52, Loss: 1.2733057737350464, Lr:0.0001\n",
      "Epoch 6, Step: 53, Loss: 2.078108549118042, Lr:0.0001\n",
      "Epoch 6, Step: 54, Loss: 0.9324491024017334, Lr:0.0001\n",
      "Epoch 6, Step: 55, Loss: 1.8813097476959229, Lr:0.0001\n",
      "Epoch 6, Step: 56, Loss: 1.892996907234192, Lr:0.0001\n",
      "Epoch 6, Step: 57, Loss: 1.1434216499328613, Lr:0.0001\n",
      "Epoch 6, Step: 58, Loss: 2.293487071990967, Lr:0.0001\n",
      "Epoch 6, Step: 59, Loss: 1.3557696342468262, Lr:0.0001\n",
      "Epoch 6, Step: 60, Loss: 0.7134289145469666, Lr:0.0001\n",
      "Epoch 6, Step: 61, Loss: 0.7036358714103699, Lr:0.0001\n",
      "Epoch 6, Step: 62, Loss: 1.0502989292144775, Lr:0.0001\n",
      "Epoch 6, Step: 63, Loss: 1.0915429592132568, Lr:0.0001\n",
      "Epoch 6, Step: 64, Loss: 1.986777901649475, Lr:0.0001\n",
      "Epoch 6, Step: 65, Loss: 1.2051491737365723, Lr:0.0001\n",
      "Epoch 6, Step: 66, Loss: 1.465214490890503, Lr:0.0001\n",
      "Epoch 6, Step: 67, Loss: 0.6588090658187866, Lr:0.0001\n",
      "Epoch 6, Step: 68, Loss: 0.9078524112701416, Lr:0.0001\n",
      "Epoch 6, Step: 69, Loss: 1.3041925430297852, Lr:0.0001\n",
      "Epoch 6, Step: 70, Loss: 1.2589781284332275, Lr:0.0001\n",
      "Epoch 6, Step: 71, Loss: 0.743704080581665, Lr:0.0001\n",
      "Epoch 6, Step: 72, Loss: 1.1081746816635132, Lr:0.0001\n",
      "Epoch 6, Step: 73, Loss: 0.8646938800811768, Lr:0.0001\n",
      "Epoch 6, Step: 74, Loss: 1.014366865158081, Lr:0.0001\n",
      "Epoch 6, Step: 75, Loss: 2.053056478500366, Lr:0.0001\n",
      "Epoch 6, Step: 76, Loss: 1.6686716079711914, Lr:0.0001\n",
      "Epoch 6, Step: 77, Loss: 1.109944224357605, Lr:0.0001\n",
      "Epoch 6, Step: 78, Loss: 0.9171761274337769, Lr:0.0001\n",
      "Epoch 6, Step: 79, Loss: 0.9900572896003723, Lr:0.0001\n",
      "Epoch 6, Step: 80, Loss: 1.2769362926483154, Lr:0.0001\n",
      "Epoch 6, Step: 81, Loss: 0.8981994390487671, Lr:0.0001\n",
      "Epoch 6, Step: 82, Loss: 1.2259846925735474, Lr:0.0001\n",
      "Epoch 6, Step: 83, Loss: 1.8096468448638916, Lr:0.0001\n",
      "Epoch 6, Step: 84, Loss: 0.5053086280822754, Lr:0.0001\n",
      "Epoch 6, Step: 85, Loss: 1.382168173789978, Lr:0.0001\n",
      "Epoch 6, Step: 86, Loss: 1.0519123077392578, Lr:0.0001\n",
      "Epoch 6, Step: 87, Loss: 1.0729275941848755, Lr:0.0001\n",
      "Epoch 6, Step: 88, Loss: 1.7464581727981567, Lr:0.0001\n",
      "Epoch 6, Step: 89, Loss: 0.9427677989006042, Lr:0.0001\n",
      "Epoch 6, Step: 90, Loss: 0.6325492858886719, Lr:0.0001\n",
      "Epoch 6, Step: 91, Loss: 0.7829983830451965, Lr:0.0001\n",
      "Epoch 6, Step: 92, Loss: 1.1260526180267334, Lr:0.0001\n",
      "Epoch 6, Step: 93, Loss: 0.8344930410385132, Lr:0.0001\n",
      "Epoch 6, Step: 94, Loss: 1.4403992891311646, Lr:0.0001\n",
      "Epoch 6, Step: 95, Loss: 0.6659457683563232, Lr:0.0001\n",
      "Epoch 6, Step: 96, Loss: 0.8033813238143921, Lr:0.0001\n",
      "Epoch 6, Step: 97, Loss: 0.8294426202774048, Lr:0.0001\n",
      "Epoch 6, Step: 98, Loss: 0.5827159881591797, Lr:0.0001\n",
      "Epoch 6, Step: 99, Loss: 2.1353607177734375, Lr:0.0001\n",
      "Epoch 6, Step: 100, Loss: 0.4588039219379425, Lr:0.0001\n",
      "Epoch 6, Step: 101, Loss: 1.3847081661224365, Lr:0.0001\n",
      "Epoch 6, Step: 102, Loss: 1.0954797267913818, Lr:0.0001\n",
      "Epoch 6, Step: 103, Loss: 0.7201362252235413, Lr:0.0001\n",
      "Epoch 6, Step: 104, Loss: 0.37359684705734253, Lr:0.0001\n",
      "Epoch 6, Step: 105, Loss: 0.6009948253631592, Lr:0.0001\n",
      "Epoch 6, Step: 106, Loss: 1.35210382938385, Lr:0.0001\n",
      "Epoch 6, Step: 107, Loss: 1.0251215696334839, Lr:0.0001\n",
      "Epoch 6, Step: 108, Loss: 1.1190868616104126, Lr:0.0001\n",
      "Epoch 6, Step: 109, Loss: 1.177909255027771, Lr:0.0001\n",
      "Epoch 6, Step: 110, Loss: 0.8817409873008728, Lr:0.0001\n",
      "Epoch 6, Step: 111, Loss: 0.6213863492012024, Lr:0.0001\n",
      "Epoch 6, Step: 112, Loss: 0.6338492631912231, Lr:0.0001\n",
      "Epoch 6, Step: 113, Loss: 1.3361972570419312, Lr:0.0001\n",
      "Epoch 6, Step: 114, Loss: 2.518564462661743, Lr:0.0001\n",
      "Epoch 6, Step: 115, Loss: 0.8499273657798767, Lr:0.0001\n",
      "Epoch 6, Step: 116, Loss: 0.49142712354660034, Lr:0.0001\n",
      "Epoch 6, Step: 117, Loss: 0.5605575442314148, Lr:0.0001\n",
      "Epoch 6, Step: 118, Loss: 0.5723536610603333, Lr:0.0001\n",
      "Epoch 6, Step: 119, Loss: 1.845603346824646, Lr:0.0001\n",
      "Epoch 6, Step: 120, Loss: 1.6559383869171143, Lr:0.0001\n",
      "Epoch 6, Step: 121, Loss: 1.0543642044067383, Lr:0.0001\n",
      "Epoch 6, Step: 122, Loss: 1.56974196434021, Lr:0.0001\n",
      "Epoch 6, Step: 123, Loss: 0.4697595238685608, Lr:0.0001\n",
      "Epoch 6, Step: 124, Loss: 1.6872543096542358, Lr:0.0001\n",
      "Epoch 6, Step: 125, Loss: 1.089493989944458, Lr:0.0001\n",
      "Epoch 6, Step: 126, Loss: 0.748504638671875, Lr:0.0001\n",
      "Epoch 6, Step: 127, Loss: 0.9044644832611084, Lr:0.0001\n",
      "Epoch 6, Step: 128, Loss: 0.28613975644111633, Lr:0.0001\n",
      "Epoch 6, Step: 129, Loss: 1.9076942205429077, Lr:0.0001\n",
      "Epoch 6, Step: 130, Loss: 1.5592414140701294, Lr:0.0001\n",
      "Epoch 6, Step: 131, Loss: 0.5064966082572937, Lr:0.0001\n",
      "Epoch 6, Step: 132, Loss: 1.0592107772827148, Lr:0.0001\n",
      "Epoch 6, Step: 133, Loss: 2.3135604858398438, Lr:0.0001\n",
      "Epoch 6, Step: 134, Loss: 0.5969420671463013, Lr:0.0001\n",
      "Epoch 6, Step: 135, Loss: 2.4128973484039307, Lr:0.0001\n",
      "Epoch 6, Step: 136, Loss: 1.3434295654296875, Lr:0.0001\n",
      "Epoch 6, Step: 137, Loss: 2.3987784385681152, Lr:0.0001\n",
      "Epoch 6, Step: 138, Loss: 0.5118640661239624, Lr:0.0001\n",
      "Epoch 6, Step: 139, Loss: 0.3816760778427124, Lr:0.0001\n",
      "Epoch 6, Step: 140, Loss: 1.2029458284378052, Lr:0.0001\n",
      "Epoch 6, Step: 141, Loss: 0.8419119119644165, Lr:0.0001\n",
      "Epoch 6, Step: 142, Loss: 1.017089605331421, Lr:0.0001\n",
      "Epoch 6, Step: 143, Loss: 0.7462548017501831, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 6\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.3198 (0.3198) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0100 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.6774 (1.4418) acc1: 50.0000 (54.5455) acc5: 100.0000 (100.0000) time: 0.0067 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0067 s / it)\n",
      "* Acc@1 54.545 Acc@5 100.000 loss 1.442\n",
      "Accuracy of the network on the 33 test image: 54.5%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 7, Step: 0, Loss: 0.6092623472213745, Lr:0.0001\n",
      "Epoch 7, Step: 1, Loss: 1.4182534217834473, Lr:0.0001\n",
      "Epoch 7, Step: 2, Loss: 1.120543122291565, Lr:0.0001\n",
      "Epoch 7, Step: 3, Loss: 1.03428316116333, Lr:0.0001\n",
      "Epoch 7, Step: 4, Loss: 0.6030185222625732, Lr:0.0001\n",
      "Epoch 7, Step: 5, Loss: 1.3524852991104126, Lr:0.0001\n",
      "Epoch 7, Step: 6, Loss: 0.5642094612121582, Lr:0.0001\n",
      "Epoch 7, Step: 7, Loss: 1.4611443281173706, Lr:0.0001\n",
      "Epoch 7, Step: 8, Loss: 1.0321176052093506, Lr:0.0001\n",
      "Epoch 7, Step: 9, Loss: 0.6931876540184021, Lr:0.0001\n",
      "Epoch 7, Step: 10, Loss: 2.076981782913208, Lr:0.0001\n",
      "Epoch 7, Step: 11, Loss: 1.2511377334594727, Lr:0.0001\n",
      "Epoch 7, Step: 12, Loss: 1.1626694202423096, Lr:0.0001\n",
      "Epoch 7, Step: 13, Loss: 1.274981141090393, Lr:0.0001\n",
      "Epoch 7, Step: 14, Loss: 0.9219628572463989, Lr:0.0001\n",
      "Epoch 7, Step: 15, Loss: 1.3195550441741943, Lr:0.0001\n",
      "Epoch 7, Step: 16, Loss: 0.965652585029602, Lr:0.0001\n",
      "Epoch 7, Step: 17, Loss: 0.8241263628005981, Lr:0.0001\n",
      "Epoch 7, Step: 18, Loss: 1.4124772548675537, Lr:0.0001\n",
      "Epoch 7, Step: 19, Loss: 1.7171605825424194, Lr:0.0001\n",
      "Epoch 7, Step: 20, Loss: 0.9274269342422485, Lr:0.0001\n",
      "Epoch 7, Step: 21, Loss: 0.9623217582702637, Lr:0.0001\n",
      "Epoch 7, Step: 22, Loss: 1.061492681503296, Lr:0.0001\n",
      "Epoch 7, Step: 23, Loss: 0.6700736284255981, Lr:0.0001\n",
      "Epoch 7, Step: 24, Loss: 1.1964590549468994, Lr:0.0001\n",
      "Epoch 7, Step: 25, Loss: 0.6601855754852295, Lr:0.0001\n",
      "Epoch 7, Step: 26, Loss: 1.861937165260315, Lr:0.0001\n",
      "Epoch 7, Step: 27, Loss: 1.563550591468811, Lr:0.0001\n",
      "Epoch 7, Step: 28, Loss: 0.5261170864105225, Lr:0.0001\n",
      "Epoch 7, Step: 29, Loss: 1.7846262454986572, Lr:0.0001\n",
      "Epoch 7, Step: 30, Loss: 0.9853143692016602, Lr:0.0001\n",
      "Epoch 7, Step: 31, Loss: 0.9320165514945984, Lr:0.0001\n",
      "Epoch 7, Step: 32, Loss: 1.3684308528900146, Lr:0.0001\n",
      "Epoch 7, Step: 33, Loss: 0.9387809038162231, Lr:0.0001\n",
      "Epoch 7, Step: 34, Loss: 1.012943983078003, Lr:0.0001\n",
      "Epoch 7, Step: 35, Loss: 1.0020756721496582, Lr:0.0001\n",
      "Epoch 7, Step: 36, Loss: 1.0302685499191284, Lr:0.0001\n",
      "Epoch 7, Step: 37, Loss: 1.16177499294281, Lr:0.0001\n",
      "Epoch 7, Step: 38, Loss: 0.2218797653913498, Lr:0.0001\n",
      "Epoch 7, Step: 39, Loss: 1.4996808767318726, Lr:0.0001\n",
      "Epoch 7, Step: 40, Loss: 0.7088486552238464, Lr:0.0001\n",
      "Epoch 7, Step: 41, Loss: 0.9790636301040649, Lr:0.0001\n",
      "Epoch 7, Step: 42, Loss: 0.6750736236572266, Lr:0.0001\n",
      "Epoch 7, Step: 43, Loss: 1.0365432500839233, Lr:0.0001\n",
      "Epoch 7, Step: 44, Loss: 1.285305142402649, Lr:0.0001\n",
      "Epoch 7, Step: 45, Loss: 0.4273727238178253, Lr:0.0001\n",
      "Epoch 7, Step: 46, Loss: 0.38842135667800903, Lr:0.0001\n",
      "Epoch 7, Step: 47, Loss: 0.6372705698013306, Lr:0.0001\n",
      "Epoch 7, Step: 48, Loss: 0.7426197528839111, Lr:0.0001\n",
      "Epoch 7, Step: 49, Loss: 0.48048514127731323, Lr:0.0001\n",
      "Epoch 7, Step: 50, Loss: 1.0570600032806396, Lr:0.0001\n",
      "Epoch 7, Step: 51, Loss: 0.41041263937950134, Lr:0.0001\n",
      "Epoch 7, Step: 52, Loss: 1.2455477714538574, Lr:0.0001\n",
      "Epoch 7, Step: 53, Loss: 0.641849160194397, Lr:0.0001\n",
      "Epoch 7, Step: 54, Loss: 0.9697995781898499, Lr:0.0001\n",
      "Epoch 7, Step: 55, Loss: 1.621349811553955, Lr:0.0001\n",
      "Epoch 7, Step: 56, Loss: 0.745223879814148, Lr:0.0001\n",
      "Epoch 7, Step: 57, Loss: 0.30877619981765747, Lr:0.0001\n",
      "Epoch 7, Step: 58, Loss: 0.9005928039550781, Lr:0.0001\n",
      "Epoch 7, Step: 59, Loss: 1.5822999477386475, Lr:0.0001\n",
      "Epoch 7, Step: 60, Loss: 0.650898277759552, Lr:0.0001\n",
      "Epoch 7, Step: 61, Loss: 0.6469180583953857, Lr:0.0001\n",
      "Epoch 7, Step: 62, Loss: 1.3627992868423462, Lr:0.0001\n",
      "Epoch 7, Step: 63, Loss: 0.5894799828529358, Lr:0.0001\n",
      "Epoch 7, Step: 64, Loss: 0.9783646464347839, Lr:0.0001\n",
      "Epoch 7, Step: 65, Loss: 1.5749155282974243, Lr:0.0001\n",
      "Epoch 7, Step: 66, Loss: 1.0314244031906128, Lr:0.0001\n",
      "Epoch 7, Step: 67, Loss: 1.30995774269104, Lr:0.0001\n",
      "Epoch 7, Step: 68, Loss: 1.2772915363311768, Lr:0.0001\n",
      "Epoch 7, Step: 69, Loss: 0.6454939246177673, Lr:0.0001\n",
      "Epoch 7, Step: 70, Loss: 1.2072958946228027, Lr:0.0001\n",
      "Epoch 7, Step: 71, Loss: 0.5197199583053589, Lr:0.0001\n",
      "Epoch 7, Step: 72, Loss: 0.9756888151168823, Lr:0.0001\n",
      "Epoch 7, Step: 73, Loss: 0.3953586220741272, Lr:0.0001\n",
      "Epoch 7, Step: 74, Loss: 0.4611787796020508, Lr:0.0001\n",
      "Epoch 7, Step: 75, Loss: 1.08473801612854, Lr:0.0001\n",
      "Epoch 7, Step: 76, Loss: 0.4701980948448181, Lr:0.0001\n",
      "Epoch 7, Step: 77, Loss: 1.2467248439788818, Lr:0.0001\n",
      "Epoch 7, Step: 78, Loss: 1.1136562824249268, Lr:0.0001\n",
      "Epoch 7, Step: 79, Loss: 0.6641155481338501, Lr:0.0001\n",
      "Epoch 7, Step: 80, Loss: 2.4063398838043213, Lr:0.0001\n",
      "Epoch 7, Step: 81, Loss: 2.286038875579834, Lr:0.0001\n",
      "Epoch 7, Step: 82, Loss: 1.1421685218811035, Lr:0.0001\n",
      "Epoch 7, Step: 83, Loss: 1.3419387340545654, Lr:0.0001\n",
      "Epoch 7, Step: 84, Loss: 1.2136170864105225, Lr:0.0001\n",
      "Epoch 7, Step: 85, Loss: 1.209350347518921, Lr:0.0001\n",
      "Epoch 7, Step: 86, Loss: 1.4524040222167969, Lr:0.0001\n",
      "Epoch 7, Step: 87, Loss: 0.5119301080703735, Lr:0.0001\n",
      "Epoch 7, Step: 88, Loss: 0.5693445801734924, Lr:0.0001\n",
      "Epoch 7, Step: 89, Loss: 2.212111234664917, Lr:0.0001\n",
      "Epoch 7, Step: 90, Loss: 0.769609808921814, Lr:0.0001\n",
      "Epoch 7, Step: 91, Loss: 1.6287494897842407, Lr:0.0001\n",
      "Epoch 7, Step: 92, Loss: 0.6826956868171692, Lr:0.0001\n",
      "Epoch 7, Step: 93, Loss: 1.1135491132736206, Lr:0.0001\n",
      "Epoch 7, Step: 94, Loss: 1.281180739402771, Lr:0.0001\n",
      "Epoch 7, Step: 95, Loss: 1.5817326307296753, Lr:0.0001\n",
      "Epoch 7, Step: 96, Loss: 0.8907948732376099, Lr:0.0001\n",
      "Epoch 7, Step: 97, Loss: 1.181328296661377, Lr:0.0001\n",
      "Epoch 7, Step: 98, Loss: 1.896660327911377, Lr:0.0001\n",
      "Epoch 7, Step: 99, Loss: 1.8838238716125488, Lr:0.0001\n",
      "Epoch 7, Step: 100, Loss: 2.7525787353515625, Lr:0.0001\n",
      "Epoch 7, Step: 101, Loss: 1.292109727859497, Lr:0.0001\n",
      "Epoch 7, Step: 102, Loss: 1.2061271667480469, Lr:0.0001\n",
      "Epoch 7, Step: 103, Loss: 1.1490134000778198, Lr:0.0001\n",
      "Epoch 7, Step: 104, Loss: 1.6679294109344482, Lr:0.0001\n",
      "Epoch 7, Step: 105, Loss: 1.221522569656372, Lr:0.0001\n",
      "Epoch 7, Step: 106, Loss: 0.6700611114501953, Lr:0.0001\n",
      "Epoch 7, Step: 107, Loss: 0.9789683818817139, Lr:0.0001\n",
      "Epoch 7, Step: 108, Loss: 2.1012167930603027, Lr:0.0001\n",
      "Epoch 7, Step: 109, Loss: 1.5316554307937622, Lr:0.0001\n",
      "Epoch 7, Step: 110, Loss: 0.9234949946403503, Lr:0.0001\n",
      "Epoch 7, Step: 111, Loss: 0.6761727333068848, Lr:0.0001\n",
      "Epoch 7, Step: 112, Loss: 0.5576674938201904, Lr:0.0001\n",
      "Epoch 7, Step: 113, Loss: 1.740779161453247, Lr:0.0001\n",
      "Epoch 7, Step: 114, Loss: 0.7646956443786621, Lr:0.0001\n",
      "Epoch 7, Step: 115, Loss: 0.9130106568336487, Lr:0.0001\n",
      "Epoch 7, Step: 116, Loss: 1.1289281845092773, Lr:0.0001\n",
      "Epoch 7, Step: 117, Loss: 1.3569018840789795, Lr:0.0001\n",
      "Epoch 7, Step: 118, Loss: 1.264420747756958, Lr:0.0001\n",
      "Epoch 7, Step: 119, Loss: 1.6327018737792969, Lr:0.0001\n",
      "Epoch 7, Step: 120, Loss: 1.0085078477859497, Lr:0.0001\n",
      "Epoch 7, Step: 121, Loss: 0.7435063123703003, Lr:0.0001\n",
      "Epoch 7, Step: 122, Loss: 1.3202039003372192, Lr:0.0001\n",
      "Epoch 7, Step: 123, Loss: 2.354555368423462, Lr:0.0001\n",
      "Epoch 7, Step: 124, Loss: 1.0252869129180908, Lr:0.0001\n",
      "Epoch 7, Step: 125, Loss: 0.9090033769607544, Lr:0.0001\n",
      "Epoch 7, Step: 126, Loss: 0.3659035563468933, Lr:0.0001\n",
      "Epoch 7, Step: 127, Loss: 0.40436089038848877, Lr:0.0001\n",
      "Epoch 7, Step: 128, Loss: 0.7377286553382874, Lr:0.0001\n",
      "Epoch 7, Step: 129, Loss: 1.4036633968353271, Lr:0.0001\n",
      "Epoch 7, Step: 130, Loss: 0.8246076703071594, Lr:0.0001\n",
      "Epoch 7, Step: 131, Loss: 0.5939141511917114, Lr:0.0001\n",
      "Epoch 7, Step: 132, Loss: 1.04935622215271, Lr:0.0001\n",
      "Epoch 7, Step: 133, Loss: 0.5374922752380371, Lr:0.0001\n",
      "Epoch 7, Step: 134, Loss: 1.0617238283157349, Lr:0.0001\n",
      "Epoch 7, Step: 135, Loss: 1.00310480594635, Lr:0.0001\n",
      "Epoch 7, Step: 136, Loss: 1.1565239429473877, Lr:0.0001\n",
      "Epoch 7, Step: 137, Loss: 2.1497833728790283, Lr:0.0001\n",
      "Epoch 7, Step: 138, Loss: 0.4762197732925415, Lr:0.0001\n",
      "Epoch 7, Step: 139, Loss: 0.7437249422073364, Lr:0.0001\n",
      "Epoch 7, Step: 140, Loss: 1.8570680618286133, Lr:0.0001\n",
      "Epoch 7, Step: 141, Loss: 0.8302394151687622, Lr:0.0001\n",
      "Epoch 7, Step: 142, Loss: 0.5529730319976807, Lr:0.0001\n",
      "Epoch 7, Step: 143, Loss: 1.094559669494629, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 7\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1079 (0.1079) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0109 data: 0.0049 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7459 (0.8729) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0077 data: 0.0033 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0078 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 0.873\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 8, Step: 0, Loss: 2.7118887901306152, Lr:0.0001\n",
      "Epoch 8, Step: 1, Loss: 0.3708014488220215, Lr:0.0001\n",
      "Epoch 8, Step: 2, Loss: 1.0969613790512085, Lr:0.0001\n",
      "Epoch 8, Step: 3, Loss: 0.7117869853973389, Lr:0.0001\n",
      "Epoch 8, Step: 4, Loss: 0.6536051630973816, Lr:0.0001\n",
      "Epoch 8, Step: 5, Loss: 0.8729081153869629, Lr:0.0001\n",
      "Epoch 8, Step: 6, Loss: 1.1483550071716309, Lr:0.0001\n",
      "Epoch 8, Step: 7, Loss: 0.9752587676048279, Lr:0.0001\n",
      "Epoch 8, Step: 8, Loss: 1.1801304817199707, Lr:0.0001\n",
      "Epoch 8, Step: 9, Loss: 1.813269853591919, Lr:0.0001\n",
      "Epoch 8, Step: 10, Loss: 1.2326990365982056, Lr:0.0001\n",
      "Epoch 8, Step: 11, Loss: 0.8181313276290894, Lr:0.0001\n",
      "Epoch 8, Step: 12, Loss: 0.44957512617111206, Lr:0.0001\n",
      "Epoch 8, Step: 13, Loss: 1.9195890426635742, Lr:0.0001\n",
      "Epoch 8, Step: 14, Loss: 1.1960549354553223, Lr:0.0001\n",
      "Epoch 8, Step: 15, Loss: 0.51239413022995, Lr:0.0001\n",
      "Epoch 8, Step: 16, Loss: 1.0109238624572754, Lr:0.0001\n",
      "Epoch 8, Step: 17, Loss: 1.160813808441162, Lr:0.0001\n",
      "Epoch 8, Step: 18, Loss: 0.321231871843338, Lr:0.0001\n",
      "Epoch 8, Step: 19, Loss: 1.4074839353561401, Lr:0.0001\n",
      "Epoch 8, Step: 20, Loss: 1.3844192028045654, Lr:0.0001\n",
      "Epoch 8, Step: 21, Loss: 0.8856472969055176, Lr:0.0001\n",
      "Epoch 8, Step: 22, Loss: 0.473965585231781, Lr:0.0001\n",
      "Epoch 8, Step: 23, Loss: 1.3391129970550537, Lr:0.0001\n",
      "Epoch 8, Step: 24, Loss: 0.8982062339782715, Lr:0.0001\n",
      "Epoch 8, Step: 25, Loss: 0.3970996141433716, Lr:0.0001\n",
      "Epoch 8, Step: 26, Loss: 1.1623162031173706, Lr:0.0001\n",
      "Epoch 8, Step: 27, Loss: 0.8139656186103821, Lr:0.0001\n",
      "Epoch 8, Step: 28, Loss: 0.4227142333984375, Lr:0.0001\n",
      "Epoch 8, Step: 29, Loss: 0.3814275860786438, Lr:0.0001\n",
      "Epoch 8, Step: 30, Loss: 0.7661078572273254, Lr:0.0001\n",
      "Epoch 8, Step: 31, Loss: 0.4263533651828766, Lr:0.0001\n",
      "Epoch 8, Step: 32, Loss: 0.5822197794914246, Lr:0.0001\n",
      "Epoch 8, Step: 33, Loss: 0.7280838489532471, Lr:0.0001\n",
      "Epoch 8, Step: 34, Loss: 1.3017977476119995, Lr:0.0001\n",
      "Epoch 8, Step: 35, Loss: 0.6430956125259399, Lr:0.0001\n",
      "Epoch 8, Step: 36, Loss: 1.7031004428863525, Lr:0.0001\n",
      "Epoch 8, Step: 37, Loss: 1.002971887588501, Lr:0.0001\n",
      "Epoch 8, Step: 38, Loss: 0.4885455369949341, Lr:0.0001\n",
      "Epoch 8, Step: 39, Loss: 0.9500702023506165, Lr:0.0001\n",
      "Epoch 8, Step: 40, Loss: 1.7686519622802734, Lr:0.0001\n",
      "Epoch 8, Step: 41, Loss: 1.6565163135528564, Lr:0.0001\n",
      "Epoch 8, Step: 42, Loss: 1.5270273685455322, Lr:0.0001\n",
      "Epoch 8, Step: 43, Loss: 0.599995493888855, Lr:0.0001\n",
      "Epoch 8, Step: 44, Loss: 1.1332972049713135, Lr:0.0001\n",
      "Epoch 8, Step: 45, Loss: 0.43532291054725647, Lr:0.0001\n",
      "Epoch 8, Step: 46, Loss: 0.8116121292114258, Lr:0.0001\n",
      "Epoch 8, Step: 47, Loss: 1.568441390991211, Lr:0.0001\n",
      "Epoch 8, Step: 48, Loss: 1.687408447265625, Lr:0.0001\n",
      "Epoch 8, Step: 49, Loss: 1.370149850845337, Lr:0.0001\n",
      "Epoch 8, Step: 50, Loss: 0.49448662996292114, Lr:0.0001\n",
      "Epoch 8, Step: 51, Loss: 0.41022515296936035, Lr:0.0001\n",
      "Epoch 8, Step: 52, Loss: 1.313504934310913, Lr:0.0001\n",
      "Epoch 8, Step: 53, Loss: 1.8156431913375854, Lr:0.0001\n",
      "Epoch 8, Step: 54, Loss: 1.0434640645980835, Lr:0.0001\n",
      "Epoch 8, Step: 55, Loss: 1.676308035850525, Lr:0.0001\n",
      "Epoch 8, Step: 56, Loss: 0.8603448867797852, Lr:0.0001\n",
      "Epoch 8, Step: 57, Loss: 0.8347529172897339, Lr:0.0001\n",
      "Epoch 8, Step: 58, Loss: 1.0964218378067017, Lr:0.0001\n",
      "Epoch 8, Step: 59, Loss: 1.1487324237823486, Lr:0.0001\n",
      "Epoch 8, Step: 60, Loss: 0.9653398990631104, Lr:0.0001\n",
      "Epoch 8, Step: 61, Loss: 0.6027361750602722, Lr:0.0001\n",
      "Epoch 8, Step: 62, Loss: 1.0771312713623047, Lr:0.0001\n",
      "Epoch 8, Step: 63, Loss: 0.9514048099517822, Lr:0.0001\n",
      "Epoch 8, Step: 64, Loss: 0.4609852433204651, Lr:0.0001\n",
      "Epoch 8, Step: 65, Loss: 1.6496237516403198, Lr:0.0001\n",
      "Epoch 8, Step: 66, Loss: 0.6677418351173401, Lr:0.0001\n",
      "Epoch 8, Step: 67, Loss: 1.203596591949463, Lr:0.0001\n",
      "Epoch 8, Step: 68, Loss: 0.9615623354911804, Lr:0.0001\n",
      "Epoch 8, Step: 69, Loss: 0.8794883489608765, Lr:0.0001\n",
      "Epoch 8, Step: 70, Loss: 1.2220144271850586, Lr:0.0001\n",
      "Epoch 8, Step: 71, Loss: 1.0021066665649414, Lr:0.0001\n",
      "Epoch 8, Step: 72, Loss: 0.9486845135688782, Lr:0.0001\n",
      "Epoch 8, Step: 73, Loss: 0.3538382649421692, Lr:0.0001\n",
      "Epoch 8, Step: 74, Loss: 0.6157859563827515, Lr:0.0001\n",
      "Epoch 8, Step: 75, Loss: 1.3066890239715576, Lr:0.0001\n",
      "Epoch 8, Step: 76, Loss: 1.5290395021438599, Lr:0.0001\n",
      "Epoch 8, Step: 77, Loss: 1.0284861326217651, Lr:0.0001\n",
      "Epoch 8, Step: 78, Loss: 1.1051256656646729, Lr:0.0001\n",
      "Epoch 8, Step: 79, Loss: 0.6773898601531982, Lr:0.0001\n",
      "Epoch 8, Step: 80, Loss: 0.9700047969818115, Lr:0.0001\n",
      "Epoch 8, Step: 81, Loss: 0.7902554869651794, Lr:0.0001\n",
      "Epoch 8, Step: 82, Loss: 1.4977999925613403, Lr:0.0001\n",
      "Epoch 8, Step: 83, Loss: 0.7153685092926025, Lr:0.0001\n",
      "Epoch 8, Step: 84, Loss: 0.6751496195793152, Lr:0.0001\n",
      "Epoch 8, Step: 85, Loss: 0.6619853377342224, Lr:0.0001\n",
      "Epoch 8, Step: 86, Loss: 0.9472227096557617, Lr:0.0001\n",
      "Epoch 8, Step: 87, Loss: 0.60428786277771, Lr:0.0001\n",
      "Epoch 8, Step: 88, Loss: 0.7825461030006409, Lr:0.0001\n",
      "Epoch 8, Step: 89, Loss: 0.6931347846984863, Lr:0.0001\n",
      "Epoch 8, Step: 90, Loss: 0.3433885872364044, Lr:0.0001\n",
      "Epoch 8, Step: 91, Loss: 0.20234368741512299, Lr:0.0001\n",
      "Epoch 8, Step: 92, Loss: 1.6535606384277344, Lr:0.0001\n",
      "Epoch 8, Step: 93, Loss: 1.0660054683685303, Lr:0.0001\n",
      "Epoch 8, Step: 94, Loss: 1.1235194206237793, Lr:0.0001\n",
      "Epoch 8, Step: 95, Loss: 1.1222939491271973, Lr:0.0001\n",
      "Epoch 8, Step: 96, Loss: 1.8373312950134277, Lr:0.0001\n",
      "Epoch 8, Step: 97, Loss: 0.5645261406898499, Lr:0.0001\n",
      "Epoch 8, Step: 98, Loss: 1.0997250080108643, Lr:0.0001\n",
      "Epoch 8, Step: 99, Loss: 0.609729528427124, Lr:0.0001\n",
      "Epoch 8, Step: 100, Loss: 0.8471260070800781, Lr:0.0001\n",
      "Epoch 8, Step: 101, Loss: 0.6437082886695862, Lr:0.0001\n",
      "Epoch 8, Step: 102, Loss: 1.9952418804168701, Lr:0.0001\n",
      "Epoch 8, Step: 103, Loss: 1.158167839050293, Lr:0.0001\n",
      "Epoch 8, Step: 104, Loss: 1.7872693538665771, Lr:0.0001\n",
      "Epoch 8, Step: 105, Loss: 0.66218101978302, Lr:0.0001\n",
      "Epoch 8, Step: 106, Loss: 1.2692787647247314, Lr:0.0001\n",
      "Epoch 8, Step: 107, Loss: 0.614362359046936, Lr:0.0001\n",
      "Epoch 8, Step: 108, Loss: 0.8947842121124268, Lr:0.0001\n",
      "Epoch 8, Step: 109, Loss: 0.7458785772323608, Lr:0.0001\n",
      "Epoch 8, Step: 110, Loss: 0.8949523568153381, Lr:0.0001\n",
      "Epoch 8, Step: 111, Loss: 1.0487276315689087, Lr:0.0001\n",
      "Epoch 8, Step: 112, Loss: 0.6460270285606384, Lr:0.0001\n",
      "Epoch 8, Step: 113, Loss: 1.0528429746627808, Lr:0.0001\n",
      "Epoch 8, Step: 114, Loss: 1.9951521158218384, Lr:0.0001\n",
      "Epoch 8, Step: 115, Loss: 1.5675249099731445, Lr:0.0001\n",
      "Epoch 8, Step: 116, Loss: 1.222677230834961, Lr:0.0001\n",
      "Epoch 8, Step: 117, Loss: 1.0443278551101685, Lr:0.0001\n",
      "Epoch 8, Step: 118, Loss: 1.188750982284546, Lr:0.0001\n",
      "Epoch 8, Step: 119, Loss: 1.2278205156326294, Lr:0.0001\n",
      "Epoch 8, Step: 120, Loss: 1.1491057872772217, Lr:0.0001\n",
      "Epoch 8, Step: 121, Loss: 0.510416567325592, Lr:0.0001\n",
      "Epoch 8, Step: 122, Loss: 0.7461714744567871, Lr:0.0001\n",
      "Epoch 8, Step: 123, Loss: 1.4639701843261719, Lr:0.0001\n",
      "Epoch 8, Step: 124, Loss: 0.8127936124801636, Lr:0.0001\n",
      "Epoch 8, Step: 125, Loss: 2.252429485321045, Lr:0.0001\n",
      "Epoch 8, Step: 126, Loss: 2.345015048980713, Lr:0.0001\n",
      "Epoch 8, Step: 127, Loss: 0.22371260821819305, Lr:0.0001\n",
      "Epoch 8, Step: 128, Loss: 0.6368855237960815, Lr:0.0001\n",
      "Epoch 8, Step: 129, Loss: 2.053079128265381, Lr:0.0001\n",
      "Epoch 8, Step: 130, Loss: 0.20189009606838226, Lr:0.0001\n",
      "Epoch 8, Step: 131, Loss: 1.5082879066467285, Lr:0.0001\n",
      "Epoch 8, Step: 132, Loss: 1.3590114116668701, Lr:0.0001\n",
      "Epoch 8, Step: 133, Loss: 1.0448769330978394, Lr:0.0001\n",
      "Epoch 8, Step: 134, Loss: 0.9362998008728027, Lr:0.0001\n",
      "Epoch 8, Step: 135, Loss: 0.8303748965263367, Lr:0.0001\n",
      "Epoch 8, Step: 136, Loss: 1.1524875164031982, Lr:0.0001\n",
      "Epoch 8, Step: 137, Loss: 0.957905113697052, Lr:0.0001\n",
      "Epoch 8, Step: 138, Loss: 0.8171558380126953, Lr:0.0001\n",
      "Epoch 8, Step: 139, Loss: 1.0893197059631348, Lr:0.0001\n",
      "Epoch 8, Step: 140, Loss: 0.732183575630188, Lr:0.0001\n",
      "Epoch 8, Step: 141, Loss: 0.6180768013000488, Lr:0.0001\n",
      "Epoch 8, Step: 142, Loss: 0.8345059156417847, Lr:0.0001\n",
      "Epoch 8, Step: 143, Loss: 2.405177116394043, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 8\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.2596 (0.2596) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0120 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.9232 (0.9447) acc1: 50.0000 (57.5758) acc5: 100.0000 (100.0000) time: 0.0088 data: 0.0037 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0088 s / it)\n",
      "* Acc@1 57.576 Acc@5 100.000 loss 0.945\n",
      "Accuracy of the network on the 33 test image: 57.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 9, Step: 0, Loss: 0.6448902487754822, Lr:0.0001\n",
      "Epoch 9, Step: 1, Loss: 0.574393630027771, Lr:0.0001\n",
      "Epoch 9, Step: 2, Loss: 1.7512545585632324, Lr:0.0001\n",
      "Epoch 9, Step: 3, Loss: 0.7349678874015808, Lr:0.0001\n",
      "Epoch 9, Step: 4, Loss: 0.8908638954162598, Lr:0.0001\n",
      "Epoch 9, Step: 5, Loss: 0.9571786522865295, Lr:0.0001\n",
      "Epoch 9, Step: 6, Loss: 0.7433659434318542, Lr:0.0001\n",
      "Epoch 9, Step: 7, Loss: 0.9954569339752197, Lr:0.0001\n",
      "Epoch 9, Step: 8, Loss: 1.180724859237671, Lr:0.0001\n",
      "Epoch 9, Step: 9, Loss: 0.9461519122123718, Lr:0.0001\n",
      "Epoch 9, Step: 10, Loss: 0.39588022232055664, Lr:0.0001\n",
      "Epoch 9, Step: 11, Loss: 1.2562060356140137, Lr:0.0001\n",
      "Epoch 9, Step: 12, Loss: 0.7752547264099121, Lr:0.0001\n",
      "Epoch 9, Step: 13, Loss: 0.6854580640792847, Lr:0.0001\n",
      "Epoch 9, Step: 14, Loss: 1.3766820430755615, Lr:0.0001\n",
      "Epoch 9, Step: 15, Loss: 0.719018816947937, Lr:0.0001\n",
      "Epoch 9, Step: 16, Loss: 1.4247435331344604, Lr:0.0001\n",
      "Epoch 9, Step: 17, Loss: 1.2658138275146484, Lr:0.0001\n",
      "Epoch 9, Step: 18, Loss: 1.5948073863983154, Lr:0.0001\n",
      "Epoch 9, Step: 19, Loss: 1.1284866333007812, Lr:0.0001\n",
      "Epoch 9, Step: 20, Loss: 0.6273563504219055, Lr:0.0001\n",
      "Epoch 9, Step: 21, Loss: 0.8232317566871643, Lr:0.0001\n",
      "Epoch 9, Step: 22, Loss: 0.6123266816139221, Lr:0.0001\n",
      "Epoch 9, Step: 23, Loss: 0.9637029767036438, Lr:0.0001\n",
      "Epoch 9, Step: 24, Loss: 1.5574883222579956, Lr:0.0001\n",
      "Epoch 9, Step: 25, Loss: 0.7236807942390442, Lr:0.0001\n",
      "Epoch 9, Step: 26, Loss: 0.9621535539627075, Lr:0.0001\n",
      "Epoch 9, Step: 27, Loss: 0.246898353099823, Lr:0.0001\n",
      "Epoch 9, Step: 28, Loss: 1.0831612348556519, Lr:0.0001\n",
      "Epoch 9, Step: 29, Loss: 0.614523708820343, Lr:0.0001\n",
      "Epoch 9, Step: 30, Loss: 0.8480993509292603, Lr:0.0001\n",
      "Epoch 9, Step: 31, Loss: 1.606809377670288, Lr:0.0001\n",
      "Epoch 9, Step: 32, Loss: 0.9978857636451721, Lr:0.0001\n",
      "Epoch 9, Step: 33, Loss: 0.9721111059188843, Lr:0.0001\n",
      "Epoch 9, Step: 34, Loss: 0.8407909870147705, Lr:0.0001\n",
      "Epoch 9, Step: 35, Loss: 1.4547829627990723, Lr:0.0001\n",
      "Epoch 9, Step: 36, Loss: 2.8874645233154297, Lr:0.0001\n",
      "Epoch 9, Step: 37, Loss: 0.1786898821592331, Lr:0.0001\n",
      "Epoch 9, Step: 38, Loss: 2.0435445308685303, Lr:0.0001\n",
      "Epoch 9, Step: 39, Loss: 1.5924887657165527, Lr:0.0001\n",
      "Epoch 9, Step: 40, Loss: 1.8334892988204956, Lr:0.0001\n",
      "Epoch 9, Step: 41, Loss: 1.0101957321166992, Lr:0.0001\n",
      "Epoch 9, Step: 42, Loss: 0.6878001689910889, Lr:0.0001\n",
      "Epoch 9, Step: 43, Loss: 0.20532090961933136, Lr:0.0001\n",
      "Epoch 9, Step: 44, Loss: 0.9917881488800049, Lr:0.0001\n",
      "Epoch 9, Step: 45, Loss: 0.7183617353439331, Lr:0.0001\n",
      "Epoch 9, Step: 46, Loss: 0.4463992416858673, Lr:0.0001\n",
      "Epoch 9, Step: 47, Loss: 2.0494272708892822, Lr:0.0001\n",
      "Epoch 9, Step: 48, Loss: 1.7105592489242554, Lr:0.0001\n",
      "Epoch 9, Step: 49, Loss: 0.6455669403076172, Lr:0.0001\n",
      "Epoch 9, Step: 50, Loss: 1.2257198095321655, Lr:0.0001\n",
      "Epoch 9, Step: 51, Loss: 1.0418287515640259, Lr:0.0001\n",
      "Epoch 9, Step: 52, Loss: 1.4395735263824463, Lr:0.0001\n",
      "Epoch 9, Step: 53, Loss: 0.6383239030838013, Lr:0.0001\n",
      "Epoch 9, Step: 54, Loss: 0.7859900593757629, Lr:0.0001\n",
      "Epoch 9, Step: 55, Loss: 0.8117762804031372, Lr:0.0001\n",
      "Epoch 9, Step: 56, Loss: 1.1614443063735962, Lr:0.0001\n",
      "Epoch 9, Step: 57, Loss: 0.8147153854370117, Lr:0.0001\n",
      "Epoch 9, Step: 58, Loss: 0.671462893486023, Lr:0.0001\n",
      "Epoch 9, Step: 59, Loss: 0.6223322153091431, Lr:0.0001\n",
      "Epoch 9, Step: 60, Loss: 1.051340937614441, Lr:0.0001\n",
      "Epoch 9, Step: 61, Loss: 0.4969879686832428, Lr:0.0001\n",
      "Epoch 9, Step: 62, Loss: 0.7464702725410461, Lr:0.0001\n",
      "Epoch 9, Step: 63, Loss: 0.40137162804603577, Lr:0.0001\n",
      "Epoch 9, Step: 64, Loss: 1.8654168844223022, Lr:0.0001\n",
      "Epoch 9, Step: 65, Loss: 1.1747159957885742, Lr:0.0001\n",
      "Epoch 9, Step: 66, Loss: 1.3813518285751343, Lr:0.0001\n",
      "Epoch 9, Step: 67, Loss: 0.36350059509277344, Lr:0.0001\n",
      "Epoch 9, Step: 68, Loss: 0.7415612936019897, Lr:0.0001\n",
      "Epoch 9, Step: 69, Loss: 1.4295804500579834, Lr:0.0001\n",
      "Epoch 9, Step: 70, Loss: 1.3993158340454102, Lr:0.0001\n",
      "Epoch 9, Step: 71, Loss: 1.528090000152588, Lr:0.0001\n",
      "Epoch 9, Step: 72, Loss: 1.3974732160568237, Lr:0.0001\n",
      "Epoch 9, Step: 73, Loss: 1.7681260108947754, Lr:0.0001\n",
      "Epoch 9, Step: 74, Loss: 1.1399096250534058, Lr:0.0001\n",
      "Epoch 9, Step: 75, Loss: 1.4942172765731812, Lr:0.0001\n",
      "Epoch 9, Step: 76, Loss: 0.5140697360038757, Lr:0.0001\n",
      "Epoch 9, Step: 77, Loss: 1.2392429113388062, Lr:0.0001\n",
      "Epoch 9, Step: 78, Loss: 0.9135732054710388, Lr:0.0001\n",
      "Epoch 9, Step: 79, Loss: 0.7196043133735657, Lr:0.0001\n",
      "Epoch 9, Step: 80, Loss: 1.546478509902954, Lr:0.0001\n",
      "Epoch 9, Step: 81, Loss: 0.4656364917755127, Lr:0.0001\n",
      "Epoch 9, Step: 82, Loss: 1.0227006673812866, Lr:0.0001\n",
      "Epoch 9, Step: 83, Loss: 0.7746617794036865, Lr:0.0001\n",
      "Epoch 9, Step: 84, Loss: 1.0488414764404297, Lr:0.0001\n",
      "Epoch 9, Step: 85, Loss: 0.9835567474365234, Lr:0.0001\n",
      "Epoch 9, Step: 86, Loss: 1.8093712329864502, Lr:0.0001\n",
      "Epoch 9, Step: 87, Loss: 0.8519024848937988, Lr:0.0001\n",
      "Epoch 9, Step: 88, Loss: 0.8795219659805298, Lr:0.0001\n",
      "Epoch 9, Step: 89, Loss: 0.8680206537246704, Lr:0.0001\n",
      "Epoch 9, Step: 90, Loss: 0.46925002336502075, Lr:0.0001\n",
      "Epoch 9, Step: 91, Loss: 0.9877051711082458, Lr:0.0001\n",
      "Epoch 9, Step: 92, Loss: 2.2494747638702393, Lr:0.0001\n",
      "Epoch 9, Step: 93, Loss: 0.7009717226028442, Lr:0.0001\n",
      "Epoch 9, Step: 94, Loss: 1.0636576414108276, Lr:0.0001\n",
      "Epoch 9, Step: 95, Loss: 1.520632266998291, Lr:0.0001\n",
      "Epoch 9, Step: 96, Loss: 0.9148401021957397, Lr:0.0001\n",
      "Epoch 9, Step: 97, Loss: 0.4885904788970947, Lr:0.0001\n",
      "Epoch 9, Step: 98, Loss: 0.9441248774528503, Lr:0.0001\n",
      "Epoch 9, Step: 99, Loss: 0.4708651900291443, Lr:0.0001\n",
      "Epoch 9, Step: 100, Loss: 1.2025461196899414, Lr:0.0001\n",
      "Epoch 9, Step: 101, Loss: 1.0332248210906982, Lr:0.0001\n",
      "Epoch 9, Step: 102, Loss: 0.6709504127502441, Lr:0.0001\n",
      "Epoch 9, Step: 103, Loss: 1.2617186307907104, Lr:0.0001\n",
      "Epoch 9, Step: 104, Loss: 0.8600817322731018, Lr:0.0001\n",
      "Epoch 9, Step: 105, Loss: 1.5208977460861206, Lr:0.0001\n",
      "Epoch 9, Step: 106, Loss: 0.45521852374076843, Lr:0.0001\n",
      "Epoch 9, Step: 107, Loss: 1.2047480344772339, Lr:0.0001\n",
      "Epoch 9, Step: 108, Loss: 1.1194474697113037, Lr:0.0001\n",
      "Epoch 9, Step: 109, Loss: 1.9891533851623535, Lr:0.0001\n",
      "Epoch 9, Step: 110, Loss: 1.8577442169189453, Lr:0.0001\n",
      "Epoch 9, Step: 111, Loss: 1.029181957244873, Lr:0.0001\n",
      "Epoch 9, Step: 112, Loss: 1.3872008323669434, Lr:0.0001\n",
      "Epoch 9, Step: 113, Loss: 0.14098739624023438, Lr:0.0001\n",
      "Epoch 9, Step: 114, Loss: 1.0565685033798218, Lr:0.0001\n",
      "Epoch 9, Step: 115, Loss: 0.998969554901123, Lr:0.0001\n",
      "Epoch 9, Step: 116, Loss: 1.1106324195861816, Lr:0.0001\n",
      "Epoch 9, Step: 117, Loss: 0.6221876740455627, Lr:0.0001\n",
      "Epoch 9, Step: 118, Loss: 0.9913585186004639, Lr:0.0001\n",
      "Epoch 9, Step: 119, Loss: 1.296944499015808, Lr:0.0001\n",
      "Epoch 9, Step: 120, Loss: 1.5791337490081787, Lr:0.0001\n",
      "Epoch 9, Step: 121, Loss: 3.064126968383789, Lr:0.0001\n",
      "Epoch 9, Step: 122, Loss: 1.5501958131790161, Lr:0.0001\n",
      "Epoch 9, Step: 123, Loss: 1.9260129928588867, Lr:0.0001\n",
      "Epoch 9, Step: 124, Loss: 1.4123839139938354, Lr:0.0001\n",
      "Epoch 9, Step: 125, Loss: 0.3842761218547821, Lr:0.0001\n",
      "Epoch 9, Step: 126, Loss: 1.3304541110992432, Lr:0.0001\n",
      "Epoch 9, Step: 127, Loss: 0.8272378444671631, Lr:0.0001\n",
      "Epoch 9, Step: 128, Loss: 0.8346330523490906, Lr:0.0001\n",
      "Epoch 9, Step: 129, Loss: 1.0926101207733154, Lr:0.0001\n",
      "Epoch 9, Step: 130, Loss: 0.9325653910636902, Lr:0.0001\n",
      "Epoch 9, Step: 131, Loss: 1.3944811820983887, Lr:0.0001\n",
      "Epoch 9, Step: 132, Loss: 1.5772571563720703, Lr:0.0001\n",
      "Epoch 9, Step: 133, Loss: 1.0789852142333984, Lr:0.0001\n",
      "Epoch 9, Step: 134, Loss: 0.8048546314239502, Lr:0.0001\n",
      "Epoch 9, Step: 135, Loss: 0.6967121362686157, Lr:0.0001\n",
      "Epoch 9, Step: 136, Loss: 1.404736042022705, Lr:0.0001\n",
      "Epoch 9, Step: 137, Loss: 0.6898068189620972, Lr:0.0001\n",
      "Epoch 9, Step: 138, Loss: 0.6072096228599548, Lr:0.0001\n",
      "Epoch 9, Step: 139, Loss: 0.8298018574714661, Lr:0.0001\n",
      "Epoch 9, Step: 140, Loss: 0.7059804797172546, Lr:0.0001\n",
      "Epoch 9, Step: 141, Loss: 0.7146813869476318, Lr:0.0001\n",
      "Epoch 9, Step: 142, Loss: 0.6319197416305542, Lr:0.0001\n",
      "Epoch 9, Step: 143, Loss: 1.335993766784668, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 9\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0859 (0.0859) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0049 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7957 (0.7879) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0064 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0065 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 0.788\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 10, Step: 0, Loss: 2.0468862056732178, Lr:0.0001\n",
      "Epoch 10, Step: 1, Loss: 0.37607693672180176, Lr:0.0001\n",
      "Epoch 10, Step: 2, Loss: 1.23332941532135, Lr:0.0001\n",
      "Epoch 10, Step: 3, Loss: 0.5055081844329834, Lr:0.0001\n",
      "Epoch 10, Step: 4, Loss: 0.7269975543022156, Lr:0.0001\n",
      "Epoch 10, Step: 5, Loss: 1.0299065113067627, Lr:0.0001\n",
      "Epoch 10, Step: 6, Loss: 1.1519447565078735, Lr:0.0001\n",
      "Epoch 10, Step: 7, Loss: 0.6824772953987122, Lr:0.0001\n",
      "Epoch 10, Step: 8, Loss: 0.6963773965835571, Lr:0.0001\n",
      "Epoch 10, Step: 9, Loss: 0.858591616153717, Lr:0.0001\n",
      "Epoch 10, Step: 10, Loss: 0.31638771295547485, Lr:0.0001\n",
      "Epoch 10, Step: 11, Loss: 0.9099893569946289, Lr:0.0001\n",
      "Epoch 10, Step: 12, Loss: 0.9383497834205627, Lr:0.0001\n",
      "Epoch 10, Step: 13, Loss: 1.4535839557647705, Lr:0.0001\n",
      "Epoch 10, Step: 14, Loss: 0.9986392259597778, Lr:0.0001\n",
      "Epoch 10, Step: 15, Loss: 0.541885495185852, Lr:0.0001\n",
      "Epoch 10, Step: 16, Loss: 1.065032720565796, Lr:0.0001\n",
      "Epoch 10, Step: 17, Loss: 1.4571285247802734, Lr:0.0001\n",
      "Epoch 10, Step: 18, Loss: 1.2855581045150757, Lr:0.0001\n",
      "Epoch 10, Step: 19, Loss: 0.7655269503593445, Lr:0.0001\n",
      "Epoch 10, Step: 20, Loss: 0.9902888536453247, Lr:0.0001\n",
      "Epoch 10, Step: 21, Loss: 1.0795425176620483, Lr:0.0001\n",
      "Epoch 10, Step: 22, Loss: 1.036361813545227, Lr:0.0001\n",
      "Epoch 10, Step: 23, Loss: 0.6109514832496643, Lr:0.0001\n",
      "Epoch 10, Step: 24, Loss: 0.44582027196884155, Lr:0.0001\n",
      "Epoch 10, Step: 25, Loss: 0.2566833198070526, Lr:0.0001\n",
      "Epoch 10, Step: 26, Loss: 1.0424189567565918, Lr:0.0001\n",
      "Epoch 10, Step: 27, Loss: 0.43643391132354736, Lr:0.0001\n",
      "Epoch 10, Step: 28, Loss: 0.8466655015945435, Lr:0.0001\n",
      "Epoch 10, Step: 29, Loss: 0.5136406421661377, Lr:0.0001\n",
      "Epoch 10, Step: 30, Loss: 1.2549805641174316, Lr:0.0001\n",
      "Epoch 10, Step: 31, Loss: 0.8984012603759766, Lr:0.0001\n",
      "Epoch 10, Step: 32, Loss: 0.49977442622184753, Lr:0.0001\n",
      "Epoch 10, Step: 33, Loss: 1.1849489212036133, Lr:0.0001\n",
      "Epoch 10, Step: 34, Loss: 0.9514862895011902, Lr:0.0001\n",
      "Epoch 10, Step: 35, Loss: 0.8975781202316284, Lr:0.0001\n",
      "Epoch 10, Step: 36, Loss: 0.3780097961425781, Lr:0.0001\n",
      "Epoch 10, Step: 37, Loss: 0.9272142648696899, Lr:0.0001\n",
      "Epoch 10, Step: 38, Loss: 1.1786065101623535, Lr:0.0001\n",
      "Epoch 10, Step: 39, Loss: 0.20783524215221405, Lr:0.0001\n",
      "Epoch 10, Step: 40, Loss: 0.30991101264953613, Lr:0.0001\n",
      "Epoch 10, Step: 41, Loss: 1.0612800121307373, Lr:0.0001\n",
      "Epoch 10, Step: 42, Loss: 0.1749749630689621, Lr:0.0001\n",
      "Epoch 10, Step: 43, Loss: 0.9139527082443237, Lr:0.0001\n",
      "Epoch 10, Step: 44, Loss: 1.0135875940322876, Lr:0.0001\n",
      "Epoch 10, Step: 45, Loss: 1.000304102897644, Lr:0.0001\n",
      "Epoch 10, Step: 46, Loss: 0.4950411915779114, Lr:0.0001\n",
      "Epoch 10, Step: 47, Loss: 0.2787386476993561, Lr:0.0001\n",
      "Epoch 10, Step: 48, Loss: 0.5744487047195435, Lr:0.0001\n",
      "Epoch 10, Step: 49, Loss: 0.936482846736908, Lr:0.0001\n",
      "Epoch 10, Step: 50, Loss: 0.3518049120903015, Lr:0.0001\n",
      "Epoch 10, Step: 51, Loss: 0.15470775961875916, Lr:0.0001\n",
      "Epoch 10, Step: 52, Loss: 0.6486009359359741, Lr:0.0001\n",
      "Epoch 10, Step: 53, Loss: 0.4906083643436432, Lr:0.0001\n",
      "Epoch 10, Step: 54, Loss: 0.5967876315116882, Lr:0.0001\n",
      "Epoch 10, Step: 55, Loss: 0.7634826898574829, Lr:0.0001\n",
      "Epoch 10, Step: 56, Loss: 1.4596147537231445, Lr:0.0001\n",
      "Epoch 10, Step: 57, Loss: 0.6763064861297607, Lr:0.0001\n",
      "Epoch 10, Step: 58, Loss: 0.5722817778587341, Lr:0.0001\n",
      "Epoch 10, Step: 59, Loss: 0.3004820644855499, Lr:0.0001\n",
      "Epoch 10, Step: 60, Loss: 0.43852055072784424, Lr:0.0001\n",
      "Epoch 10, Step: 61, Loss: 1.3728392124176025, Lr:0.0001\n",
      "Epoch 10, Step: 62, Loss: 0.5240607261657715, Lr:0.0001\n",
      "Epoch 10, Step: 63, Loss: 2.0531160831451416, Lr:0.0001\n",
      "Epoch 10, Step: 64, Loss: 1.5878798961639404, Lr:0.0001\n",
      "Epoch 10, Step: 65, Loss: 1.553769588470459, Lr:0.0001\n",
      "Epoch 10, Step: 66, Loss: 1.2439132928848267, Lr:0.0001\n",
      "Epoch 10, Step: 67, Loss: 2.5166068077087402, Lr:0.0001\n",
      "Epoch 10, Step: 68, Loss: 1.2236250638961792, Lr:0.0001\n",
      "Epoch 10, Step: 69, Loss: 1.157098412513733, Lr:0.0001\n",
      "Epoch 10, Step: 70, Loss: 2.330502510070801, Lr:0.0001\n",
      "Epoch 10, Step: 71, Loss: 0.9753313064575195, Lr:0.0001\n",
      "Epoch 10, Step: 72, Loss: 2.435450792312622, Lr:0.0001\n",
      "Epoch 10, Step: 73, Loss: 0.4863901138305664, Lr:0.0001\n",
      "Epoch 10, Step: 74, Loss: 0.8915224671363831, Lr:0.0001\n",
      "Epoch 10, Step: 75, Loss: 0.1345263570547104, Lr:0.0001\n",
      "Epoch 10, Step: 76, Loss: 2.0218420028686523, Lr:0.0001\n",
      "Epoch 10, Step: 77, Loss: 0.9737706184387207, Lr:0.0001\n",
      "Epoch 10, Step: 78, Loss: 0.9928669929504395, Lr:0.0001\n",
      "Epoch 10, Step: 79, Loss: 0.3027445077896118, Lr:0.0001\n",
      "Epoch 10, Step: 80, Loss: 0.893103301525116, Lr:0.0001\n",
      "Epoch 10, Step: 81, Loss: 1.163925051689148, Lr:0.0001\n",
      "Epoch 10, Step: 82, Loss: 0.8434144854545593, Lr:0.0001\n",
      "Epoch 10, Step: 83, Loss: 0.9171065092086792, Lr:0.0001\n",
      "Epoch 10, Step: 84, Loss: 0.9936297535896301, Lr:0.0001\n",
      "Epoch 10, Step: 85, Loss: 0.9740193486213684, Lr:0.0001\n",
      "Epoch 10, Step: 86, Loss: 0.3406480550765991, Lr:0.0001\n",
      "Epoch 10, Step: 87, Loss: 1.2819058895111084, Lr:0.0001\n",
      "Epoch 10, Step: 88, Loss: 1.3688998222351074, Lr:0.0001\n",
      "Epoch 10, Step: 89, Loss: 0.856399655342102, Lr:0.0001\n",
      "Epoch 10, Step: 90, Loss: 2.097630500793457, Lr:0.0001\n",
      "Epoch 10, Step: 91, Loss: 1.2959438562393188, Lr:0.0001\n",
      "Epoch 10, Step: 92, Loss: 1.038629412651062, Lr:0.0001\n",
      "Epoch 10, Step: 93, Loss: 0.7865492105484009, Lr:0.0001\n",
      "Epoch 10, Step: 94, Loss: 1.5142621994018555, Lr:0.0001\n",
      "Epoch 10, Step: 95, Loss: 1.1923316717147827, Lr:0.0001\n",
      "Epoch 10, Step: 96, Loss: 0.42446184158325195, Lr:0.0001\n",
      "Epoch 10, Step: 97, Loss: 2.27321195602417, Lr:0.0001\n",
      "Epoch 10, Step: 98, Loss: 0.9349905848503113, Lr:0.0001\n",
      "Epoch 10, Step: 99, Loss: 1.0603021383285522, Lr:0.0001\n",
      "Epoch 10, Step: 100, Loss: 2.268109083175659, Lr:0.0001\n",
      "Epoch 10, Step: 101, Loss: 0.868280291557312, Lr:0.0001\n",
      "Epoch 10, Step: 102, Loss: 0.8938418626785278, Lr:0.0001\n",
      "Epoch 10, Step: 103, Loss: 0.8393989205360413, Lr:0.0001\n",
      "Epoch 10, Step: 104, Loss: 0.6688519716262817, Lr:0.0001\n",
      "Epoch 10, Step: 105, Loss: 0.8276983499526978, Lr:0.0001\n",
      "Epoch 10, Step: 106, Loss: 0.748903751373291, Lr:0.0001\n",
      "Epoch 10, Step: 107, Loss: 1.0837953090667725, Lr:0.0001\n",
      "Epoch 10, Step: 108, Loss: 0.7947761416435242, Lr:0.0001\n",
      "Epoch 10, Step: 109, Loss: 1.6656171083450317, Lr:0.0001\n",
      "Epoch 10, Step: 110, Loss: 0.8509843349456787, Lr:0.0001\n",
      "Epoch 10, Step: 111, Loss: 1.1639387607574463, Lr:0.0001\n",
      "Epoch 10, Step: 112, Loss: 0.8763233423233032, Lr:0.0001\n",
      "Epoch 10, Step: 113, Loss: 0.6987631320953369, Lr:0.0001\n",
      "Epoch 10, Step: 114, Loss: 0.41445064544677734, Lr:0.0001\n",
      "Epoch 10, Step: 115, Loss: 0.8605891466140747, Lr:0.0001\n",
      "Epoch 10, Step: 116, Loss: 1.0759977102279663, Lr:0.0001\n",
      "Epoch 10, Step: 117, Loss: 2.269923210144043, Lr:0.0001\n",
      "Epoch 10, Step: 118, Loss: 0.801313042640686, Lr:0.0001\n",
      "Epoch 10, Step: 119, Loss: 1.1169036626815796, Lr:0.0001\n",
      "Epoch 10, Step: 120, Loss: 0.9179556369781494, Lr:0.0001\n",
      "Epoch 10, Step: 121, Loss: 0.9309097528457642, Lr:0.0001\n",
      "Epoch 10, Step: 122, Loss: 1.3752813339233398, Lr:0.0001\n",
      "Epoch 10, Step: 123, Loss: 0.9832593202590942, Lr:0.0001\n",
      "Epoch 10, Step: 124, Loss: 0.725369393825531, Lr:0.0001\n",
      "Epoch 10, Step: 125, Loss: 1.3402984142303467, Lr:0.0001\n",
      "Epoch 10, Step: 126, Loss: 0.7703056335449219, Lr:0.0001\n",
      "Epoch 10, Step: 127, Loss: 0.8123542666435242, Lr:0.0001\n",
      "Epoch 10, Step: 128, Loss: 1.4752311706542969, Lr:0.0001\n",
      "Epoch 10, Step: 129, Loss: 0.7728875875473022, Lr:0.0001\n",
      "Epoch 10, Step: 130, Loss: 1.1037324666976929, Lr:0.0001\n",
      "Epoch 10, Step: 131, Loss: 0.4145592451095581, Lr:0.0001\n",
      "Epoch 10, Step: 132, Loss: 0.3912009298801422, Lr:0.0001\n",
      "Epoch 10, Step: 133, Loss: 0.6729623079299927, Lr:0.0001\n",
      "Epoch 10, Step: 134, Loss: 0.3613348603248596, Lr:0.0001\n",
      "Epoch 10, Step: 135, Loss: 0.6938896179199219, Lr:0.0001\n",
      "Epoch 10, Step: 136, Loss: 0.5928066372871399, Lr:0.0001\n",
      "Epoch 10, Step: 137, Loss: 0.9398247003555298, Lr:0.0001\n",
      "Epoch 10, Step: 138, Loss: 0.7348726391792297, Lr:0.0001\n",
      "Epoch 10, Step: 139, Loss: 0.274922251701355, Lr:0.0001\n",
      "Epoch 10, Step: 140, Loss: 0.48240548372268677, Lr:0.0001\n",
      "Epoch 10, Step: 141, Loss: 1.9569580554962158, Lr:0.0001\n",
      "Epoch 10, Step: 142, Loss: 1.3832648992538452, Lr:0.0001\n",
      "Epoch 10, Step: 143, Loss: 0.31415703892707825, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 10\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0279 (0.0279) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.8180 (0.8844) acc1: 50.0000 (60.6061) acc5: 100.0000 (100.0000) time: 0.0063 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0063 s / it)\n",
      "* Acc@1 60.606 Acc@5 100.000 loss 0.884\n",
      "Accuracy of the network on the 33 test image: 60.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 11, Step: 0, Loss: 1.5743168592453003, Lr:0.0001\n",
      "Epoch 11, Step: 1, Loss: 1.7326724529266357, Lr:0.0001\n",
      "Epoch 11, Step: 2, Loss: 0.45082223415374756, Lr:0.0001\n",
      "Epoch 11, Step: 3, Loss: 0.9416210055351257, Lr:0.0001\n",
      "Epoch 11, Step: 4, Loss: 0.44258683919906616, Lr:0.0001\n",
      "Epoch 11, Step: 5, Loss: 0.85024094581604, Lr:0.0001\n",
      "Epoch 11, Step: 6, Loss: 0.981403648853302, Lr:0.0001\n",
      "Epoch 11, Step: 7, Loss: 1.0151309967041016, Lr:0.0001\n",
      "Epoch 11, Step: 8, Loss: 2.227173089981079, Lr:0.0001\n",
      "Epoch 11, Step: 9, Loss: 0.5723302960395813, Lr:0.0001\n",
      "Epoch 11, Step: 10, Loss: 1.3524446487426758, Lr:0.0001\n",
      "Epoch 11, Step: 11, Loss: 0.7295361757278442, Lr:0.0001\n",
      "Epoch 11, Step: 12, Loss: 0.45764586329460144, Lr:0.0001\n",
      "Epoch 11, Step: 13, Loss: 1.2124168872833252, Lr:0.0001\n",
      "Epoch 11, Step: 14, Loss: 0.572300910949707, Lr:0.0001\n",
      "Epoch 11, Step: 15, Loss: 1.4036896228790283, Lr:0.0001\n",
      "Epoch 11, Step: 16, Loss: 0.7403981685638428, Lr:0.0001\n",
      "Epoch 11, Step: 17, Loss: 0.9124364852905273, Lr:0.0001\n",
      "Epoch 11, Step: 18, Loss: 0.6682526469230652, Lr:0.0001\n",
      "Epoch 11, Step: 19, Loss: 0.5775958895683289, Lr:0.0001\n",
      "Epoch 11, Step: 20, Loss: 0.7351717352867126, Lr:0.0001\n",
      "Epoch 11, Step: 21, Loss: 0.9713365435600281, Lr:0.0001\n",
      "Epoch 11, Step: 22, Loss: 2.205164670944214, Lr:0.0001\n",
      "Epoch 11, Step: 23, Loss: 0.6579541563987732, Lr:0.0001\n",
      "Epoch 11, Step: 24, Loss: 0.7913246154785156, Lr:0.0001\n",
      "Epoch 11, Step: 25, Loss: 1.0384048223495483, Lr:0.0001\n",
      "Epoch 11, Step: 26, Loss: 0.8736922740936279, Lr:0.0001\n",
      "Epoch 11, Step: 27, Loss: 1.7903701066970825, Lr:0.0001\n",
      "Epoch 11, Step: 28, Loss: 0.1590222716331482, Lr:0.0001\n",
      "Epoch 11, Step: 29, Loss: 0.77134108543396, Lr:0.0001\n",
      "Epoch 11, Step: 30, Loss: 0.8598769307136536, Lr:0.0001\n",
      "Epoch 11, Step: 31, Loss: 0.31863123178482056, Lr:0.0001\n",
      "Epoch 11, Step: 32, Loss: 1.7122596502304077, Lr:0.0001\n",
      "Epoch 11, Step: 33, Loss: 0.8906155824661255, Lr:0.0001\n",
      "Epoch 11, Step: 34, Loss: 0.6381356716156006, Lr:0.0001\n",
      "Epoch 11, Step: 35, Loss: 0.6769018173217773, Lr:0.0001\n",
      "Epoch 11, Step: 36, Loss: 3.592057943344116, Lr:0.0001\n",
      "Epoch 11, Step: 37, Loss: 1.3076565265655518, Lr:0.0001\n",
      "Epoch 11, Step: 38, Loss: 1.3746683597564697, Lr:0.0001\n",
      "Epoch 11, Step: 39, Loss: 1.4948146343231201, Lr:0.0001\n",
      "Epoch 11, Step: 40, Loss: 0.9470310211181641, Lr:0.0001\n",
      "Epoch 11, Step: 41, Loss: 0.6719476580619812, Lr:0.0001\n",
      "Epoch 11, Step: 42, Loss: 2.05767822265625, Lr:0.0001\n",
      "Epoch 11, Step: 43, Loss: 1.6773228645324707, Lr:0.0001\n",
      "Epoch 11, Step: 44, Loss: 0.4064147472381592, Lr:0.0001\n",
      "Epoch 11, Step: 45, Loss: 0.8153326511383057, Lr:0.0001\n",
      "Epoch 11, Step: 46, Loss: 0.3093913197517395, Lr:0.0001\n",
      "Epoch 11, Step: 47, Loss: 1.0851494073867798, Lr:0.0001\n",
      "Epoch 11, Step: 48, Loss: 1.2628265619277954, Lr:0.0001\n",
      "Epoch 11, Step: 49, Loss: 0.47952744364738464, Lr:0.0001\n",
      "Epoch 11, Step: 50, Loss: 1.0021947622299194, Lr:0.0001\n",
      "Epoch 11, Step: 51, Loss: 0.7618904709815979, Lr:0.0001\n",
      "Epoch 11, Step: 52, Loss: 1.0417746305465698, Lr:0.0001\n",
      "Epoch 11, Step: 53, Loss: 1.9575865268707275, Lr:0.0001\n",
      "Epoch 11, Step: 54, Loss: 1.5722728967666626, Lr:0.0001\n",
      "Epoch 11, Step: 55, Loss: 0.6938589811325073, Lr:0.0001\n",
      "Epoch 11, Step: 56, Loss: 0.6336523294448853, Lr:0.0001\n",
      "Epoch 11, Step: 57, Loss: 1.0069525241851807, Lr:0.0001\n",
      "Epoch 11, Step: 58, Loss: 0.8453453779220581, Lr:0.0001\n",
      "Epoch 11, Step: 59, Loss: 0.6593456864356995, Lr:0.0001\n",
      "Epoch 11, Step: 60, Loss: 0.2557874321937561, Lr:0.0001\n",
      "Epoch 11, Step: 61, Loss: 1.0184108018875122, Lr:0.0001\n",
      "Epoch 11, Step: 62, Loss: 1.4401856660842896, Lr:0.0001\n",
      "Epoch 11, Step: 63, Loss: 0.7920672297477722, Lr:0.0001\n",
      "Epoch 11, Step: 64, Loss: 0.7932599782943726, Lr:0.0001\n",
      "Epoch 11, Step: 65, Loss: 0.16669869422912598, Lr:0.0001\n",
      "Epoch 11, Step: 66, Loss: 1.9978249073028564, Lr:0.0001\n",
      "Epoch 11, Step: 67, Loss: 0.5985938906669617, Lr:0.0001\n",
      "Epoch 11, Step: 68, Loss: 1.1677582263946533, Lr:0.0001\n",
      "Epoch 11, Step: 69, Loss: 1.4564638137817383, Lr:0.0001\n",
      "Epoch 11, Step: 70, Loss: 0.7868385314941406, Lr:0.0001\n",
      "Epoch 11, Step: 71, Loss: 1.5148552656173706, Lr:0.0001\n",
      "Epoch 11, Step: 72, Loss: 1.0287150144577026, Lr:0.0001\n",
      "Epoch 11, Step: 73, Loss: 0.45492297410964966, Lr:0.0001\n",
      "Epoch 11, Step: 74, Loss: 1.4756982326507568, Lr:0.0001\n",
      "Epoch 11, Step: 75, Loss: 0.48823651671409607, Lr:0.0001\n",
      "Epoch 11, Step: 76, Loss: 0.6050493717193604, Lr:0.0001\n",
      "Epoch 11, Step: 77, Loss: 1.1133556365966797, Lr:0.0001\n",
      "Epoch 11, Step: 78, Loss: 1.1276241540908813, Lr:0.0001\n",
      "Epoch 11, Step: 79, Loss: 1.0695061683654785, Lr:0.0001\n",
      "Epoch 11, Step: 80, Loss: 0.8221261501312256, Lr:0.0001\n",
      "Epoch 11, Step: 81, Loss: 1.5982415676116943, Lr:0.0001\n",
      "Epoch 11, Step: 82, Loss: 0.6967222690582275, Lr:0.0001\n",
      "Epoch 11, Step: 83, Loss: 1.429396629333496, Lr:0.0001\n",
      "Epoch 11, Step: 84, Loss: 0.6333205103874207, Lr:0.0001\n",
      "Epoch 11, Step: 85, Loss: 0.3605310320854187, Lr:0.0001\n",
      "Epoch 11, Step: 86, Loss: 0.2965203821659088, Lr:0.0001\n",
      "Epoch 11, Step: 87, Loss: 0.8683348894119263, Lr:0.0001\n",
      "Epoch 11, Step: 88, Loss: 0.9774106740951538, Lr:0.0001\n",
      "Epoch 11, Step: 89, Loss: 1.2884763479232788, Lr:0.0001\n",
      "Epoch 11, Step: 90, Loss: 1.2504009008407593, Lr:0.0001\n",
      "Epoch 11, Step: 91, Loss: 1.7473201751708984, Lr:0.0001\n",
      "Epoch 11, Step: 92, Loss: 0.8373141288757324, Lr:0.0001\n",
      "Epoch 11, Step: 93, Loss: 0.7461603879928589, Lr:0.0001\n",
      "Epoch 11, Step: 94, Loss: 0.9942494034767151, Lr:0.0001\n",
      "Epoch 11, Step: 95, Loss: 0.6428773403167725, Lr:0.0001\n",
      "Epoch 11, Step: 96, Loss: 0.8667367696762085, Lr:0.0001\n",
      "Epoch 11, Step: 97, Loss: 1.5738261938095093, Lr:0.0001\n",
      "Epoch 11, Step: 98, Loss: 1.1717482805252075, Lr:0.0001\n",
      "Epoch 11, Step: 99, Loss: 0.2684028148651123, Lr:0.0001\n",
      "Epoch 11, Step: 100, Loss: 0.8574720025062561, Lr:0.0001\n",
      "Epoch 11, Step: 101, Loss: 0.6823025941848755, Lr:0.0001\n",
      "Epoch 11, Step: 102, Loss: 0.4422069191932678, Lr:0.0001\n",
      "Epoch 11, Step: 103, Loss: 0.5491189956665039, Lr:0.0001\n",
      "Epoch 11, Step: 104, Loss: 1.5151416063308716, Lr:0.0001\n",
      "Epoch 11, Step: 105, Loss: 0.832115888595581, Lr:0.0001\n",
      "Epoch 11, Step: 106, Loss: 0.7164266109466553, Lr:0.0001\n",
      "Epoch 11, Step: 107, Loss: 1.0132572650909424, Lr:0.0001\n",
      "Epoch 11, Step: 108, Loss: 0.6413224339485168, Lr:0.0001\n",
      "Epoch 11, Step: 109, Loss: 1.028232216835022, Lr:0.0001\n",
      "Epoch 11, Step: 110, Loss: 1.4111299514770508, Lr:0.0001\n",
      "Epoch 11, Step: 111, Loss: 0.3714429438114166, Lr:0.0001\n",
      "Epoch 11, Step: 112, Loss: 1.2185745239257812, Lr:0.0001\n",
      "Epoch 11, Step: 113, Loss: 0.5343338251113892, Lr:0.0001\n",
      "Epoch 11, Step: 114, Loss: 0.4274182915687561, Lr:0.0001\n",
      "Epoch 11, Step: 115, Loss: 0.2822381854057312, Lr:0.0001\n",
      "Epoch 11, Step: 116, Loss: 0.8208858370780945, Lr:0.0001\n",
      "Epoch 11, Step: 117, Loss: 0.986852765083313, Lr:0.0001\n",
      "Epoch 11, Step: 118, Loss: 0.9552384614944458, Lr:0.0001\n",
      "Epoch 11, Step: 119, Loss: 0.19380353391170502, Lr:0.0001\n",
      "Epoch 11, Step: 120, Loss: 0.9856871366500854, Lr:0.0001\n",
      "Epoch 11, Step: 121, Loss: 1.3417069911956787, Lr:0.0001\n",
      "Epoch 11, Step: 122, Loss: 1.2886345386505127, Lr:0.0001\n",
      "Epoch 11, Step: 123, Loss: 1.090972661972046, Lr:0.0001\n",
      "Epoch 11, Step: 124, Loss: 0.47291961312294006, Lr:0.0001\n",
      "Epoch 11, Step: 125, Loss: 0.5656503438949585, Lr:0.0001\n",
      "Epoch 11, Step: 126, Loss: 0.7031348347663879, Lr:0.0001\n",
      "Epoch 11, Step: 127, Loss: 0.25095877051353455, Lr:0.0001\n",
      "Epoch 11, Step: 128, Loss: 0.510675847530365, Lr:0.0001\n",
      "Epoch 11, Step: 129, Loss: 1.0122778415679932, Lr:0.0001\n",
      "Epoch 11, Step: 130, Loss: 0.7206234931945801, Lr:0.0001\n",
      "Epoch 11, Step: 131, Loss: 2.7413241863250732, Lr:0.0001\n",
      "Epoch 11, Step: 132, Loss: 1.212006688117981, Lr:0.0001\n",
      "Epoch 11, Step: 133, Loss: 0.8416234850883484, Lr:0.0001\n",
      "Epoch 11, Step: 134, Loss: 0.2049815058708191, Lr:0.0001\n",
      "Epoch 11, Step: 135, Loss: 0.8528552055358887, Lr:0.0001\n",
      "Epoch 11, Step: 136, Loss: 0.6894398331642151, Lr:0.0001\n",
      "Epoch 11, Step: 137, Loss: 1.6319106817245483, Lr:0.0001\n",
      "Epoch 11, Step: 138, Loss: 0.726674497127533, Lr:0.0001\n",
      "Epoch 11, Step: 139, Loss: 1.3297795057296753, Lr:0.0001\n",
      "Epoch 11, Step: 140, Loss: 0.7865049242973328, Lr:0.0001\n",
      "Epoch 11, Step: 141, Loss: 1.3757511377334595, Lr:0.0001\n",
      "Epoch 11, Step: 142, Loss: 0.802542507648468, Lr:0.0001\n",
      "Epoch 11, Step: 143, Loss: 2.709263801574707, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 11\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0410 (0.0410) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0109 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5862 (0.6855) acc1: 100.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0075 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0077 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.686\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 12, Step: 0, Loss: 0.19959867000579834, Lr:0.0001\n",
      "Epoch 12, Step: 1, Loss: 0.2358364462852478, Lr:0.0001\n",
      "Epoch 12, Step: 2, Loss: 0.6760222911834717, Lr:0.0001\n",
      "Epoch 12, Step: 3, Loss: 0.7970715165138245, Lr:0.0001\n",
      "Epoch 12, Step: 4, Loss: 0.4841504991054535, Lr:0.0001\n",
      "Epoch 12, Step: 5, Loss: 0.23345616459846497, Lr:0.0001\n",
      "Epoch 12, Step: 6, Loss: 0.9188734292984009, Lr:0.0001\n",
      "Epoch 12, Step: 7, Loss: 0.8723177313804626, Lr:0.0001\n",
      "Epoch 12, Step: 8, Loss: 0.7536084651947021, Lr:0.0001\n",
      "Epoch 12, Step: 9, Loss: 0.944853663444519, Lr:0.0001\n",
      "Epoch 12, Step: 10, Loss: 0.4127368927001953, Lr:0.0001\n",
      "Epoch 12, Step: 11, Loss: 1.0274484157562256, Lr:0.0001\n",
      "Epoch 12, Step: 12, Loss: 0.7243769764900208, Lr:0.0001\n",
      "Epoch 12, Step: 13, Loss: 0.6506000757217407, Lr:0.0001\n",
      "Epoch 12, Step: 14, Loss: 0.4814528822898865, Lr:0.0001\n",
      "Epoch 12, Step: 15, Loss: 0.5675808191299438, Lr:0.0001\n",
      "Epoch 12, Step: 16, Loss: 0.6921951770782471, Lr:0.0001\n",
      "Epoch 12, Step: 17, Loss: 0.6100847125053406, Lr:0.0001\n",
      "Epoch 12, Step: 18, Loss: 1.3691627979278564, Lr:0.0001\n",
      "Epoch 12, Step: 19, Loss: 0.16872701048851013, Lr:0.0001\n",
      "Epoch 12, Step: 20, Loss: 0.42605355381965637, Lr:0.0001\n",
      "Epoch 12, Step: 21, Loss: 0.5220695734024048, Lr:0.0001\n",
      "Epoch 12, Step: 22, Loss: 1.2579007148742676, Lr:0.0001\n",
      "Epoch 12, Step: 23, Loss: 1.035408616065979, Lr:0.0001\n",
      "Epoch 12, Step: 24, Loss: 0.6978281140327454, Lr:0.0001\n",
      "Epoch 12, Step: 25, Loss: 0.7530074715614319, Lr:0.0001\n",
      "Epoch 12, Step: 26, Loss: 0.7457285523414612, Lr:0.0001\n",
      "Epoch 12, Step: 27, Loss: 0.8673193454742432, Lr:0.0001\n",
      "Epoch 12, Step: 28, Loss: 0.13330620527267456, Lr:0.0001\n",
      "Epoch 12, Step: 29, Loss: 2.3394389152526855, Lr:0.0001\n",
      "Epoch 12, Step: 30, Loss: 0.42317596077919006, Lr:0.0001\n",
      "Epoch 12, Step: 31, Loss: 1.0377269983291626, Lr:0.0001\n",
      "Epoch 12, Step: 32, Loss: 1.598020315170288, Lr:0.0001\n",
      "Epoch 12, Step: 33, Loss: 1.185166835784912, Lr:0.0001\n",
      "Epoch 12, Step: 34, Loss: 0.6077223420143127, Lr:0.0001\n",
      "Epoch 12, Step: 35, Loss: 0.7905970215797424, Lr:0.0001\n",
      "Epoch 12, Step: 36, Loss: 1.1589850187301636, Lr:0.0001\n",
      "Epoch 12, Step: 37, Loss: 1.1856963634490967, Lr:0.0001\n",
      "Epoch 12, Step: 38, Loss: 1.4859397411346436, Lr:0.0001\n",
      "Epoch 12, Step: 39, Loss: 1.045529842376709, Lr:0.0001\n",
      "Epoch 12, Step: 40, Loss: 0.2976478636264801, Lr:0.0001\n",
      "Epoch 12, Step: 41, Loss: 0.7453392744064331, Lr:0.0001\n",
      "Epoch 12, Step: 42, Loss: 0.13138656318187714, Lr:0.0001\n",
      "Epoch 12, Step: 43, Loss: 0.6247002482414246, Lr:0.0001\n",
      "Epoch 12, Step: 44, Loss: 1.1292724609375, Lr:0.0001\n",
      "Epoch 12, Step: 45, Loss: 1.0786926746368408, Lr:0.0001\n",
      "Epoch 12, Step: 46, Loss: 1.8225420713424683, Lr:0.0001\n",
      "Epoch 12, Step: 47, Loss: 1.3921828269958496, Lr:0.0001\n",
      "Epoch 12, Step: 48, Loss: 1.0463018417358398, Lr:0.0001\n",
      "Epoch 12, Step: 49, Loss: 0.39098408818244934, Lr:0.0001\n",
      "Epoch 12, Step: 50, Loss: 0.6593341827392578, Lr:0.0001\n",
      "Epoch 12, Step: 51, Loss: 1.1107909679412842, Lr:0.0001\n",
      "Epoch 12, Step: 52, Loss: 0.38748010993003845, Lr:0.0001\n",
      "Epoch 12, Step: 53, Loss: 2.3573410511016846, Lr:0.0001\n",
      "Epoch 12, Step: 54, Loss: 0.988781750202179, Lr:0.0001\n",
      "Epoch 12, Step: 55, Loss: 0.5951741933822632, Lr:0.0001\n",
      "Epoch 12, Step: 56, Loss: 0.5249876976013184, Lr:0.0001\n",
      "Epoch 12, Step: 57, Loss: 2.436004161834717, Lr:0.0001\n",
      "Epoch 12, Step: 58, Loss: 0.7896481156349182, Lr:0.0001\n",
      "Epoch 12, Step: 59, Loss: 1.1763771772384644, Lr:0.0001\n",
      "Epoch 12, Step: 60, Loss: 0.885815441608429, Lr:0.0001\n",
      "Epoch 12, Step: 61, Loss: 0.5874218940734863, Lr:0.0001\n",
      "Epoch 12, Step: 62, Loss: 1.3542513847351074, Lr:0.0001\n",
      "Epoch 12, Step: 63, Loss: 1.1321094036102295, Lr:0.0001\n",
      "Epoch 12, Step: 64, Loss: 1.9049876928329468, Lr:0.0001\n",
      "Epoch 12, Step: 65, Loss: 1.3279707431793213, Lr:0.0001\n",
      "Epoch 12, Step: 66, Loss: 0.642548680305481, Lr:0.0001\n",
      "Epoch 12, Step: 67, Loss: 1.048485517501831, Lr:0.0001\n",
      "Epoch 12, Step: 68, Loss: 1.0819690227508545, Lr:0.0001\n",
      "Epoch 12, Step: 69, Loss: 0.8538905382156372, Lr:0.0001\n",
      "Epoch 12, Step: 70, Loss: 1.6289191246032715, Lr:0.0001\n",
      "Epoch 12, Step: 71, Loss: 0.6613388657569885, Lr:0.0001\n",
      "Epoch 12, Step: 72, Loss: 0.8994319438934326, Lr:0.0001\n",
      "Epoch 12, Step: 73, Loss: 2.002312421798706, Lr:0.0001\n",
      "Epoch 12, Step: 74, Loss: 1.4558442831039429, Lr:0.0001\n",
      "Epoch 12, Step: 75, Loss: 1.3214226961135864, Lr:0.0001\n",
      "Epoch 12, Step: 76, Loss: 1.5747828483581543, Lr:0.0001\n",
      "Epoch 12, Step: 77, Loss: 0.8252728581428528, Lr:0.0001\n",
      "Epoch 12, Step: 78, Loss: 1.0289623737335205, Lr:0.0001\n",
      "Epoch 12, Step: 79, Loss: 0.5885756015777588, Lr:0.0001\n",
      "Epoch 12, Step: 80, Loss: 0.6550033688545227, Lr:0.0001\n",
      "Epoch 12, Step: 81, Loss: 1.0014801025390625, Lr:0.0001\n",
      "Epoch 12, Step: 82, Loss: 1.8036372661590576, Lr:0.0001\n",
      "Epoch 12, Step: 83, Loss: 0.6328269839286804, Lr:0.0001\n",
      "Epoch 12, Step: 84, Loss: 1.1110163927078247, Lr:0.0001\n",
      "Epoch 12, Step: 85, Loss: 0.7375746369361877, Lr:0.0001\n",
      "Epoch 12, Step: 86, Loss: 0.6790809631347656, Lr:0.0001\n",
      "Epoch 12, Step: 87, Loss: 0.5132380723953247, Lr:0.0001\n",
      "Epoch 12, Step: 88, Loss: 0.46124234795570374, Lr:0.0001\n",
      "Epoch 12, Step: 89, Loss: 1.0796644687652588, Lr:0.0001\n",
      "Epoch 12, Step: 90, Loss: 0.7012184858322144, Lr:0.0001\n",
      "Epoch 12, Step: 91, Loss: 0.41177383065223694, Lr:0.0001\n",
      "Epoch 12, Step: 92, Loss: 0.7164729833602905, Lr:0.0001\n",
      "Epoch 12, Step: 93, Loss: 0.9737748503684998, Lr:0.0001\n",
      "Epoch 12, Step: 94, Loss: 2.257258892059326, Lr:0.0001\n",
      "Epoch 12, Step: 95, Loss: 1.2022945880889893, Lr:0.0001\n",
      "Epoch 12, Step: 96, Loss: 1.1530117988586426, Lr:0.0001\n",
      "Epoch 12, Step: 97, Loss: 1.3904414176940918, Lr:0.0001\n",
      "Epoch 12, Step: 98, Loss: 0.49574169516563416, Lr:0.0001\n",
      "Epoch 12, Step: 99, Loss: 0.48392772674560547, Lr:0.0001\n",
      "Epoch 12, Step: 100, Loss: 0.251764714717865, Lr:0.0001\n",
      "Epoch 12, Step: 101, Loss: 0.1516917496919632, Lr:0.0001\n",
      "Epoch 12, Step: 102, Loss: 0.20050060749053955, Lr:0.0001\n",
      "Epoch 12, Step: 103, Loss: 2.1178691387176514, Lr:0.0001\n",
      "Epoch 12, Step: 104, Loss: 1.427459478378296, Lr:0.0001\n",
      "Epoch 12, Step: 105, Loss: 0.3108750879764557, Lr:0.0001\n",
      "Epoch 12, Step: 106, Loss: 0.9118468165397644, Lr:0.0001\n",
      "Epoch 12, Step: 107, Loss: 0.8094968199729919, Lr:0.0001\n",
      "Epoch 12, Step: 108, Loss: 1.381746530532837, Lr:0.0001\n",
      "Epoch 12, Step: 109, Loss: 1.4856232404708862, Lr:0.0001\n",
      "Epoch 12, Step: 110, Loss: 1.2341492176055908, Lr:0.0001\n",
      "Epoch 12, Step: 111, Loss: 0.33264219760894775, Lr:0.0001\n",
      "Epoch 12, Step: 112, Loss: 0.8491263389587402, Lr:0.0001\n",
      "Epoch 12, Step: 113, Loss: 0.5904929637908936, Lr:0.0001\n",
      "Epoch 12, Step: 114, Loss: 1.6182537078857422, Lr:0.0001\n",
      "Epoch 12, Step: 115, Loss: 0.7179742455482483, Lr:0.0001\n",
      "Epoch 12, Step: 116, Loss: 0.9459366202354431, Lr:0.0001\n",
      "Epoch 12, Step: 117, Loss: 1.7133164405822754, Lr:0.0001\n",
      "Epoch 12, Step: 118, Loss: 1.3436665534973145, Lr:0.0001\n",
      "Epoch 12, Step: 119, Loss: 1.3863999843597412, Lr:0.0001\n",
      "Epoch 12, Step: 120, Loss: 0.4477711319923401, Lr:0.0001\n",
      "Epoch 12, Step: 121, Loss: 0.5455136895179749, Lr:0.0001\n",
      "Epoch 12, Step: 122, Loss: 0.7502530813217163, Lr:0.0001\n",
      "Epoch 12, Step: 123, Loss: 0.7805535793304443, Lr:0.0001\n",
      "Epoch 12, Step: 124, Loss: 2.0353567600250244, Lr:0.0001\n",
      "Epoch 12, Step: 125, Loss: 0.6292297840118408, Lr:0.0001\n",
      "Epoch 12, Step: 126, Loss: 1.0542924404144287, Lr:0.0001\n",
      "Epoch 12, Step: 127, Loss: 1.4011720418930054, Lr:0.0001\n",
      "Epoch 12, Step: 128, Loss: 0.7353066802024841, Lr:0.0001\n",
      "Epoch 12, Step: 129, Loss: 1.0878243446350098, Lr:0.0001\n",
      "Epoch 12, Step: 130, Loss: 1.2346619367599487, Lr:0.0001\n",
      "Epoch 12, Step: 131, Loss: 1.1303443908691406, Lr:0.0001\n",
      "Epoch 12, Step: 132, Loss: 1.0214167833328247, Lr:0.0001\n",
      "Epoch 12, Step: 133, Loss: 0.6799291372299194, Lr:0.0001\n",
      "Epoch 12, Step: 134, Loss: 1.6522741317749023, Lr:0.0001\n",
      "Epoch 12, Step: 135, Loss: 1.2620153427124023, Lr:0.0001\n",
      "Epoch 12, Step: 136, Loss: 0.46776682138442993, Lr:0.0001\n",
      "Epoch 12, Step: 137, Loss: 0.5510548949241638, Lr:0.0001\n",
      "Epoch 12, Step: 138, Loss: 0.9909343719482422, Lr:0.0001\n",
      "Epoch 12, Step: 139, Loss: 1.9086403846740723, Lr:0.0001\n",
      "Epoch 12, Step: 140, Loss: 0.4090905785560608, Lr:0.0001\n",
      "Epoch 12, Step: 141, Loss: 0.880407989025116, Lr:0.0001\n",
      "Epoch 12, Step: 142, Loss: 0.4139382243156433, Lr:0.0001\n",
      "Epoch 12, Step: 143, Loss: 0.9392052888870239, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 12\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1096 (0.1096) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4711 (0.6497) acc1: 100.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0068 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0068 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.650\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 13, Step: 0, Loss: 0.6910302639007568, Lr:0.0001\n",
      "Epoch 13, Step: 1, Loss: 2.7863240242004395, Lr:0.0001\n",
      "Epoch 13, Step: 2, Loss: 1.0631474256515503, Lr:0.0001\n",
      "Epoch 13, Step: 3, Loss: 0.4938929080963135, Lr:0.0001\n",
      "Epoch 13, Step: 4, Loss: 0.7731415033340454, Lr:0.0001\n",
      "Epoch 13, Step: 5, Loss: 0.8418602347373962, Lr:0.0001\n",
      "Epoch 13, Step: 6, Loss: 0.8791653513908386, Lr:0.0001\n",
      "Epoch 13, Step: 7, Loss: 0.5432873368263245, Lr:0.0001\n",
      "Epoch 13, Step: 8, Loss: 1.9224021434783936, Lr:0.0001\n",
      "Epoch 13, Step: 9, Loss: 0.5291757583618164, Lr:0.0001\n",
      "Epoch 13, Step: 10, Loss: 1.1823978424072266, Lr:0.0001\n",
      "Epoch 13, Step: 11, Loss: 0.3868425190448761, Lr:0.0001\n",
      "Epoch 13, Step: 12, Loss: 0.47070014476776123, Lr:0.0001\n",
      "Epoch 13, Step: 13, Loss: 0.7038657069206238, Lr:0.0001\n",
      "Epoch 13, Step: 14, Loss: 0.7942993640899658, Lr:0.0001\n",
      "Epoch 13, Step: 15, Loss: 1.3485506772994995, Lr:0.0001\n",
      "Epoch 13, Step: 16, Loss: 1.200372338294983, Lr:0.0001\n",
      "Epoch 13, Step: 17, Loss: 0.5550431609153748, Lr:0.0001\n",
      "Epoch 13, Step: 18, Loss: 1.0410881042480469, Lr:0.0001\n",
      "Epoch 13, Step: 19, Loss: 0.37550029158592224, Lr:0.0001\n",
      "Epoch 13, Step: 20, Loss: 0.2361306995153427, Lr:0.0001\n",
      "Epoch 13, Step: 21, Loss: 1.7401630878448486, Lr:0.0001\n",
      "Epoch 13, Step: 22, Loss: 1.2663971185684204, Lr:0.0001\n",
      "Epoch 13, Step: 23, Loss: 0.7383344173431396, Lr:0.0001\n",
      "Epoch 13, Step: 24, Loss: 0.47229403257369995, Lr:0.0001\n",
      "Epoch 13, Step: 25, Loss: 0.733965277671814, Lr:0.0001\n",
      "Epoch 13, Step: 26, Loss: 0.6081099510192871, Lr:0.0001\n",
      "Epoch 13, Step: 27, Loss: 1.0820709466934204, Lr:0.0001\n",
      "Epoch 13, Step: 28, Loss: 0.7406890392303467, Lr:0.0001\n",
      "Epoch 13, Step: 29, Loss: 0.3414945900440216, Lr:0.0001\n",
      "Epoch 13, Step: 30, Loss: 0.5128035545349121, Lr:0.0001\n",
      "Epoch 13, Step: 31, Loss: 1.030766248703003, Lr:0.0001\n",
      "Epoch 13, Step: 32, Loss: 0.38659974932670593, Lr:0.0001\n",
      "Epoch 13, Step: 33, Loss: 0.37933066487312317, Lr:0.0001\n",
      "Epoch 13, Step: 34, Loss: 1.1002979278564453, Lr:0.0001\n",
      "Epoch 13, Step: 35, Loss: 1.213304042816162, Lr:0.0001\n",
      "Epoch 13, Step: 36, Loss: 1.0205169916152954, Lr:0.0001\n",
      "Epoch 13, Step: 37, Loss: 1.0942795276641846, Lr:0.0001\n",
      "Epoch 13, Step: 38, Loss: 1.5438573360443115, Lr:0.0001\n",
      "Epoch 13, Step: 39, Loss: 0.9326645731925964, Lr:0.0001\n",
      "Epoch 13, Step: 40, Loss: 0.5322118997573853, Lr:0.0001\n",
      "Epoch 13, Step: 41, Loss: 0.6770988702774048, Lr:0.0001\n",
      "Epoch 13, Step: 42, Loss: 1.3394354581832886, Lr:0.0001\n",
      "Epoch 13, Step: 43, Loss: 0.9938107132911682, Lr:0.0001\n",
      "Epoch 13, Step: 44, Loss: 0.5188127756118774, Lr:0.0001\n",
      "Epoch 13, Step: 45, Loss: 1.2323575019836426, Lr:0.0001\n",
      "Epoch 13, Step: 46, Loss: 1.0101661682128906, Lr:0.0001\n",
      "Epoch 13, Step: 47, Loss: 0.9520162343978882, Lr:0.0001\n",
      "Epoch 13, Step: 48, Loss: 0.3640240728855133, Lr:0.0001\n",
      "Epoch 13, Step: 49, Loss: 0.6329769492149353, Lr:0.0001\n",
      "Epoch 13, Step: 50, Loss: 0.26371124386787415, Lr:0.0001\n",
      "Epoch 13, Step: 51, Loss: 0.7636888027191162, Lr:0.0001\n",
      "Epoch 13, Step: 52, Loss: 1.3436393737792969, Lr:0.0001\n",
      "Epoch 13, Step: 53, Loss: 0.24972833693027496, Lr:0.0001\n",
      "Epoch 13, Step: 54, Loss: 0.9877328872680664, Lr:0.0001\n",
      "Epoch 13, Step: 55, Loss: 1.9582664966583252, Lr:0.0001\n",
      "Epoch 13, Step: 56, Loss: 0.7509281635284424, Lr:0.0001\n",
      "Epoch 13, Step: 57, Loss: 1.0520700216293335, Lr:0.0001\n",
      "Epoch 13, Step: 58, Loss: 1.2191237211227417, Lr:0.0001\n",
      "Epoch 13, Step: 59, Loss: 0.703127384185791, Lr:0.0001\n",
      "Epoch 13, Step: 60, Loss: 0.42960506677627563, Lr:0.0001\n",
      "Epoch 13, Step: 61, Loss: 0.6394813060760498, Lr:0.0001\n",
      "Epoch 13, Step: 62, Loss: 0.7446142435073853, Lr:0.0001\n",
      "Epoch 13, Step: 63, Loss: 0.924439013004303, Lr:0.0001\n",
      "Epoch 13, Step: 64, Loss: 1.174109935760498, Lr:0.0001\n",
      "Epoch 13, Step: 65, Loss: 1.0259567499160767, Lr:0.0001\n",
      "Epoch 13, Step: 66, Loss: 1.050686240196228, Lr:0.0001\n",
      "Epoch 13, Step: 67, Loss: 2.011054039001465, Lr:0.0001\n",
      "Epoch 13, Step: 68, Loss: 0.9581674933433533, Lr:0.0001\n",
      "Epoch 13, Step: 69, Loss: 0.5867496132850647, Lr:0.0001\n",
      "Epoch 13, Step: 70, Loss: 2.28902006149292, Lr:0.0001\n",
      "Epoch 13, Step: 71, Loss: 0.46464890241622925, Lr:0.0001\n",
      "Epoch 13, Step: 72, Loss: 1.017503023147583, Lr:0.0001\n",
      "Epoch 13, Step: 73, Loss: 0.9760186672210693, Lr:0.0001\n",
      "Epoch 13, Step: 74, Loss: 0.7637902498245239, Lr:0.0001\n",
      "Epoch 13, Step: 75, Loss: 0.7427198886871338, Lr:0.0001\n",
      "Epoch 13, Step: 76, Loss: 0.47872984409332275, Lr:0.0001\n",
      "Epoch 13, Step: 77, Loss: 0.3911762833595276, Lr:0.0001\n",
      "Epoch 13, Step: 78, Loss: 0.9475178718566895, Lr:0.0001\n",
      "Epoch 13, Step: 79, Loss: 0.8688547611236572, Lr:0.0001\n",
      "Epoch 13, Step: 80, Loss: 1.467832088470459, Lr:0.0001\n",
      "Epoch 13, Step: 81, Loss: 1.1349207162857056, Lr:0.0001\n",
      "Epoch 13, Step: 82, Loss: 1.0690155029296875, Lr:0.0001\n",
      "Epoch 13, Step: 83, Loss: 1.2836880683898926, Lr:0.0001\n",
      "Epoch 13, Step: 84, Loss: 1.3026363849639893, Lr:0.0001\n",
      "Epoch 13, Step: 85, Loss: 0.5752989649772644, Lr:0.0001\n",
      "Epoch 13, Step: 86, Loss: 1.292197585105896, Lr:0.0001\n",
      "Epoch 13, Step: 87, Loss: 0.8548524379730225, Lr:0.0001\n",
      "Epoch 13, Step: 88, Loss: 0.8378100395202637, Lr:0.0001\n",
      "Epoch 13, Step: 89, Loss: 0.7923632264137268, Lr:0.0001\n",
      "Epoch 13, Step: 90, Loss: 1.7123888731002808, Lr:0.0001\n",
      "Epoch 13, Step: 91, Loss: 0.3893386721611023, Lr:0.0001\n",
      "Epoch 13, Step: 92, Loss: 0.7369521856307983, Lr:0.0001\n",
      "Epoch 13, Step: 93, Loss: 2.2838664054870605, Lr:0.0001\n",
      "Epoch 13, Step: 94, Loss: 1.2049579620361328, Lr:0.0001\n",
      "Epoch 13, Step: 95, Loss: 0.5749088525772095, Lr:0.0001\n",
      "Epoch 13, Step: 96, Loss: 0.41084033250808716, Lr:0.0001\n",
      "Epoch 13, Step: 97, Loss: 0.8442652821540833, Lr:0.0001\n",
      "Epoch 13, Step: 98, Loss: 1.0273419618606567, Lr:0.0001\n",
      "Epoch 13, Step: 99, Loss: 0.6448756456375122, Lr:0.0001\n",
      "Epoch 13, Step: 100, Loss: 2.9131808280944824, Lr:0.0001\n",
      "Epoch 13, Step: 101, Loss: 2.2702417373657227, Lr:0.0001\n",
      "Epoch 13, Step: 102, Loss: 2.0625805854797363, Lr:0.0001\n",
      "Epoch 13, Step: 103, Loss: 0.7020155191421509, Lr:0.0001\n",
      "Epoch 13, Step: 104, Loss: 0.8951005339622498, Lr:0.0001\n",
      "Epoch 13, Step: 105, Loss: 0.4147395193576813, Lr:0.0001\n",
      "Epoch 13, Step: 106, Loss: 0.6398434042930603, Lr:0.0001\n",
      "Epoch 13, Step: 107, Loss: 1.795657992362976, Lr:0.0001\n",
      "Epoch 13, Step: 108, Loss: 0.3990606963634491, Lr:0.0001\n",
      "Epoch 13, Step: 109, Loss: 0.8667565584182739, Lr:0.0001\n",
      "Epoch 13, Step: 110, Loss: 1.4300669431686401, Lr:0.0001\n",
      "Epoch 13, Step: 111, Loss: 0.8407393097877502, Lr:0.0001\n",
      "Epoch 13, Step: 112, Loss: 0.7722154855728149, Lr:0.0001\n",
      "Epoch 13, Step: 113, Loss: 0.6332500576972961, Lr:0.0001\n",
      "Epoch 13, Step: 114, Loss: 0.1939675211906433, Lr:0.0001\n",
      "Epoch 13, Step: 115, Loss: 0.6571733951568604, Lr:0.0001\n",
      "Epoch 13, Step: 116, Loss: 0.36382460594177246, Lr:0.0001\n",
      "Epoch 13, Step: 117, Loss: 1.2212133407592773, Lr:0.0001\n",
      "Epoch 13, Step: 118, Loss: 0.31742775440216064, Lr:0.0001\n",
      "Epoch 13, Step: 119, Loss: 0.6531710028648376, Lr:0.0001\n",
      "Epoch 13, Step: 120, Loss: 0.5960695743560791, Lr:0.0001\n",
      "Epoch 13, Step: 121, Loss: 0.495474636554718, Lr:0.0001\n",
      "Epoch 13, Step: 122, Loss: 0.8437833786010742, Lr:0.0001\n",
      "Epoch 13, Step: 123, Loss: 0.809600293636322, Lr:0.0001\n",
      "Epoch 13, Step: 124, Loss: 1.0918134450912476, Lr:0.0001\n",
      "Epoch 13, Step: 125, Loss: 0.39539623260498047, Lr:0.0001\n",
      "Epoch 13, Step: 126, Loss: 1.2176564931869507, Lr:0.0001\n",
      "Epoch 13, Step: 127, Loss: 0.5863807797431946, Lr:0.0001\n",
      "Epoch 13, Step: 128, Loss: 0.4948323667049408, Lr:0.0001\n",
      "Epoch 13, Step: 129, Loss: 0.8426372408866882, Lr:0.0001\n",
      "Epoch 13, Step: 130, Loss: 0.4294617474079132, Lr:0.0001\n",
      "Epoch 13, Step: 131, Loss: 0.34992823004722595, Lr:0.0001\n",
      "Epoch 13, Step: 132, Loss: 1.0293463468551636, Lr:0.0001\n",
      "Epoch 13, Step: 133, Loss: 0.9512077569961548, Lr:0.0001\n",
      "Epoch 13, Step: 134, Loss: 0.6816805005073547, Lr:0.0001\n",
      "Epoch 13, Step: 135, Loss: 2.8915295600891113, Lr:0.0001\n",
      "Epoch 13, Step: 136, Loss: 1.3350601196289062, Lr:0.0001\n",
      "Epoch 13, Step: 137, Loss: 0.13653989136219025, Lr:0.0001\n",
      "Epoch 13, Step: 138, Loss: 0.7665901780128479, Lr:0.0001\n",
      "Epoch 13, Step: 139, Loss: 0.9638617038726807, Lr:0.0001\n",
      "Epoch 13, Step: 140, Loss: 0.8354476094245911, Lr:0.0001\n",
      "Epoch 13, Step: 141, Loss: 1.0773625373840332, Lr:0.0001\n",
      "Epoch 13, Step: 142, Loss: 1.1044694185256958, Lr:0.0001\n",
      "Epoch 13, Step: 143, Loss: 0.6018588542938232, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 13\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0298 (0.0298) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5169 (0.7187) acc1: 75.0000 (84.8485) acc5: 100.0000 (100.0000) time: 0.0065 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0067 s / it)\n",
      "* Acc@1 84.848 Acc@5 100.000 loss 0.719\n",
      "Accuracy of the network on the 33 test image: 84.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 14, Step: 0, Loss: 0.37867510318756104, Lr:0.0001\n",
      "Epoch 14, Step: 1, Loss: 0.46528246998786926, Lr:0.0001\n",
      "Epoch 14, Step: 2, Loss: 0.9736469984054565, Lr:0.0001\n",
      "Epoch 14, Step: 3, Loss: 1.9922062158584595, Lr:0.0001\n",
      "Epoch 14, Step: 4, Loss: 1.1026464700698853, Lr:0.0001\n",
      "Epoch 14, Step: 5, Loss: 0.5228034257888794, Lr:0.0001\n",
      "Epoch 14, Step: 6, Loss: 0.907213568687439, Lr:0.0001\n",
      "Epoch 14, Step: 7, Loss: 0.5626025199890137, Lr:0.0001\n",
      "Epoch 14, Step: 8, Loss: 0.44193828105926514, Lr:0.0001\n",
      "Epoch 14, Step: 9, Loss: 0.5901000499725342, Lr:0.0001\n",
      "Epoch 14, Step: 10, Loss: 0.5027401447296143, Lr:0.0001\n",
      "Epoch 14, Step: 11, Loss: 0.38884061574935913, Lr:0.0001\n",
      "Epoch 14, Step: 12, Loss: 1.3073515892028809, Lr:0.0001\n",
      "Epoch 14, Step: 13, Loss: 0.8607502579689026, Lr:0.0001\n",
      "Epoch 14, Step: 14, Loss: 0.4457762837409973, Lr:0.0001\n",
      "Epoch 14, Step: 15, Loss: 0.17867863178253174, Lr:0.0001\n",
      "Epoch 14, Step: 16, Loss: 0.9729692935943604, Lr:0.0001\n",
      "Epoch 14, Step: 17, Loss: 0.2114945650100708, Lr:0.0001\n",
      "Epoch 14, Step: 18, Loss: 1.7030160427093506, Lr:0.0001\n",
      "Epoch 14, Step: 19, Loss: 0.4714277386665344, Lr:0.0001\n",
      "Epoch 14, Step: 20, Loss: 0.5341698527336121, Lr:0.0001\n",
      "Epoch 14, Step: 21, Loss: 0.3669643998146057, Lr:0.0001\n",
      "Epoch 14, Step: 22, Loss: 1.0170084238052368, Lr:0.0001\n",
      "Epoch 14, Step: 23, Loss: 1.0390377044677734, Lr:0.0001\n",
      "Epoch 14, Step: 24, Loss: 0.20086374878883362, Lr:0.0001\n",
      "Epoch 14, Step: 25, Loss: 0.6155022382736206, Lr:0.0001\n",
      "Epoch 14, Step: 26, Loss: 0.7885264754295349, Lr:0.0001\n",
      "Epoch 14, Step: 27, Loss: 0.7037597894668579, Lr:0.0001\n",
      "Epoch 14, Step: 28, Loss: 0.6079623699188232, Lr:0.0001\n",
      "Epoch 14, Step: 29, Loss: 0.3448631465435028, Lr:0.0001\n",
      "Epoch 14, Step: 30, Loss: 2.844078540802002, Lr:0.0001\n",
      "Epoch 14, Step: 31, Loss: 0.8122252225875854, Lr:0.0001\n",
      "Epoch 14, Step: 32, Loss: 0.2984727621078491, Lr:0.0001\n",
      "Epoch 14, Step: 33, Loss: 0.5958850979804993, Lr:0.0001\n",
      "Epoch 14, Step: 34, Loss: 1.7496167421340942, Lr:0.0001\n",
      "Epoch 14, Step: 35, Loss: 1.2630023956298828, Lr:0.0001\n",
      "Epoch 14, Step: 36, Loss: 0.10603872686624527, Lr:0.0001\n",
      "Epoch 14, Step: 37, Loss: 0.7013115882873535, Lr:0.0001\n",
      "Epoch 14, Step: 38, Loss: 0.7790896892547607, Lr:0.0001\n",
      "Epoch 14, Step: 39, Loss: 1.1908931732177734, Lr:0.0001\n",
      "Epoch 14, Step: 40, Loss: 0.8617135286331177, Lr:0.0001\n",
      "Epoch 14, Step: 41, Loss: 0.4921126961708069, Lr:0.0001\n",
      "Epoch 14, Step: 42, Loss: 0.6420008540153503, Lr:0.0001\n",
      "Epoch 14, Step: 43, Loss: 1.4243865013122559, Lr:0.0001\n",
      "Epoch 14, Step: 44, Loss: 1.215676188468933, Lr:0.0001\n",
      "Epoch 14, Step: 45, Loss: 0.31720077991485596, Lr:0.0001\n",
      "Epoch 14, Step: 46, Loss: 1.180443525314331, Lr:0.0001\n",
      "Epoch 14, Step: 47, Loss: 0.9910284280776978, Lr:0.0001\n",
      "Epoch 14, Step: 48, Loss: 1.2272974252700806, Lr:0.0001\n",
      "Epoch 14, Step: 49, Loss: 1.0064191818237305, Lr:0.0001\n",
      "Epoch 14, Step: 50, Loss: 0.44477999210357666, Lr:0.0001\n",
      "Epoch 14, Step: 51, Loss: 1.59529447555542, Lr:0.0001\n",
      "Epoch 14, Step: 52, Loss: 0.4376707971096039, Lr:0.0001\n",
      "Epoch 14, Step: 53, Loss: 0.8194387555122375, Lr:0.0001\n",
      "Epoch 14, Step: 54, Loss: 0.9037773013114929, Lr:0.0001\n",
      "Epoch 14, Step: 55, Loss: 1.3706693649291992, Lr:0.0001\n",
      "Epoch 14, Step: 56, Loss: 0.8102938532829285, Lr:0.0001\n",
      "Epoch 14, Step: 57, Loss: 1.1013517379760742, Lr:0.0001\n",
      "Epoch 14, Step: 58, Loss: 1.04707670211792, Lr:0.0001\n",
      "Epoch 14, Step: 59, Loss: 0.4100533425807953, Lr:0.0001\n",
      "Epoch 14, Step: 60, Loss: 0.6603526473045349, Lr:0.0001\n",
      "Epoch 14, Step: 61, Loss: 2.005124092102051, Lr:0.0001\n",
      "Epoch 14, Step: 62, Loss: 0.7221291065216064, Lr:0.0001\n",
      "Epoch 14, Step: 63, Loss: 0.12463585287332535, Lr:0.0001\n",
      "Epoch 14, Step: 64, Loss: 1.127150297164917, Lr:0.0001\n",
      "Epoch 14, Step: 65, Loss: 1.066314697265625, Lr:0.0001\n",
      "Epoch 14, Step: 66, Loss: 0.36627957224845886, Lr:0.0001\n",
      "Epoch 14, Step: 67, Loss: 0.6154375672340393, Lr:0.0001\n",
      "Epoch 14, Step: 68, Loss: 0.996282696723938, Lr:0.0001\n",
      "Epoch 14, Step: 69, Loss: 1.0029869079589844, Lr:0.0001\n",
      "Epoch 14, Step: 70, Loss: 0.8861569762229919, Lr:0.0001\n",
      "Epoch 14, Step: 71, Loss: 0.796400785446167, Lr:0.0001\n",
      "Epoch 14, Step: 72, Loss: 0.8741832971572876, Lr:0.0001\n",
      "Epoch 14, Step: 73, Loss: 1.6545665264129639, Lr:0.0001\n",
      "Epoch 14, Step: 74, Loss: 1.059777021408081, Lr:0.0001\n",
      "Epoch 14, Step: 75, Loss: 1.483986258506775, Lr:0.0001\n",
      "Epoch 14, Step: 76, Loss: 0.8896517753601074, Lr:0.0001\n",
      "Epoch 14, Step: 77, Loss: 1.112262487411499, Lr:0.0001\n",
      "Epoch 14, Step: 78, Loss: 1.600487470626831, Lr:0.0001\n",
      "Epoch 14, Step: 79, Loss: 0.9565557837486267, Lr:0.0001\n",
      "Epoch 14, Step: 80, Loss: 0.6754459142684937, Lr:0.0001\n",
      "Epoch 14, Step: 81, Loss: 0.7762491106987, Lr:0.0001\n",
      "Epoch 14, Step: 82, Loss: 1.6951806545257568, Lr:0.0001\n",
      "Epoch 14, Step: 83, Loss: 1.2921499013900757, Lr:0.0001\n",
      "Epoch 14, Step: 84, Loss: 2.5150740146636963, Lr:0.0001\n",
      "Epoch 14, Step: 85, Loss: 0.1885305494070053, Lr:0.0001\n",
      "Epoch 14, Step: 86, Loss: 0.4577866494655609, Lr:0.0001\n",
      "Epoch 14, Step: 87, Loss: 0.8964927196502686, Lr:0.0001\n",
      "Epoch 14, Step: 88, Loss: 1.0910018682479858, Lr:0.0001\n",
      "Epoch 14, Step: 89, Loss: 1.6825823783874512, Lr:0.0001\n",
      "Epoch 14, Step: 90, Loss: 0.7576854228973389, Lr:0.0001\n",
      "Epoch 14, Step: 91, Loss: 0.7863795161247253, Lr:0.0001\n",
      "Epoch 14, Step: 92, Loss: 0.3968883454799652, Lr:0.0001\n",
      "Epoch 14, Step: 93, Loss: 0.2948073148727417, Lr:0.0001\n",
      "Epoch 14, Step: 94, Loss: 2.468752145767212, Lr:0.0001\n",
      "Epoch 14, Step: 95, Loss: 0.7942085266113281, Lr:0.0001\n",
      "Epoch 14, Step: 96, Loss: 1.1045342683792114, Lr:0.0001\n",
      "Epoch 14, Step: 97, Loss: 0.8122259974479675, Lr:0.0001\n",
      "Epoch 14, Step: 98, Loss: 0.4938631057739258, Lr:0.0001\n",
      "Epoch 14, Step: 99, Loss: 0.9488027691841125, Lr:0.0001\n",
      "Epoch 14, Step: 100, Loss: 1.0352110862731934, Lr:0.0001\n",
      "Epoch 14, Step: 101, Loss: 0.8465332984924316, Lr:0.0001\n",
      "Epoch 14, Step: 102, Loss: 0.5296996831893921, Lr:0.0001\n",
      "Epoch 14, Step: 103, Loss: 0.5544779300689697, Lr:0.0001\n",
      "Epoch 14, Step: 104, Loss: 0.6874536871910095, Lr:0.0001\n",
      "Epoch 14, Step: 105, Loss: 0.9601633548736572, Lr:0.0001\n",
      "Epoch 14, Step: 106, Loss: 0.6297403573989868, Lr:0.0001\n",
      "Epoch 14, Step: 107, Loss: 0.5925301909446716, Lr:0.0001\n",
      "Epoch 14, Step: 108, Loss: 0.33463647961616516, Lr:0.0001\n",
      "Epoch 14, Step: 109, Loss: 0.4052541255950928, Lr:0.0001\n",
      "Epoch 14, Step: 110, Loss: 0.7018585205078125, Lr:0.0001\n",
      "Epoch 14, Step: 111, Loss: 0.4833473265171051, Lr:0.0001\n",
      "Epoch 14, Step: 112, Loss: 1.2630579471588135, Lr:0.0001\n",
      "Epoch 14, Step: 113, Loss: 1.5355556011199951, Lr:0.0001\n",
      "Epoch 14, Step: 114, Loss: 0.7830671072006226, Lr:0.0001\n",
      "Epoch 14, Step: 115, Loss: 0.4658217430114746, Lr:0.0001\n",
      "Epoch 14, Step: 116, Loss: 0.7647339105606079, Lr:0.0001\n",
      "Epoch 14, Step: 117, Loss: 0.9740487933158875, Lr:0.0001\n",
      "Epoch 14, Step: 118, Loss: 0.5203951597213745, Lr:0.0001\n",
      "Epoch 14, Step: 119, Loss: 0.7601835131645203, Lr:0.0001\n",
      "Epoch 14, Step: 120, Loss: 1.0641283988952637, Lr:0.0001\n",
      "Epoch 14, Step: 121, Loss: 0.988068699836731, Lr:0.0001\n",
      "Epoch 14, Step: 122, Loss: 0.6445860862731934, Lr:0.0001\n",
      "Epoch 14, Step: 123, Loss: 1.7058031558990479, Lr:0.0001\n",
      "Epoch 14, Step: 124, Loss: 0.6031821966171265, Lr:0.0001\n",
      "Epoch 14, Step: 125, Loss: 1.2648341655731201, Lr:0.0001\n",
      "Epoch 14, Step: 126, Loss: 0.32936999201774597, Lr:0.0001\n",
      "Epoch 14, Step: 127, Loss: 0.4827541708946228, Lr:0.0001\n",
      "Epoch 14, Step: 128, Loss: 0.6474355459213257, Lr:0.0001\n",
      "Epoch 14, Step: 129, Loss: 2.356853485107422, Lr:0.0001\n",
      "Epoch 14, Step: 130, Loss: 0.8611375093460083, Lr:0.0001\n",
      "Epoch 14, Step: 131, Loss: 2.3640127182006836, Lr:0.0001\n",
      "Epoch 14, Step: 132, Loss: 0.5799658894538879, Lr:0.0001\n",
      "Epoch 14, Step: 133, Loss: 1.2349839210510254, Lr:0.0001\n",
      "Epoch 14, Step: 134, Loss: 0.45514243841171265, Lr:0.0001\n",
      "Epoch 14, Step: 135, Loss: 0.5761743783950806, Lr:0.0001\n",
      "Epoch 14, Step: 136, Loss: 0.45062246918678284, Lr:0.0001\n",
      "Epoch 14, Step: 137, Loss: 0.39853331446647644, Lr:0.0001\n",
      "Epoch 14, Step: 138, Loss: 1.1773457527160645, Lr:0.0001\n",
      "Epoch 14, Step: 139, Loss: 2.5395936965942383, Lr:0.0001\n",
      "Epoch 14, Step: 140, Loss: 0.5419709086418152, Lr:0.0001\n",
      "Epoch 14, Step: 141, Loss: 0.22002050280570984, Lr:0.0001\n",
      "Epoch 14, Step: 142, Loss: 1.201670527458191, Lr:0.0001\n",
      "Epoch 14, Step: 143, Loss: 1.172878384590149, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 14\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0283 (0.0283) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0119 data: 0.0069 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.6035 (0.9255) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0078 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0080 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 0.925\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 15, Step: 0, Loss: 1.4881641864776611, Lr:0.0001\n",
      "Epoch 15, Step: 1, Loss: 0.5186870694160461, Lr:0.0001\n",
      "Epoch 15, Step: 2, Loss: 0.9520999193191528, Lr:0.0001\n",
      "Epoch 15, Step: 3, Loss: 0.16206401586532593, Lr:0.0001\n",
      "Epoch 15, Step: 4, Loss: 0.24270886182785034, Lr:0.0001\n",
      "Epoch 15, Step: 5, Loss: 0.88850337266922, Lr:0.0001\n",
      "Epoch 15, Step: 6, Loss: 1.617666244506836, Lr:0.0001\n",
      "Epoch 15, Step: 7, Loss: 0.8970528841018677, Lr:0.0001\n",
      "Epoch 15, Step: 8, Loss: 0.23616603016853333, Lr:0.0001\n",
      "Epoch 15, Step: 9, Loss: 1.15804123878479, Lr:0.0001\n",
      "Epoch 15, Step: 10, Loss: 1.319612979888916, Lr:0.0001\n",
      "Epoch 15, Step: 11, Loss: 1.1078517436981201, Lr:0.0001\n",
      "Epoch 15, Step: 12, Loss: 0.4248194694519043, Lr:0.0001\n",
      "Epoch 15, Step: 13, Loss: 0.8000103235244751, Lr:0.0001\n",
      "Epoch 15, Step: 14, Loss: 1.2605897188186646, Lr:0.0001\n",
      "Epoch 15, Step: 15, Loss: 0.6415705680847168, Lr:0.0001\n",
      "Epoch 15, Step: 16, Loss: 0.524594247341156, Lr:0.0001\n",
      "Epoch 15, Step: 17, Loss: 0.4831150472164154, Lr:0.0001\n",
      "Epoch 15, Step: 18, Loss: 1.0939496755599976, Lr:0.0001\n",
      "Epoch 15, Step: 19, Loss: 1.3116916418075562, Lr:0.0001\n",
      "Epoch 15, Step: 20, Loss: 1.252333402633667, Lr:0.0001\n",
      "Epoch 15, Step: 21, Loss: 0.8343533277511597, Lr:0.0001\n",
      "Epoch 15, Step: 22, Loss: 0.33331260085105896, Lr:0.0001\n",
      "Epoch 15, Step: 23, Loss: 1.1760199069976807, Lr:0.0001\n",
      "Epoch 15, Step: 24, Loss: 0.5820938944816589, Lr:0.0001\n",
      "Epoch 15, Step: 25, Loss: 0.29705971479415894, Lr:0.0001\n",
      "Epoch 15, Step: 26, Loss: 0.4725537598133087, Lr:0.0001\n",
      "Epoch 15, Step: 27, Loss: 0.5593185424804688, Lr:0.0001\n",
      "Epoch 15, Step: 28, Loss: 0.7062036991119385, Lr:0.0001\n",
      "Epoch 15, Step: 29, Loss: 0.7601301670074463, Lr:0.0001\n",
      "Epoch 15, Step: 30, Loss: 0.9067188501358032, Lr:0.0001\n",
      "Epoch 15, Step: 31, Loss: 1.1808671951293945, Lr:0.0001\n",
      "Epoch 15, Step: 32, Loss: 0.9515702724456787, Lr:0.0001\n",
      "Epoch 15, Step: 33, Loss: 0.41144824028015137, Lr:0.0001\n",
      "Epoch 15, Step: 34, Loss: 0.7356102466583252, Lr:0.0001\n",
      "Epoch 15, Step: 35, Loss: 0.9787713289260864, Lr:0.0001\n",
      "Epoch 15, Step: 36, Loss: 1.0184831619262695, Lr:0.0001\n",
      "Epoch 15, Step: 37, Loss: 1.1237789392471313, Lr:0.0001\n",
      "Epoch 15, Step: 38, Loss: 0.4483794867992401, Lr:0.0001\n",
      "Epoch 15, Step: 39, Loss: 0.9738848209381104, Lr:0.0001\n",
      "Epoch 15, Step: 40, Loss: 1.2030689716339111, Lr:0.0001\n",
      "Epoch 15, Step: 41, Loss: 0.4340279698371887, Lr:0.0001\n",
      "Epoch 15, Step: 42, Loss: 0.3867286741733551, Lr:0.0001\n",
      "Epoch 15, Step: 43, Loss: 2.984628438949585, Lr:0.0001\n",
      "Epoch 15, Step: 44, Loss: 1.340277910232544, Lr:0.0001\n",
      "Epoch 15, Step: 45, Loss: 0.7184789180755615, Lr:0.0001\n",
      "Epoch 15, Step: 46, Loss: 0.535476803779602, Lr:0.0001\n",
      "Epoch 15, Step: 47, Loss: 1.161952018737793, Lr:0.0001\n",
      "Epoch 15, Step: 48, Loss: 1.2110044956207275, Lr:0.0001\n",
      "Epoch 15, Step: 49, Loss: 0.8258051872253418, Lr:0.0001\n",
      "Epoch 15, Step: 50, Loss: 0.5200482606887817, Lr:0.0001\n",
      "Epoch 15, Step: 51, Loss: 1.1863584518432617, Lr:0.0001\n",
      "Epoch 15, Step: 52, Loss: 0.413655161857605, Lr:0.0001\n",
      "Epoch 15, Step: 53, Loss: 0.35848763585090637, Lr:0.0001\n",
      "Epoch 15, Step: 54, Loss: 1.1004269123077393, Lr:0.0001\n",
      "Epoch 15, Step: 55, Loss: 0.5496597290039062, Lr:0.0001\n",
      "Epoch 15, Step: 56, Loss: 0.3834249675273895, Lr:0.0001\n",
      "Epoch 15, Step: 57, Loss: 0.3919927775859833, Lr:0.0001\n",
      "Epoch 15, Step: 58, Loss: 0.7492021918296814, Lr:0.0001\n",
      "Epoch 15, Step: 59, Loss: 0.29758504033088684, Lr:0.0001\n",
      "Epoch 15, Step: 60, Loss: 1.0431554317474365, Lr:0.0001\n",
      "Epoch 15, Step: 61, Loss: 0.8021225929260254, Lr:0.0001\n",
      "Epoch 15, Step: 62, Loss: 1.1111936569213867, Lr:0.0001\n",
      "Epoch 15, Step: 63, Loss: 0.6819459199905396, Lr:0.0001\n",
      "Epoch 15, Step: 64, Loss: 0.5789991617202759, Lr:0.0001\n",
      "Epoch 15, Step: 65, Loss: 0.420920729637146, Lr:0.0001\n",
      "Epoch 15, Step: 66, Loss: 0.7478490471839905, Lr:0.0001\n",
      "Epoch 15, Step: 67, Loss: 1.2648085355758667, Lr:0.0001\n",
      "Epoch 15, Step: 68, Loss: 0.6200243234634399, Lr:0.0001\n",
      "Epoch 15, Step: 69, Loss: 0.45302414894104004, Lr:0.0001\n",
      "Epoch 15, Step: 70, Loss: 0.98126620054245, Lr:0.0001\n",
      "Epoch 15, Step: 71, Loss: 0.18344900012016296, Lr:0.0001\n",
      "Epoch 15, Step: 72, Loss: 0.2365686595439911, Lr:0.0001\n",
      "Epoch 15, Step: 73, Loss: 0.5274351835250854, Lr:0.0001\n",
      "Epoch 15, Step: 74, Loss: 0.6421782374382019, Lr:0.0001\n",
      "Epoch 15, Step: 75, Loss: 1.313915729522705, Lr:0.0001\n",
      "Epoch 15, Step: 76, Loss: 0.6815287470817566, Lr:0.0001\n",
      "Epoch 15, Step: 77, Loss: 0.43928757309913635, Lr:0.0001\n",
      "Epoch 15, Step: 78, Loss: 0.5777735710144043, Lr:0.0001\n",
      "Epoch 15, Step: 79, Loss: 0.6415742635726929, Lr:0.0001\n",
      "Epoch 15, Step: 80, Loss: 0.7244691252708435, Lr:0.0001\n",
      "Epoch 15, Step: 81, Loss: 0.7551839351654053, Lr:0.0001\n",
      "Epoch 15, Step: 82, Loss: 0.6788880228996277, Lr:0.0001\n",
      "Epoch 15, Step: 83, Loss: 0.7192498445510864, Lr:0.0001\n",
      "Epoch 15, Step: 84, Loss: 0.614946186542511, Lr:0.0001\n",
      "Epoch 15, Step: 85, Loss: 0.2783362567424774, Lr:0.0001\n",
      "Epoch 15, Step: 86, Loss: 0.5908733606338501, Lr:0.0001\n",
      "Epoch 15, Step: 87, Loss: 2.4237656593322754, Lr:0.0001\n",
      "Epoch 15, Step: 88, Loss: 0.9487272500991821, Lr:0.0001\n",
      "Epoch 15, Step: 89, Loss: 0.5052686333656311, Lr:0.0001\n",
      "Epoch 15, Step: 90, Loss: 2.044020891189575, Lr:0.0001\n",
      "Epoch 15, Step: 91, Loss: 0.4543294310569763, Lr:0.0001\n",
      "Epoch 15, Step: 92, Loss: 1.3029232025146484, Lr:0.0001\n",
      "Epoch 15, Step: 93, Loss: 0.4027707576751709, Lr:0.0001\n",
      "Epoch 15, Step: 94, Loss: 0.44361573457717896, Lr:0.0001\n",
      "Epoch 15, Step: 95, Loss: 0.37718749046325684, Lr:0.0001\n",
      "Epoch 15, Step: 96, Loss: 1.810072422027588, Lr:0.0001\n",
      "Epoch 15, Step: 97, Loss: 0.4075421392917633, Lr:0.0001\n",
      "Epoch 15, Step: 98, Loss: 1.2435479164123535, Lr:0.0001\n",
      "Epoch 15, Step: 99, Loss: 0.8309499025344849, Lr:0.0001\n",
      "Epoch 15, Step: 100, Loss: 1.8111039400100708, Lr:0.0001\n",
      "Epoch 15, Step: 101, Loss: 0.4603201448917389, Lr:0.0001\n",
      "Epoch 15, Step: 102, Loss: 1.8945369720458984, Lr:0.0001\n",
      "Epoch 15, Step: 103, Loss: 0.4234122335910797, Lr:0.0001\n",
      "Epoch 15, Step: 104, Loss: 0.38792818784713745, Lr:0.0001\n",
      "Epoch 15, Step: 105, Loss: 1.2437119483947754, Lr:0.0001\n",
      "Epoch 15, Step: 106, Loss: 0.7570570707321167, Lr:0.0001\n",
      "Epoch 15, Step: 107, Loss: 1.171076774597168, Lr:0.0001\n",
      "Epoch 15, Step: 108, Loss: 0.6139333844184875, Lr:0.0001\n",
      "Epoch 15, Step: 109, Loss: 3.5642995834350586, Lr:0.0001\n",
      "Epoch 15, Step: 110, Loss: 0.3498852849006653, Lr:0.0001\n",
      "Epoch 15, Step: 111, Loss: 0.6888771057128906, Lr:0.0001\n",
      "Epoch 15, Step: 112, Loss: 0.6836859583854675, Lr:0.0001\n",
      "Epoch 15, Step: 113, Loss: 2.2848105430603027, Lr:0.0001\n",
      "Epoch 15, Step: 114, Loss: 2.204585552215576, Lr:0.0001\n",
      "Epoch 15, Step: 115, Loss: 0.5964279770851135, Lr:0.0001\n",
      "Epoch 15, Step: 116, Loss: 2.3388187885284424, Lr:0.0001\n",
      "Epoch 15, Step: 117, Loss: 1.064802646636963, Lr:0.0001\n",
      "Epoch 15, Step: 118, Loss: 0.9748275279998779, Lr:0.0001\n",
      "Epoch 15, Step: 119, Loss: 0.49873781204223633, Lr:0.0001\n",
      "Epoch 15, Step: 120, Loss: 1.158451795578003, Lr:0.0001\n",
      "Epoch 15, Step: 121, Loss: 1.0762770175933838, Lr:0.0001\n",
      "Epoch 15, Step: 122, Loss: 0.849986732006073, Lr:0.0001\n",
      "Epoch 15, Step: 123, Loss: 0.6338943243026733, Lr:0.0001\n",
      "Epoch 15, Step: 124, Loss: 0.8770095109939575, Lr:0.0001\n",
      "Epoch 15, Step: 125, Loss: 1.6350936889648438, Lr:0.0001\n",
      "Epoch 15, Step: 126, Loss: 0.697851836681366, Lr:0.0001\n",
      "Epoch 15, Step: 127, Loss: 0.8633792400360107, Lr:0.0001\n",
      "Epoch 15, Step: 128, Loss: 0.509016752243042, Lr:0.0001\n",
      "Epoch 15, Step: 129, Loss: 1.5423165559768677, Lr:0.0001\n",
      "Epoch 15, Step: 130, Loss: 0.2636508643627167, Lr:0.0001\n",
      "Epoch 15, Step: 131, Loss: 1.808153748512268, Lr:0.0001\n",
      "Epoch 15, Step: 132, Loss: 1.3653695583343506, Lr:0.0001\n",
      "Epoch 15, Step: 133, Loss: 0.9400067925453186, Lr:0.0001\n",
      "Epoch 15, Step: 134, Loss: 1.0656530857086182, Lr:0.0001\n",
      "Epoch 15, Step: 135, Loss: 0.8206304907798767, Lr:0.0001\n",
      "Epoch 15, Step: 136, Loss: 1.4583028554916382, Lr:0.0001\n",
      "Epoch 15, Step: 137, Loss: 0.22681432962417603, Lr:0.0001\n",
      "Epoch 15, Step: 138, Loss: 1.3466253280639648, Lr:0.0001\n",
      "Epoch 15, Step: 139, Loss: 0.6196768283843994, Lr:0.0001\n",
      "Epoch 15, Step: 140, Loss: 0.8065523505210876, Lr:0.0001\n",
      "Epoch 15, Step: 141, Loss: 0.953707218170166, Lr:0.0001\n",
      "Epoch 15, Step: 142, Loss: 1.155396819114685, Lr:0.0001\n",
      "Epoch 15, Step: 143, Loss: 0.6185096502304077, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 15\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0516 (0.0516) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.8111 (0.7003) acc1: 75.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0082 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0083 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.700\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 16, Step: 0, Loss: 0.20339305698871613, Lr:0.0001\n",
      "Epoch 16, Step: 1, Loss: 0.5389396548271179, Lr:0.0001\n",
      "Epoch 16, Step: 2, Loss: 1.4268494844436646, Lr:0.0001\n",
      "Epoch 16, Step: 3, Loss: 0.6424669623374939, Lr:0.0001\n",
      "Epoch 16, Step: 4, Loss: 0.6105594038963318, Lr:0.0001\n",
      "Epoch 16, Step: 5, Loss: 2.055823564529419, Lr:0.0001\n",
      "Epoch 16, Step: 6, Loss: 0.5899755954742432, Lr:0.0001\n",
      "Epoch 16, Step: 7, Loss: 0.8871486783027649, Lr:0.0001\n",
      "Epoch 16, Step: 8, Loss: 1.4976578950881958, Lr:0.0001\n",
      "Epoch 16, Step: 9, Loss: 0.23336592316627502, Lr:0.0001\n",
      "Epoch 16, Step: 10, Loss: 1.5284581184387207, Lr:0.0001\n",
      "Epoch 16, Step: 11, Loss: 1.342331886291504, Lr:0.0001\n",
      "Epoch 16, Step: 12, Loss: 0.7401584386825562, Lr:0.0001\n",
      "Epoch 16, Step: 13, Loss: 1.0605781078338623, Lr:0.0001\n",
      "Epoch 16, Step: 14, Loss: 0.8018821477890015, Lr:0.0001\n",
      "Epoch 16, Step: 15, Loss: 1.2455676794052124, Lr:0.0001\n",
      "Epoch 16, Step: 16, Loss: 0.21155616641044617, Lr:0.0001\n",
      "Epoch 16, Step: 17, Loss: 1.2347215414047241, Lr:0.0001\n",
      "Epoch 16, Step: 18, Loss: 0.544716477394104, Lr:0.0001\n",
      "Epoch 16, Step: 19, Loss: 0.7706454396247864, Lr:0.0001\n",
      "Epoch 16, Step: 20, Loss: 0.7311643958091736, Lr:0.0001\n",
      "Epoch 16, Step: 21, Loss: 0.35988521575927734, Lr:0.0001\n",
      "Epoch 16, Step: 22, Loss: 0.5433563590049744, Lr:0.0001\n",
      "Epoch 16, Step: 23, Loss: 1.6852781772613525, Lr:0.0001\n",
      "Epoch 16, Step: 24, Loss: 0.7193309664726257, Lr:0.0001\n",
      "Epoch 16, Step: 25, Loss: 0.1212824136018753, Lr:0.0001\n",
      "Epoch 16, Step: 26, Loss: 0.8252065181732178, Lr:0.0001\n",
      "Epoch 16, Step: 27, Loss: 1.6184539794921875, Lr:0.0001\n",
      "Epoch 16, Step: 28, Loss: 1.0071995258331299, Lr:0.0001\n",
      "Epoch 16, Step: 29, Loss: 0.8788114786148071, Lr:0.0001\n",
      "Epoch 16, Step: 30, Loss: 0.9127548933029175, Lr:0.0001\n",
      "Epoch 16, Step: 31, Loss: 0.6386829018592834, Lr:0.0001\n",
      "Epoch 16, Step: 32, Loss: 1.0127218961715698, Lr:0.0001\n",
      "Epoch 16, Step: 33, Loss: 0.5278517007827759, Lr:0.0001\n",
      "Epoch 16, Step: 34, Loss: 0.6464223861694336, Lr:0.0001\n",
      "Epoch 16, Step: 35, Loss: 1.7771929502487183, Lr:0.0001\n",
      "Epoch 16, Step: 36, Loss: 1.4377069473266602, Lr:0.0001\n",
      "Epoch 16, Step: 37, Loss: 0.5244287848472595, Lr:0.0001\n",
      "Epoch 16, Step: 38, Loss: 0.542612612247467, Lr:0.0001\n",
      "Epoch 16, Step: 39, Loss: 0.5907927751541138, Lr:0.0001\n",
      "Epoch 16, Step: 40, Loss: 1.3070942163467407, Lr:0.0001\n",
      "Epoch 16, Step: 41, Loss: 0.6880717873573303, Lr:0.0001\n",
      "Epoch 16, Step: 42, Loss: 1.4734457731246948, Lr:0.0001\n",
      "Epoch 16, Step: 43, Loss: 0.6207453608512878, Lr:0.0001\n",
      "Epoch 16, Step: 44, Loss: 0.3182581663131714, Lr:0.0001\n",
      "Epoch 16, Step: 45, Loss: 0.8036627769470215, Lr:0.0001\n",
      "Epoch 16, Step: 46, Loss: 0.20065444707870483, Lr:0.0001\n",
      "Epoch 16, Step: 47, Loss: 1.1520031690597534, Lr:0.0001\n",
      "Epoch 16, Step: 48, Loss: 0.5758243799209595, Lr:0.0001\n",
      "Epoch 16, Step: 49, Loss: 0.08950841426849365, Lr:0.0001\n",
      "Epoch 16, Step: 50, Loss: 0.62575364112854, Lr:0.0001\n",
      "Epoch 16, Step: 51, Loss: 1.777603268623352, Lr:0.0001\n",
      "Epoch 16, Step: 52, Loss: 0.9975406527519226, Lr:0.0001\n",
      "Epoch 16, Step: 53, Loss: 0.6735621094703674, Lr:0.0001\n",
      "Epoch 16, Step: 54, Loss: 0.6039660573005676, Lr:0.0001\n",
      "Epoch 16, Step: 55, Loss: 0.7211837768554688, Lr:0.0001\n",
      "Epoch 16, Step: 56, Loss: 0.7791008949279785, Lr:0.0001\n",
      "Epoch 16, Step: 57, Loss: 1.2853482961654663, Lr:0.0001\n",
      "Epoch 16, Step: 58, Loss: 0.7716007828712463, Lr:0.0001\n",
      "Epoch 16, Step: 59, Loss: 0.5956180691719055, Lr:0.0001\n",
      "Epoch 16, Step: 60, Loss: 0.908154308795929, Lr:0.0001\n",
      "Epoch 16, Step: 61, Loss: 1.1667473316192627, Lr:0.0001\n",
      "Epoch 16, Step: 62, Loss: 0.6324067711830139, Lr:0.0001\n",
      "Epoch 16, Step: 63, Loss: 1.0364211797714233, Lr:0.0001\n",
      "Epoch 16, Step: 64, Loss: 0.1583869606256485, Lr:0.0001\n",
      "Epoch 16, Step: 65, Loss: 1.3979475498199463, Lr:0.0001\n",
      "Epoch 16, Step: 66, Loss: 0.9422434568405151, Lr:0.0001\n",
      "Epoch 16, Step: 67, Loss: 1.082714319229126, Lr:0.0001\n",
      "Epoch 16, Step: 68, Loss: 1.3289822340011597, Lr:0.0001\n",
      "Epoch 16, Step: 69, Loss: 0.41394615173339844, Lr:0.0001\n",
      "Epoch 16, Step: 70, Loss: 0.7739914655685425, Lr:0.0001\n",
      "Epoch 16, Step: 71, Loss: 0.7501121759414673, Lr:0.0001\n",
      "Epoch 16, Step: 72, Loss: 0.4176122546195984, Lr:0.0001\n",
      "Epoch 16, Step: 73, Loss: 0.19279417395591736, Lr:0.0001\n",
      "Epoch 16, Step: 74, Loss: 0.7064748406410217, Lr:0.0001\n",
      "Epoch 16, Step: 75, Loss: 1.2874374389648438, Lr:0.0001\n",
      "Epoch 16, Step: 76, Loss: 0.5065140128135681, Lr:0.0001\n",
      "Epoch 16, Step: 77, Loss: 0.7671933174133301, Lr:0.0001\n",
      "Epoch 16, Step: 78, Loss: 2.3388707637786865, Lr:0.0001\n",
      "Epoch 16, Step: 79, Loss: 0.6716915369033813, Lr:0.0001\n",
      "Epoch 16, Step: 80, Loss: 0.9179643392562866, Lr:0.0001\n",
      "Epoch 16, Step: 81, Loss: 0.8136573433876038, Lr:0.0001\n",
      "Epoch 16, Step: 82, Loss: 0.6029232740402222, Lr:0.0001\n",
      "Epoch 16, Step: 83, Loss: 0.8481581211090088, Lr:0.0001\n",
      "Epoch 16, Step: 84, Loss: 0.5193576812744141, Lr:0.0001\n",
      "Epoch 16, Step: 85, Loss: 0.5570467114448547, Lr:0.0001\n",
      "Epoch 16, Step: 86, Loss: 1.9050734043121338, Lr:0.0001\n",
      "Epoch 16, Step: 87, Loss: 0.7735050916671753, Lr:0.0001\n",
      "Epoch 16, Step: 88, Loss: 0.9369316101074219, Lr:0.0001\n",
      "Epoch 16, Step: 89, Loss: 0.6139700412750244, Lr:0.0001\n",
      "Epoch 16, Step: 90, Loss: 0.1585586965084076, Lr:0.0001\n",
      "Epoch 16, Step: 91, Loss: 0.9541638493537903, Lr:0.0001\n",
      "Epoch 16, Step: 92, Loss: 1.2620162963867188, Lr:0.0001\n",
      "Epoch 16, Step: 93, Loss: 0.286805659532547, Lr:0.0001\n",
      "Epoch 16, Step: 94, Loss: 1.074367642402649, Lr:0.0001\n",
      "Epoch 16, Step: 95, Loss: 1.0551469326019287, Lr:0.0001\n",
      "Epoch 16, Step: 96, Loss: 0.4985350966453552, Lr:0.0001\n",
      "Epoch 16, Step: 97, Loss: 1.6837778091430664, Lr:0.0001\n",
      "Epoch 16, Step: 98, Loss: 0.5275968313217163, Lr:0.0001\n",
      "Epoch 16, Step: 99, Loss: 0.7922767400741577, Lr:0.0001\n",
      "Epoch 16, Step: 100, Loss: 0.8375654220581055, Lr:0.0001\n",
      "Epoch 16, Step: 101, Loss: 0.8109177947044373, Lr:0.0001\n",
      "Epoch 16, Step: 102, Loss: 0.7022557258605957, Lr:0.0001\n",
      "Epoch 16, Step: 103, Loss: 1.1746246814727783, Lr:0.0001\n",
      "Epoch 16, Step: 104, Loss: 0.6562857627868652, Lr:0.0001\n",
      "Epoch 16, Step: 105, Loss: 1.0268560647964478, Lr:0.0001\n",
      "Epoch 16, Step: 106, Loss: 1.3275954723358154, Lr:0.0001\n",
      "Epoch 16, Step: 107, Loss: 1.4364831447601318, Lr:0.0001\n",
      "Epoch 16, Step: 108, Loss: 0.9149050712585449, Lr:0.0001\n",
      "Epoch 16, Step: 109, Loss: 0.8535975217819214, Lr:0.0001\n",
      "Epoch 16, Step: 110, Loss: 0.7374299764633179, Lr:0.0001\n",
      "Epoch 16, Step: 111, Loss: 0.7239136695861816, Lr:0.0001\n",
      "Epoch 16, Step: 112, Loss: 1.0846657752990723, Lr:0.0001\n",
      "Epoch 16, Step: 113, Loss: 0.2557715177536011, Lr:0.0001\n",
      "Epoch 16, Step: 114, Loss: 0.837405800819397, Lr:0.0001\n",
      "Epoch 16, Step: 115, Loss: 2.136474609375, Lr:0.0001\n",
      "Epoch 16, Step: 116, Loss: 1.1753814220428467, Lr:0.0001\n",
      "Epoch 16, Step: 117, Loss: 0.9220337867736816, Lr:0.0001\n",
      "Epoch 16, Step: 118, Loss: 0.7510858774185181, Lr:0.0001\n",
      "Epoch 16, Step: 119, Loss: 0.3090290427207947, Lr:0.0001\n",
      "Epoch 16, Step: 120, Loss: 0.32330992817878723, Lr:0.0001\n",
      "Epoch 16, Step: 121, Loss: 1.2088062763214111, Lr:0.0001\n",
      "Epoch 16, Step: 122, Loss: 0.6843485832214355, Lr:0.0001\n",
      "Epoch 16, Step: 123, Loss: 0.4275578558444977, Lr:0.0001\n",
      "Epoch 16, Step: 124, Loss: 0.7092024683952332, Lr:0.0001\n",
      "Epoch 16, Step: 125, Loss: 1.750693917274475, Lr:0.0001\n",
      "Epoch 16, Step: 126, Loss: 1.1408034563064575, Lr:0.0001\n",
      "Epoch 16, Step: 127, Loss: 0.867719829082489, Lr:0.0001\n",
      "Epoch 16, Step: 128, Loss: 1.3119597434997559, Lr:0.0001\n",
      "Epoch 16, Step: 129, Loss: 0.9770044684410095, Lr:0.0001\n",
      "Epoch 16, Step: 130, Loss: 1.0535032749176025, Lr:0.0001\n",
      "Epoch 16, Step: 131, Loss: 0.8698502779006958, Lr:0.0001\n",
      "Epoch 16, Step: 132, Loss: 1.4154279232025146, Lr:0.0001\n",
      "Epoch 16, Step: 133, Loss: 3.4083664417266846, Lr:0.0001\n",
      "Epoch 16, Step: 134, Loss: 0.6410425901412964, Lr:0.0001\n",
      "Epoch 16, Step: 135, Loss: 0.486013263463974, Lr:0.0001\n",
      "Epoch 16, Step: 136, Loss: 1.7605011463165283, Lr:0.0001\n",
      "Epoch 16, Step: 137, Loss: 1.1329306364059448, Lr:0.0001\n",
      "Epoch 16, Step: 138, Loss: 0.5019968748092651, Lr:0.0001\n",
      "Epoch 16, Step: 139, Loss: 0.6092766523361206, Lr:0.0001\n",
      "Epoch 16, Step: 140, Loss: 1.7066881656646729, Lr:0.0001\n",
      "Epoch 16, Step: 141, Loss: 0.15836673974990845, Lr:0.0001\n",
      "Epoch 16, Step: 142, Loss: 2.3349125385284424, Lr:0.0001\n",
      "Epoch 16, Step: 143, Loss: 0.4684451222419739, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 16\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0688 (0.0688) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0120 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4536 (0.6436) acc1: 100.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0090 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0090 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.644\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 17, Step: 0, Loss: 1.8905296325683594, Lr:0.0001\n",
      "Epoch 17, Step: 1, Loss: 1.0118677616119385, Lr:0.0001\n",
      "Epoch 17, Step: 2, Loss: 0.3654046654701233, Lr:0.0001\n",
      "Epoch 17, Step: 3, Loss: 0.8450453281402588, Lr:0.0001\n",
      "Epoch 17, Step: 4, Loss: 0.7140102386474609, Lr:0.0001\n",
      "Epoch 17, Step: 5, Loss: 0.47392988204956055, Lr:0.0001\n",
      "Epoch 17, Step: 6, Loss: 0.6695536375045776, Lr:0.0001\n",
      "Epoch 17, Step: 7, Loss: 1.3822298049926758, Lr:0.0001\n",
      "Epoch 17, Step: 8, Loss: 0.7537643909454346, Lr:0.0001\n",
      "Epoch 17, Step: 9, Loss: 0.32297664880752563, Lr:0.0001\n",
      "Epoch 17, Step: 10, Loss: 0.6811139583587646, Lr:0.0001\n",
      "Epoch 17, Step: 11, Loss: 0.31058332324028015, Lr:0.0001\n",
      "Epoch 17, Step: 12, Loss: 0.5091008543968201, Lr:0.0001\n",
      "Epoch 17, Step: 13, Loss: 0.8795391321182251, Lr:0.0001\n",
      "Epoch 17, Step: 14, Loss: 0.46730199456214905, Lr:0.0001\n",
      "Epoch 17, Step: 15, Loss: 0.7036053538322449, Lr:0.0001\n",
      "Epoch 17, Step: 16, Loss: 0.6983282566070557, Lr:0.0001\n",
      "Epoch 17, Step: 17, Loss: 0.9304515719413757, Lr:0.0001\n",
      "Epoch 17, Step: 18, Loss: 0.6035509705543518, Lr:0.0001\n",
      "Epoch 17, Step: 19, Loss: 0.9741448760032654, Lr:0.0001\n",
      "Epoch 17, Step: 20, Loss: 1.210078477859497, Lr:0.0001\n",
      "Epoch 17, Step: 21, Loss: 1.2586958408355713, Lr:0.0001\n",
      "Epoch 17, Step: 22, Loss: 0.4571545124053955, Lr:0.0001\n",
      "Epoch 17, Step: 23, Loss: 0.45748552680015564, Lr:0.0001\n",
      "Epoch 17, Step: 24, Loss: 0.2088339924812317, Lr:0.0001\n",
      "Epoch 17, Step: 25, Loss: 0.4224603772163391, Lr:0.0001\n",
      "Epoch 17, Step: 26, Loss: 0.8882226943969727, Lr:0.0001\n",
      "Epoch 17, Step: 27, Loss: 1.00716233253479, Lr:0.0001\n",
      "Epoch 17, Step: 28, Loss: 0.5731639862060547, Lr:0.0001\n",
      "Epoch 17, Step: 29, Loss: 0.928532063961029, Lr:0.0001\n",
      "Epoch 17, Step: 30, Loss: 0.7370578646659851, Lr:0.0001\n",
      "Epoch 17, Step: 31, Loss: 0.44760408997535706, Lr:0.0001\n",
      "Epoch 17, Step: 32, Loss: 1.4005478620529175, Lr:0.0001\n",
      "Epoch 17, Step: 33, Loss: 0.5911895632743835, Lr:0.0001\n",
      "Epoch 17, Step: 34, Loss: 0.18714866042137146, Lr:0.0001\n",
      "Epoch 17, Step: 35, Loss: 1.172614574432373, Lr:0.0001\n",
      "Epoch 17, Step: 36, Loss: 0.7390018105506897, Lr:0.0001\n",
      "Epoch 17, Step: 37, Loss: 0.5004780292510986, Lr:0.0001\n",
      "Epoch 17, Step: 38, Loss: 1.1822301149368286, Lr:0.0001\n",
      "Epoch 17, Step: 39, Loss: 0.3434205949306488, Lr:0.0001\n",
      "Epoch 17, Step: 40, Loss: 1.378388524055481, Lr:0.0001\n",
      "Epoch 17, Step: 41, Loss: 0.5206831097602844, Lr:0.0001\n",
      "Epoch 17, Step: 42, Loss: 0.9706808924674988, Lr:0.0001\n",
      "Epoch 17, Step: 43, Loss: 1.0870076417922974, Lr:0.0001\n",
      "Epoch 17, Step: 44, Loss: 0.5446272492408752, Lr:0.0001\n",
      "Epoch 17, Step: 45, Loss: 0.6538960933685303, Lr:0.0001\n",
      "Epoch 17, Step: 46, Loss: 0.2535628378391266, Lr:0.0001\n",
      "Epoch 17, Step: 47, Loss: 1.6019034385681152, Lr:0.0001\n",
      "Epoch 17, Step: 48, Loss: 0.7808626294136047, Lr:0.0001\n",
      "Epoch 17, Step: 49, Loss: 1.5818910598754883, Lr:0.0001\n",
      "Epoch 17, Step: 50, Loss: 0.802405834197998, Lr:0.0001\n",
      "Epoch 17, Step: 51, Loss: 0.5253193974494934, Lr:0.0001\n",
      "Epoch 17, Step: 52, Loss: 1.218882441520691, Lr:0.0001\n",
      "Epoch 17, Step: 53, Loss: 0.09531722217798233, Lr:0.0001\n",
      "Epoch 17, Step: 54, Loss: 2.225613832473755, Lr:0.0001\n",
      "Epoch 17, Step: 55, Loss: 0.8851051926612854, Lr:0.0001\n",
      "Epoch 17, Step: 56, Loss: 2.015833616256714, Lr:0.0001\n",
      "Epoch 17, Step: 57, Loss: 0.923829972743988, Lr:0.0001\n",
      "Epoch 17, Step: 58, Loss: 1.1311135292053223, Lr:0.0001\n",
      "Epoch 17, Step: 59, Loss: 0.9727810025215149, Lr:0.0001\n",
      "Epoch 17, Step: 60, Loss: 0.2753015160560608, Lr:0.0001\n",
      "Epoch 17, Step: 61, Loss: 0.4812487065792084, Lr:0.0001\n",
      "Epoch 17, Step: 62, Loss: 0.6962913274765015, Lr:0.0001\n",
      "Epoch 17, Step: 63, Loss: 0.5196171998977661, Lr:0.0001\n",
      "Epoch 17, Step: 64, Loss: 0.4365086257457733, Lr:0.0001\n",
      "Epoch 17, Step: 65, Loss: 0.4828909635543823, Lr:0.0001\n",
      "Epoch 17, Step: 66, Loss: 2.157392978668213, Lr:0.0001\n",
      "Epoch 17, Step: 67, Loss: 1.037427544593811, Lr:0.0001\n",
      "Epoch 17, Step: 68, Loss: 1.7304673194885254, Lr:0.0001\n",
      "Epoch 17, Step: 69, Loss: 1.3469183444976807, Lr:0.0001\n",
      "Epoch 17, Step: 70, Loss: 0.4406093955039978, Lr:0.0001\n",
      "Epoch 17, Step: 71, Loss: 0.33101099729537964, Lr:0.0001\n",
      "Epoch 17, Step: 72, Loss: 1.4654443264007568, Lr:0.0001\n",
      "Epoch 17, Step: 73, Loss: 1.012253761291504, Lr:0.0001\n",
      "Epoch 17, Step: 74, Loss: 0.7363763451576233, Lr:0.0001\n",
      "Epoch 17, Step: 75, Loss: 1.366636872291565, Lr:0.0001\n",
      "Epoch 17, Step: 76, Loss: 1.4324090480804443, Lr:0.0001\n",
      "Epoch 17, Step: 77, Loss: 1.4532698392868042, Lr:0.0001\n",
      "Epoch 17, Step: 78, Loss: 0.5829009413719177, Lr:0.0001\n",
      "Epoch 17, Step: 79, Loss: 1.1781179904937744, Lr:0.0001\n",
      "Epoch 17, Step: 80, Loss: 1.5835025310516357, Lr:0.0001\n",
      "Epoch 17, Step: 81, Loss: 2.122976779937744, Lr:0.0001\n",
      "Epoch 17, Step: 82, Loss: 0.777752697467804, Lr:0.0001\n",
      "Epoch 17, Step: 83, Loss: 1.3523341417312622, Lr:0.0001\n",
      "Epoch 17, Step: 84, Loss: 0.5481272339820862, Lr:0.0001\n",
      "Epoch 17, Step: 85, Loss: 0.9981418251991272, Lr:0.0001\n",
      "Epoch 17, Step: 86, Loss: 0.7047469019889832, Lr:0.0001\n",
      "Epoch 17, Step: 87, Loss: 0.9481903910636902, Lr:0.0001\n",
      "Epoch 17, Step: 88, Loss: 0.501567006111145, Lr:0.0001\n",
      "Epoch 17, Step: 89, Loss: 0.643430233001709, Lr:0.0001\n",
      "Epoch 17, Step: 90, Loss: 0.7789197564125061, Lr:0.0001\n",
      "Epoch 17, Step: 91, Loss: 0.48421454429626465, Lr:0.0001\n",
      "Epoch 17, Step: 92, Loss: 0.6482971906661987, Lr:0.0001\n",
      "Epoch 17, Step: 93, Loss: 0.5955570936203003, Lr:0.0001\n",
      "Epoch 17, Step: 94, Loss: 1.1629767417907715, Lr:0.0001\n",
      "Epoch 17, Step: 95, Loss: 0.7736204862594604, Lr:0.0001\n",
      "Epoch 17, Step: 96, Loss: 0.6583304405212402, Lr:0.0001\n",
      "Epoch 17, Step: 97, Loss: 0.3026147782802582, Lr:0.0001\n",
      "Epoch 17, Step: 98, Loss: 0.6245829463005066, Lr:0.0001\n",
      "Epoch 17, Step: 99, Loss: 1.7322970628738403, Lr:0.0001\n",
      "Epoch 17, Step: 100, Loss: 0.8281594514846802, Lr:0.0001\n",
      "Epoch 17, Step: 101, Loss: 0.3447486162185669, Lr:0.0001\n",
      "Epoch 17, Step: 102, Loss: 0.8849291205406189, Lr:0.0001\n",
      "Epoch 17, Step: 103, Loss: 0.4384576082229614, Lr:0.0001\n",
      "Epoch 17, Step: 104, Loss: 1.4710583686828613, Lr:0.0001\n",
      "Epoch 17, Step: 105, Loss: 0.4566728174686432, Lr:0.0001\n",
      "Epoch 17, Step: 106, Loss: 0.942787766456604, Lr:0.0001\n",
      "Epoch 17, Step: 107, Loss: 2.0913941860198975, Lr:0.0001\n",
      "Epoch 17, Step: 108, Loss: 0.4482068419456482, Lr:0.0001\n",
      "Epoch 17, Step: 109, Loss: 0.7132220268249512, Lr:0.0001\n",
      "Epoch 17, Step: 110, Loss: 0.09848693013191223, Lr:0.0001\n",
      "Epoch 17, Step: 111, Loss: 0.47395291924476624, Lr:0.0001\n",
      "Epoch 17, Step: 112, Loss: 0.6702477335929871, Lr:0.0001\n",
      "Epoch 17, Step: 113, Loss: 0.5502380728721619, Lr:0.0001\n",
      "Epoch 17, Step: 114, Loss: 0.9131720066070557, Lr:0.0001\n",
      "Epoch 17, Step: 115, Loss: 0.9385499954223633, Lr:0.0001\n",
      "Epoch 17, Step: 116, Loss: 0.5853707194328308, Lr:0.0001\n",
      "Epoch 17, Step: 117, Loss: 2.7472622394561768, Lr:0.0001\n",
      "Epoch 17, Step: 118, Loss: 0.4840146601200104, Lr:0.0001\n",
      "Epoch 17, Step: 119, Loss: 0.7392133474349976, Lr:0.0001\n",
      "Epoch 17, Step: 120, Loss: 1.0882489681243896, Lr:0.0001\n",
      "Epoch 17, Step: 121, Loss: 0.9331576228141785, Lr:0.0001\n",
      "Epoch 17, Step: 122, Loss: 0.7856153249740601, Lr:0.0001\n",
      "Epoch 17, Step: 123, Loss: 1.2838064432144165, Lr:0.0001\n",
      "Epoch 17, Step: 124, Loss: 1.0089784860610962, Lr:0.0001\n",
      "Epoch 17, Step: 125, Loss: 0.6925727725028992, Lr:0.0001\n",
      "Epoch 17, Step: 126, Loss: 0.8984225988388062, Lr:0.0001\n",
      "Epoch 17, Step: 127, Loss: 0.7063891291618347, Lr:0.0001\n",
      "Epoch 17, Step: 128, Loss: 0.29579150676727295, Lr:0.0001\n",
      "Epoch 17, Step: 129, Loss: 0.5436859726905823, Lr:0.0001\n",
      "Epoch 17, Step: 130, Loss: 0.7993288040161133, Lr:0.0001\n",
      "Epoch 17, Step: 131, Loss: 1.1184306144714355, Lr:0.0001\n",
      "Epoch 17, Step: 132, Loss: 0.6685737371444702, Lr:0.0001\n",
      "Epoch 17, Step: 133, Loss: 1.2414674758911133, Lr:0.0001\n",
      "Epoch 17, Step: 134, Loss: 0.17176081240177155, Lr:0.0001\n",
      "Epoch 17, Step: 135, Loss: 0.5822067260742188, Lr:0.0001\n",
      "Epoch 17, Step: 136, Loss: 1.6651071310043335, Lr:0.0001\n",
      "Epoch 17, Step: 137, Loss: 0.5738474130630493, Lr:0.0001\n",
      "Epoch 17, Step: 138, Loss: 1.2866785526275635, Lr:0.0001\n",
      "Epoch 17, Step: 139, Loss: 0.6684950590133667, Lr:0.0001\n",
      "Epoch 17, Step: 140, Loss: 0.8219236135482788, Lr:0.0001\n",
      "Epoch 17, Step: 141, Loss: 0.9037518501281738, Lr:0.0001\n",
      "Epoch 17, Step: 142, Loss: 1.2607386112213135, Lr:0.0001\n",
      "Epoch 17, Step: 143, Loss: 0.4471092224121094, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 17\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.2462 (0.2462) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0110 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7276 (0.7640) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0071 data: 0.0033 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0072 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 0.764\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 18, Step: 0, Loss: 0.8065800070762634, Lr:0.0001\n",
      "Epoch 18, Step: 1, Loss: 0.5688841342926025, Lr:0.0001\n",
      "Epoch 18, Step: 2, Loss: 0.9041866660118103, Lr:0.0001\n",
      "Epoch 18, Step: 3, Loss: 0.9073471426963806, Lr:0.0001\n",
      "Epoch 18, Step: 4, Loss: 0.6719571352005005, Lr:0.0001\n",
      "Epoch 18, Step: 5, Loss: 0.7295217514038086, Lr:0.0001\n",
      "Epoch 18, Step: 6, Loss: 0.6107404232025146, Lr:0.0001\n",
      "Epoch 18, Step: 7, Loss: 0.4269298017024994, Lr:0.0001\n",
      "Epoch 18, Step: 8, Loss: 0.49250108003616333, Lr:0.0001\n",
      "Epoch 18, Step: 9, Loss: 0.7376606464385986, Lr:0.0001\n",
      "Epoch 18, Step: 10, Loss: 0.2703244388103485, Lr:0.0001\n",
      "Epoch 18, Step: 11, Loss: 0.5233379006385803, Lr:0.0001\n",
      "Epoch 18, Step: 12, Loss: 0.48676297068595886, Lr:0.0001\n",
      "Epoch 18, Step: 13, Loss: 0.6737185716629028, Lr:0.0001\n",
      "Epoch 18, Step: 14, Loss: 0.4431478977203369, Lr:0.0001\n",
      "Epoch 18, Step: 15, Loss: 1.0881388187408447, Lr:0.0001\n",
      "Epoch 18, Step: 16, Loss: 0.6590943336486816, Lr:0.0001\n",
      "Epoch 18, Step: 17, Loss: 0.8258629441261292, Lr:0.0001\n",
      "Epoch 18, Step: 18, Loss: 0.05306164175271988, Lr:0.0001\n",
      "Epoch 18, Step: 19, Loss: 3.169039249420166, Lr:0.0001\n",
      "Epoch 18, Step: 20, Loss: 0.7251166105270386, Lr:0.0001\n",
      "Epoch 18, Step: 21, Loss: 1.4409067630767822, Lr:0.0001\n",
      "Epoch 18, Step: 22, Loss: 1.0813392400741577, Lr:0.0001\n",
      "Epoch 18, Step: 23, Loss: 0.603816032409668, Lr:0.0001\n",
      "Epoch 18, Step: 24, Loss: 0.5634948015213013, Lr:0.0001\n",
      "Epoch 18, Step: 25, Loss: 0.3860166370868683, Lr:0.0001\n",
      "Epoch 18, Step: 26, Loss: 0.47542697191238403, Lr:0.0001\n",
      "Epoch 18, Step: 27, Loss: 0.9687207341194153, Lr:0.0001\n",
      "Epoch 18, Step: 28, Loss: 0.7659083604812622, Lr:0.0001\n",
      "Epoch 18, Step: 29, Loss: 1.8973281383514404, Lr:0.0001\n",
      "Epoch 18, Step: 30, Loss: 0.5863044261932373, Lr:0.0001\n",
      "Epoch 18, Step: 31, Loss: 0.9686803221702576, Lr:0.0001\n",
      "Epoch 18, Step: 32, Loss: 1.1664741039276123, Lr:0.0001\n",
      "Epoch 18, Step: 33, Loss: 0.5969942212104797, Lr:0.0001\n",
      "Epoch 18, Step: 34, Loss: 0.3909989297389984, Lr:0.0001\n",
      "Epoch 18, Step: 35, Loss: 1.4628710746765137, Lr:0.0001\n",
      "Epoch 18, Step: 36, Loss: 1.5298113822937012, Lr:0.0001\n",
      "Epoch 18, Step: 37, Loss: 0.7609154582023621, Lr:0.0001\n",
      "Epoch 18, Step: 38, Loss: 0.3401554822921753, Lr:0.0001\n",
      "Epoch 18, Step: 39, Loss: 1.3400570154190063, Lr:0.0001\n",
      "Epoch 18, Step: 40, Loss: 1.1804187297821045, Lr:0.0001\n",
      "Epoch 18, Step: 41, Loss: 0.9829390048980713, Lr:0.0001\n",
      "Epoch 18, Step: 42, Loss: 0.7562133073806763, Lr:0.0001\n",
      "Epoch 18, Step: 43, Loss: 0.6611916422843933, Lr:0.0001\n",
      "Epoch 18, Step: 44, Loss: 0.3352590799331665, Lr:0.0001\n",
      "Epoch 18, Step: 45, Loss: 0.7030391097068787, Lr:0.0001\n",
      "Epoch 18, Step: 46, Loss: 0.41221776604652405, Lr:0.0001\n",
      "Epoch 18, Step: 47, Loss: 0.23432320356369019, Lr:0.0001\n",
      "Epoch 18, Step: 48, Loss: 0.7933617234230042, Lr:0.0001\n",
      "Epoch 18, Step: 49, Loss: 1.4557836055755615, Lr:0.0001\n",
      "Epoch 18, Step: 50, Loss: 0.3871910572052002, Lr:0.0001\n",
      "Epoch 18, Step: 51, Loss: 0.47179120779037476, Lr:0.0001\n",
      "Epoch 18, Step: 52, Loss: 0.46115991473197937, Lr:0.0001\n",
      "Epoch 18, Step: 53, Loss: 1.0699046850204468, Lr:0.0001\n",
      "Epoch 18, Step: 54, Loss: 0.5512315630912781, Lr:0.0001\n",
      "Epoch 18, Step: 55, Loss: 2.094869613647461, Lr:0.0001\n",
      "Epoch 18, Step: 56, Loss: 0.6480224132537842, Lr:0.0001\n",
      "Epoch 18, Step: 57, Loss: 0.5873717665672302, Lr:0.0001\n",
      "Epoch 18, Step: 58, Loss: 0.5274091958999634, Lr:0.0001\n",
      "Epoch 18, Step: 59, Loss: 0.2814890146255493, Lr:0.0001\n",
      "Epoch 18, Step: 60, Loss: 2.0031917095184326, Lr:0.0001\n",
      "Epoch 18, Step: 61, Loss: 0.33683377504348755, Lr:0.0001\n",
      "Epoch 18, Step: 62, Loss: 0.2276833951473236, Lr:0.0001\n",
      "Epoch 18, Step: 63, Loss: 1.1946830749511719, Lr:0.0001\n",
      "Epoch 18, Step: 64, Loss: 1.0110666751861572, Lr:0.0001\n",
      "Epoch 18, Step: 65, Loss: 0.9312241077423096, Lr:0.0001\n",
      "Epoch 18, Step: 66, Loss: 0.6720703840255737, Lr:0.0001\n",
      "Epoch 18, Step: 67, Loss: 0.43944668769836426, Lr:0.0001\n",
      "Epoch 18, Step: 68, Loss: 1.4506986141204834, Lr:0.0001\n",
      "Epoch 18, Step: 69, Loss: 0.8582445383071899, Lr:0.0001\n",
      "Epoch 18, Step: 70, Loss: 1.457715392112732, Lr:0.0001\n",
      "Epoch 18, Step: 71, Loss: 0.44752103090286255, Lr:0.0001\n",
      "Epoch 18, Step: 72, Loss: 2.287977695465088, Lr:0.0001\n",
      "Epoch 18, Step: 73, Loss: 1.357332706451416, Lr:0.0001\n",
      "Epoch 18, Step: 74, Loss: 1.2295784950256348, Lr:0.0001\n",
      "Epoch 18, Step: 75, Loss: 1.8696367740631104, Lr:0.0001\n",
      "Epoch 18, Step: 76, Loss: 0.5614345669746399, Lr:0.0001\n",
      "Epoch 18, Step: 77, Loss: 1.1252539157867432, Lr:0.0001\n",
      "Epoch 18, Step: 78, Loss: 0.10314334183931351, Lr:0.0001\n",
      "Epoch 18, Step: 79, Loss: 0.8001299500465393, Lr:0.0001\n",
      "Epoch 18, Step: 80, Loss: 0.2771604359149933, Lr:0.0001\n",
      "Epoch 18, Step: 81, Loss: 0.5491802096366882, Lr:0.0001\n",
      "Epoch 18, Step: 82, Loss: 0.8069075345993042, Lr:0.0001\n",
      "Epoch 18, Step: 83, Loss: 1.0464168787002563, Lr:0.0001\n",
      "Epoch 18, Step: 84, Loss: 0.40451863408088684, Lr:0.0001\n",
      "Epoch 18, Step: 85, Loss: 1.0786672830581665, Lr:0.0001\n",
      "Epoch 18, Step: 86, Loss: 0.5485031604766846, Lr:0.0001\n",
      "Epoch 18, Step: 87, Loss: 1.262425184249878, Lr:0.0001\n",
      "Epoch 18, Step: 88, Loss: 2.34368896484375, Lr:0.0001\n",
      "Epoch 18, Step: 89, Loss: 0.29132768511772156, Lr:0.0001\n",
      "Epoch 18, Step: 90, Loss: 0.5631329417228699, Lr:0.0001\n",
      "Epoch 18, Step: 91, Loss: 0.8114643096923828, Lr:0.0001\n",
      "Epoch 18, Step: 92, Loss: 1.911691665649414, Lr:0.0001\n",
      "Epoch 18, Step: 93, Loss: 0.8259426355361938, Lr:0.0001\n",
      "Epoch 18, Step: 94, Loss: 0.07701025903224945, Lr:0.0001\n",
      "Epoch 18, Step: 95, Loss: 1.00666344165802, Lr:0.0001\n",
      "Epoch 18, Step: 96, Loss: 0.7448928356170654, Lr:0.0001\n",
      "Epoch 18, Step: 97, Loss: 1.3896207809448242, Lr:0.0001\n",
      "Epoch 18, Step: 98, Loss: 0.3118126392364502, Lr:0.0001\n",
      "Epoch 18, Step: 99, Loss: 0.6042422652244568, Lr:0.0001\n",
      "Epoch 18, Step: 100, Loss: 1.5382235050201416, Lr:0.0001\n",
      "Epoch 18, Step: 101, Loss: 0.7702860236167908, Lr:0.0001\n",
      "Epoch 18, Step: 102, Loss: 0.7023241519927979, Lr:0.0001\n",
      "Epoch 18, Step: 103, Loss: 0.7308025360107422, Lr:0.0001\n",
      "Epoch 18, Step: 104, Loss: 1.1472474336624146, Lr:0.0001\n",
      "Epoch 18, Step: 105, Loss: 0.6563920974731445, Lr:0.0001\n",
      "Epoch 18, Step: 106, Loss: 0.6173564195632935, Lr:0.0001\n",
      "Epoch 18, Step: 107, Loss: 0.4626730680465698, Lr:0.0001\n",
      "Epoch 18, Step: 108, Loss: 0.694297194480896, Lr:0.0001\n",
      "Epoch 18, Step: 109, Loss: 0.5126182436943054, Lr:0.0001\n",
      "Epoch 18, Step: 110, Loss: 0.5440360307693481, Lr:0.0001\n",
      "Epoch 18, Step: 111, Loss: 1.1646301746368408, Lr:0.0001\n",
      "Epoch 18, Step: 112, Loss: 0.7666079998016357, Lr:0.0001\n",
      "Epoch 18, Step: 113, Loss: 0.778373658657074, Lr:0.0001\n",
      "Epoch 18, Step: 114, Loss: 0.5287577509880066, Lr:0.0001\n",
      "Epoch 18, Step: 115, Loss: 0.20000359416007996, Lr:0.0001\n",
      "Epoch 18, Step: 116, Loss: 1.0875444412231445, Lr:0.0001\n",
      "Epoch 18, Step: 117, Loss: 0.6842853426933289, Lr:0.0001\n",
      "Epoch 18, Step: 118, Loss: 1.1538569927215576, Lr:0.0001\n",
      "Epoch 18, Step: 119, Loss: 0.7454029321670532, Lr:0.0001\n",
      "Epoch 18, Step: 120, Loss: 0.38205739855766296, Lr:0.0001\n",
      "Epoch 18, Step: 121, Loss: 0.6194831132888794, Lr:0.0001\n",
      "Epoch 18, Step: 122, Loss: 0.13485383987426758, Lr:0.0001\n",
      "Epoch 18, Step: 123, Loss: 0.7479519844055176, Lr:0.0001\n",
      "Epoch 18, Step: 124, Loss: 0.4750022888183594, Lr:0.0001\n",
      "Epoch 18, Step: 125, Loss: 0.6709543466567993, Lr:0.0001\n",
      "Epoch 18, Step: 126, Loss: 0.946613073348999, Lr:0.0001\n",
      "Epoch 18, Step: 127, Loss: 0.5667524933815002, Lr:0.0001\n",
      "Epoch 18, Step: 128, Loss: 0.6546873450279236, Lr:0.0001\n",
      "Epoch 18, Step: 129, Loss: 1.413534164428711, Lr:0.0001\n",
      "Epoch 18, Step: 130, Loss: 1.1803860664367676, Lr:0.0001\n",
      "Epoch 18, Step: 131, Loss: 1.007668137550354, Lr:0.0001\n",
      "Epoch 18, Step: 132, Loss: 1.8608684539794922, Lr:0.0001\n",
      "Epoch 18, Step: 133, Loss: 2.0281224250793457, Lr:0.0001\n",
      "Epoch 18, Step: 134, Loss: 1.1187525987625122, Lr:0.0001\n",
      "Epoch 18, Step: 135, Loss: 1.2713383436203003, Lr:0.0001\n",
      "Epoch 18, Step: 136, Loss: 0.7953701019287109, Lr:0.0001\n",
      "Epoch 18, Step: 137, Loss: 0.5337570309638977, Lr:0.0001\n",
      "Epoch 18, Step: 138, Loss: 0.9046475887298584, Lr:0.0001\n",
      "Epoch 18, Step: 139, Loss: 1.120182752609253, Lr:0.0001\n",
      "Epoch 18, Step: 140, Loss: 0.29525431990623474, Lr:0.0001\n",
      "Epoch 18, Step: 141, Loss: 0.6953535676002502, Lr:0.0001\n",
      "Epoch 18, Step: 142, Loss: 0.8337458372116089, Lr:0.0001\n",
      "Epoch 18, Step: 143, Loss: 1.4277511835098267, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 18\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0276 (0.0276) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0109 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7695 (0.8837) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0080 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0081 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 0.884\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 19, Step: 0, Loss: 0.31278297305107117, Lr:0.0001\n",
      "Epoch 19, Step: 1, Loss: 1.8479455709457397, Lr:0.0001\n",
      "Epoch 19, Step: 2, Loss: 0.7587774395942688, Lr:0.0001\n",
      "Epoch 19, Step: 3, Loss: 1.818179726600647, Lr:0.0001\n",
      "Epoch 19, Step: 4, Loss: 0.7835630178451538, Lr:0.0001\n",
      "Epoch 19, Step: 5, Loss: 0.8274148106575012, Lr:0.0001\n",
      "Epoch 19, Step: 6, Loss: 1.6367199420928955, Lr:0.0001\n",
      "Epoch 19, Step: 7, Loss: 0.4546012878417969, Lr:0.0001\n",
      "Epoch 19, Step: 8, Loss: 1.4999778270721436, Lr:0.0001\n",
      "Epoch 19, Step: 9, Loss: 1.1561388969421387, Lr:0.0001\n",
      "Epoch 19, Step: 10, Loss: 0.31212010979652405, Lr:0.0001\n",
      "Epoch 19, Step: 11, Loss: 0.10129857063293457, Lr:0.0001\n",
      "Epoch 19, Step: 12, Loss: 0.7073233127593994, Lr:0.0001\n",
      "Epoch 19, Step: 13, Loss: 0.799367368221283, Lr:0.0001\n",
      "Epoch 19, Step: 14, Loss: 0.9632540941238403, Lr:0.0001\n",
      "Epoch 19, Step: 15, Loss: 0.5098984241485596, Lr:0.0001\n",
      "Epoch 19, Step: 16, Loss: 1.2666442394256592, Lr:0.0001\n",
      "Epoch 19, Step: 17, Loss: 0.5424861907958984, Lr:0.0001\n",
      "Epoch 19, Step: 18, Loss: 1.1678043603897095, Lr:0.0001\n",
      "Epoch 19, Step: 19, Loss: 0.8462741374969482, Lr:0.0001\n",
      "Epoch 19, Step: 20, Loss: 0.6343047618865967, Lr:0.0001\n",
      "Epoch 19, Step: 21, Loss: 0.6270302534103394, Lr:0.0001\n",
      "Epoch 19, Step: 22, Loss: 0.39817267656326294, Lr:0.0001\n",
      "Epoch 19, Step: 23, Loss: 1.3643372058868408, Lr:0.0001\n",
      "Epoch 19, Step: 24, Loss: 0.8335230946540833, Lr:0.0001\n",
      "Epoch 19, Step: 25, Loss: 0.952712893486023, Lr:0.0001\n",
      "Epoch 19, Step: 26, Loss: 0.1697029173374176, Lr:0.0001\n",
      "Epoch 19, Step: 27, Loss: 0.6976158618927002, Lr:0.0001\n",
      "Epoch 19, Step: 28, Loss: 0.4934905171394348, Lr:0.0001\n",
      "Epoch 19, Step: 29, Loss: 0.7830633521080017, Lr:0.0001\n",
      "Epoch 19, Step: 30, Loss: 0.299161434173584, Lr:0.0001\n",
      "Epoch 19, Step: 31, Loss: 1.754754662513733, Lr:0.0001\n",
      "Epoch 19, Step: 32, Loss: 0.2390601485967636, Lr:0.0001\n",
      "Epoch 19, Step: 33, Loss: 0.5196335315704346, Lr:0.0001\n",
      "Epoch 19, Step: 34, Loss: 0.30808669328689575, Lr:0.0001\n",
      "Epoch 19, Step: 35, Loss: 0.6723967790603638, Lr:0.0001\n",
      "Epoch 19, Step: 36, Loss: 0.4760392904281616, Lr:0.0001\n",
      "Epoch 19, Step: 37, Loss: 0.40441983938217163, Lr:0.0001\n",
      "Epoch 19, Step: 38, Loss: 0.44438374042510986, Lr:0.0001\n",
      "Epoch 19, Step: 39, Loss: 1.330655574798584, Lr:0.0001\n",
      "Epoch 19, Step: 40, Loss: 0.8573271632194519, Lr:0.0001\n",
      "Epoch 19, Step: 41, Loss: 0.56818687915802, Lr:0.0001\n",
      "Epoch 19, Step: 42, Loss: 1.1633869409561157, Lr:0.0001\n",
      "Epoch 19, Step: 43, Loss: 1.2259801626205444, Lr:0.0001\n",
      "Epoch 19, Step: 44, Loss: 1.6697609424591064, Lr:0.0001\n",
      "Epoch 19, Step: 45, Loss: 1.1678004264831543, Lr:0.0001\n",
      "Epoch 19, Step: 46, Loss: 0.8047295212745667, Lr:0.0001\n",
      "Epoch 19, Step: 47, Loss: 0.582523763179779, Lr:0.0001\n",
      "Epoch 19, Step: 48, Loss: 0.8959620594978333, Lr:0.0001\n",
      "Epoch 19, Step: 49, Loss: 1.5321640968322754, Lr:0.0001\n",
      "Epoch 19, Step: 50, Loss: 0.8017348051071167, Lr:0.0001\n",
      "Epoch 19, Step: 51, Loss: 1.229020357131958, Lr:0.0001\n",
      "Epoch 19, Step: 52, Loss: 0.9738708734512329, Lr:0.0001\n",
      "Epoch 19, Step: 53, Loss: 0.28065869212150574, Lr:0.0001\n",
      "Epoch 19, Step: 54, Loss: 0.29534363746643066, Lr:0.0001\n",
      "Epoch 19, Step: 55, Loss: 0.9512786269187927, Lr:0.0001\n",
      "Epoch 19, Step: 56, Loss: 0.2289709597826004, Lr:0.0001\n",
      "Epoch 19, Step: 57, Loss: 0.2549958825111389, Lr:0.0001\n",
      "Epoch 19, Step: 58, Loss: 0.8556920289993286, Lr:0.0001\n",
      "Epoch 19, Step: 59, Loss: 1.0869202613830566, Lr:0.0001\n",
      "Epoch 19, Step: 60, Loss: 1.2725281715393066, Lr:0.0001\n",
      "Epoch 19, Step: 61, Loss: 0.5158641338348389, Lr:0.0001\n",
      "Epoch 19, Step: 62, Loss: 1.172568440437317, Lr:0.0001\n",
      "Epoch 19, Step: 63, Loss: 0.0769709125161171, Lr:0.0001\n",
      "Epoch 19, Step: 64, Loss: 0.35548385977745056, Lr:0.0001\n",
      "Epoch 19, Step: 65, Loss: 1.7025763988494873, Lr:0.0001\n",
      "Epoch 19, Step: 66, Loss: 0.9076017737388611, Lr:0.0001\n",
      "Epoch 19, Step: 67, Loss: 0.610306978225708, Lr:0.0001\n",
      "Epoch 19, Step: 68, Loss: 0.19858881831169128, Lr:0.0001\n",
      "Epoch 19, Step: 69, Loss: 0.6673866510391235, Lr:0.0001\n",
      "Epoch 19, Step: 70, Loss: 0.7178226113319397, Lr:0.0001\n",
      "Epoch 19, Step: 71, Loss: 0.28661128878593445, Lr:0.0001\n",
      "Epoch 19, Step: 72, Loss: 2.340200901031494, Lr:0.0001\n",
      "Epoch 19, Step: 73, Loss: 0.3695480227470398, Lr:0.0001\n",
      "Epoch 19, Step: 74, Loss: 1.3118408918380737, Lr:0.0001\n",
      "Epoch 19, Step: 75, Loss: 0.9789000153541565, Lr:0.0001\n",
      "Epoch 19, Step: 76, Loss: 0.09567679464817047, Lr:0.0001\n",
      "Epoch 19, Step: 77, Loss: 0.4352732300758362, Lr:0.0001\n",
      "Epoch 19, Step: 78, Loss: 1.8007376194000244, Lr:0.0001\n",
      "Epoch 19, Step: 79, Loss: 0.0848182961344719, Lr:0.0001\n",
      "Epoch 19, Step: 80, Loss: 0.30719390511512756, Lr:0.0001\n",
      "Epoch 19, Step: 81, Loss: 0.942164421081543, Lr:0.0001\n",
      "Epoch 19, Step: 82, Loss: 1.171735405921936, Lr:0.0001\n",
      "Epoch 19, Step: 83, Loss: 0.3802219331264496, Lr:0.0001\n",
      "Epoch 19, Step: 84, Loss: 0.76298987865448, Lr:0.0001\n",
      "Epoch 19, Step: 85, Loss: 0.6485471129417419, Lr:0.0001\n",
      "Epoch 19, Step: 86, Loss: 0.8311727643013, Lr:0.0001\n",
      "Epoch 19, Step: 87, Loss: 0.2847188711166382, Lr:0.0001\n",
      "Epoch 19, Step: 88, Loss: 1.617632508277893, Lr:0.0001\n",
      "Epoch 19, Step: 89, Loss: 0.17843779921531677, Lr:0.0001\n",
      "Epoch 19, Step: 90, Loss: 0.7825222611427307, Lr:0.0001\n",
      "Epoch 19, Step: 91, Loss: 1.0648281574249268, Lr:0.0001\n",
      "Epoch 19, Step: 92, Loss: 0.9244190454483032, Lr:0.0001\n",
      "Epoch 19, Step: 93, Loss: 0.8724060654640198, Lr:0.0001\n",
      "Epoch 19, Step: 94, Loss: 0.6656278371810913, Lr:0.0001\n",
      "Epoch 19, Step: 95, Loss: 0.6516942977905273, Lr:0.0001\n",
      "Epoch 19, Step: 96, Loss: 1.2026042938232422, Lr:0.0001\n",
      "Epoch 19, Step: 97, Loss: 0.5818250179290771, Lr:0.0001\n",
      "Epoch 19, Step: 98, Loss: 0.8985030055046082, Lr:0.0001\n",
      "Epoch 19, Step: 99, Loss: 0.5164922475814819, Lr:0.0001\n",
      "Epoch 19, Step: 100, Loss: 0.5340232849121094, Lr:0.0001\n",
      "Epoch 19, Step: 101, Loss: 0.7501208782196045, Lr:0.0001\n",
      "Epoch 19, Step: 102, Loss: 0.2374010980129242, Lr:0.0001\n",
      "Epoch 19, Step: 103, Loss: 0.7187936902046204, Lr:0.0001\n",
      "Epoch 19, Step: 104, Loss: 0.9452258348464966, Lr:0.0001\n",
      "Epoch 19, Step: 105, Loss: 1.153306484222412, Lr:0.0001\n",
      "Epoch 19, Step: 106, Loss: 1.3995816707611084, Lr:0.0001\n",
      "Epoch 19, Step: 107, Loss: 0.5081598162651062, Lr:0.0001\n",
      "Epoch 19, Step: 108, Loss: 1.446821928024292, Lr:0.0001\n",
      "Epoch 19, Step: 109, Loss: 0.6669140458106995, Lr:0.0001\n",
      "Epoch 19, Step: 110, Loss: 1.581156849861145, Lr:0.0001\n",
      "Epoch 19, Step: 111, Loss: 1.353813648223877, Lr:0.0001\n",
      "Epoch 19, Step: 112, Loss: 0.7923526763916016, Lr:0.0001\n",
      "Epoch 19, Step: 113, Loss: 0.5059170722961426, Lr:0.0001\n",
      "Epoch 19, Step: 114, Loss: 0.66115802526474, Lr:0.0001\n",
      "Epoch 19, Step: 115, Loss: 0.5164331793785095, Lr:0.0001\n",
      "Epoch 19, Step: 116, Loss: 0.8524376153945923, Lr:0.0001\n",
      "Epoch 19, Step: 117, Loss: 0.7302017211914062, Lr:0.0001\n",
      "Epoch 19, Step: 118, Loss: 0.6508484482765198, Lr:0.0001\n",
      "Epoch 19, Step: 119, Loss: 0.485390305519104, Lr:0.0001\n",
      "Epoch 19, Step: 120, Loss: 0.7161499261856079, Lr:0.0001\n",
      "Epoch 19, Step: 121, Loss: 0.059778764843940735, Lr:0.0001\n",
      "Epoch 19, Step: 122, Loss: 1.4655159711837769, Lr:0.0001\n",
      "Epoch 19, Step: 123, Loss: 0.7273435592651367, Lr:0.0001\n",
      "Epoch 19, Step: 124, Loss: 1.6546235084533691, Lr:0.0001\n",
      "Epoch 19, Step: 125, Loss: 0.7064703702926636, Lr:0.0001\n",
      "Epoch 19, Step: 126, Loss: 0.5602376461029053, Lr:0.0001\n",
      "Epoch 19, Step: 127, Loss: 0.808641254901886, Lr:0.0001\n",
      "Epoch 19, Step: 128, Loss: 0.5017026662826538, Lr:0.0001\n",
      "Epoch 19, Step: 129, Loss: 0.40067601203918457, Lr:0.0001\n",
      "Epoch 19, Step: 130, Loss: 0.6316380500793457, Lr:0.0001\n",
      "Epoch 19, Step: 131, Loss: 0.8169031739234924, Lr:0.0001\n",
      "Epoch 19, Step: 132, Loss: 0.6839303374290466, Lr:0.0001\n",
      "Epoch 19, Step: 133, Loss: 2.7215585708618164, Lr:0.0001\n",
      "Epoch 19, Step: 134, Loss: 0.8502625823020935, Lr:0.0001\n",
      "Epoch 19, Step: 135, Loss: 0.3647836148738861, Lr:0.0001\n",
      "Epoch 19, Step: 136, Loss: 0.763949990272522, Lr:0.0001\n",
      "Epoch 19, Step: 137, Loss: 0.3657762110233307, Lr:0.0001\n",
      "Epoch 19, Step: 138, Loss: 0.5980056524276733, Lr:0.0001\n",
      "Epoch 19, Step: 139, Loss: 0.33643481135368347, Lr:0.0001\n",
      "Epoch 19, Step: 140, Loss: 1.1424634456634521, Lr:0.0001\n",
      "Epoch 19, Step: 141, Loss: 1.0270196199417114, Lr:0.0001\n",
      "Epoch 19, Step: 142, Loss: 1.0252320766448975, Lr:0.0001\n",
      "Epoch 19, Step: 143, Loss: 1.0035243034362793, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 19\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.3361 (0.3361) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0111 data: 0.0050 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 1.0536 (0.9786) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0088 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0088 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 0.979\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 20, Step: 0, Loss: 0.45841479301452637, Lr:0.0001\n",
      "Epoch 20, Step: 1, Loss: 0.21919360756874084, Lr:0.0001\n",
      "Epoch 20, Step: 2, Loss: 0.6000929474830627, Lr:0.0001\n",
      "Epoch 20, Step: 3, Loss: 0.9466167688369751, Lr:0.0001\n",
      "Epoch 20, Step: 4, Loss: 0.7557920813560486, Lr:0.0001\n",
      "Epoch 20, Step: 5, Loss: 1.1223915815353394, Lr:0.0001\n",
      "Epoch 20, Step: 6, Loss: 0.07965458929538727, Lr:0.0001\n",
      "Epoch 20, Step: 7, Loss: 0.17615903913974762, Lr:0.0001\n",
      "Epoch 20, Step: 8, Loss: 0.3963061273097992, Lr:0.0001\n",
      "Epoch 20, Step: 9, Loss: 0.2687300443649292, Lr:0.0001\n",
      "Epoch 20, Step: 10, Loss: 0.4520833492279053, Lr:0.0001\n",
      "Epoch 20, Step: 11, Loss: 0.7315533757209778, Lr:0.0001\n",
      "Epoch 20, Step: 12, Loss: 0.3353921175003052, Lr:0.0001\n",
      "Epoch 20, Step: 13, Loss: 0.9259416460990906, Lr:0.0001\n",
      "Epoch 20, Step: 14, Loss: 0.4386548399925232, Lr:0.0001\n",
      "Epoch 20, Step: 15, Loss: 0.3383800685405731, Lr:0.0001\n",
      "Epoch 20, Step: 16, Loss: 0.8507310748100281, Lr:0.0001\n",
      "Epoch 20, Step: 17, Loss: 1.0597519874572754, Lr:0.0001\n",
      "Epoch 20, Step: 18, Loss: 2.197685480117798, Lr:0.0001\n",
      "Epoch 20, Step: 19, Loss: 0.942624568939209, Lr:0.0001\n",
      "Epoch 20, Step: 20, Loss: 0.8840811848640442, Lr:0.0001\n",
      "Epoch 20, Step: 21, Loss: 0.7010427713394165, Lr:0.0001\n",
      "Epoch 20, Step: 22, Loss: 0.2586594820022583, Lr:0.0001\n",
      "Epoch 20, Step: 23, Loss: 1.0391273498535156, Lr:0.0001\n",
      "Epoch 20, Step: 24, Loss: 0.5563380122184753, Lr:0.0001\n",
      "Epoch 20, Step: 25, Loss: 0.2939702570438385, Lr:0.0001\n",
      "Epoch 20, Step: 26, Loss: 0.7662636041641235, Lr:0.0001\n",
      "Epoch 20, Step: 27, Loss: 0.2543485164642334, Lr:0.0001\n",
      "Epoch 20, Step: 28, Loss: 0.13779854774475098, Lr:0.0001\n",
      "Epoch 20, Step: 29, Loss: 0.754374086856842, Lr:0.0001\n",
      "Epoch 20, Step: 30, Loss: 0.3105009198188782, Lr:0.0001\n",
      "Epoch 20, Step: 31, Loss: 2.5897397994995117, Lr:0.0001\n",
      "Epoch 20, Step: 32, Loss: 1.1693910360336304, Lr:0.0001\n",
      "Epoch 20, Step: 33, Loss: 1.1015244722366333, Lr:0.0001\n",
      "Epoch 20, Step: 34, Loss: 0.40254974365234375, Lr:0.0001\n",
      "Epoch 20, Step: 35, Loss: 1.114128589630127, Lr:0.0001\n",
      "Epoch 20, Step: 36, Loss: 0.15187452733516693, Lr:0.0001\n",
      "Epoch 20, Step: 37, Loss: 1.461532711982727, Lr:0.0001\n",
      "Epoch 20, Step: 38, Loss: 0.443968266248703, Lr:0.0001\n",
      "Epoch 20, Step: 39, Loss: 0.12694208323955536, Lr:0.0001\n",
      "Epoch 20, Step: 40, Loss: 0.4145631492137909, Lr:0.0001\n",
      "Epoch 20, Step: 41, Loss: 0.9094306230545044, Lr:0.0001\n",
      "Epoch 20, Step: 42, Loss: 0.8142073154449463, Lr:0.0001\n",
      "Epoch 20, Step: 43, Loss: 1.5914404392242432, Lr:0.0001\n",
      "Epoch 20, Step: 44, Loss: 0.7827600240707397, Lr:0.0001\n",
      "Epoch 20, Step: 45, Loss: 1.2919065952301025, Lr:0.0001\n",
      "Epoch 20, Step: 46, Loss: 0.7148230075836182, Lr:0.0001\n",
      "Epoch 20, Step: 47, Loss: 1.86383056640625, Lr:0.0001\n",
      "Epoch 20, Step: 48, Loss: 1.728988528251648, Lr:0.0001\n",
      "Epoch 20, Step: 49, Loss: 1.5423829555511475, Lr:0.0001\n",
      "Epoch 20, Step: 50, Loss: 0.3707735240459442, Lr:0.0001\n",
      "Epoch 20, Step: 51, Loss: 0.15777264535427094, Lr:0.0001\n",
      "Epoch 20, Step: 52, Loss: 2.29856014251709, Lr:0.0001\n",
      "Epoch 20, Step: 53, Loss: 0.9170283675193787, Lr:0.0001\n",
      "Epoch 20, Step: 54, Loss: 0.14468127489089966, Lr:0.0001\n",
      "Epoch 20, Step: 55, Loss: 0.7468151450157166, Lr:0.0001\n",
      "Epoch 20, Step: 56, Loss: 0.973642110824585, Lr:0.0001\n",
      "Epoch 20, Step: 57, Loss: 0.344765305519104, Lr:0.0001\n",
      "Epoch 20, Step: 58, Loss: 0.760568380355835, Lr:0.0001\n",
      "Epoch 20, Step: 59, Loss: 0.7847129702568054, Lr:0.0001\n",
      "Epoch 20, Step: 60, Loss: 0.26379191875457764, Lr:0.0001\n",
      "Epoch 20, Step: 61, Loss: 2.7567813396453857, Lr:0.0001\n",
      "Epoch 20, Step: 62, Loss: 0.7429279088973999, Lr:0.0001\n",
      "Epoch 20, Step: 63, Loss: 0.789788007736206, Lr:0.0001\n",
      "Epoch 20, Step: 64, Loss: 0.4686264395713806, Lr:0.0001\n",
      "Epoch 20, Step: 65, Loss: 1.117021083831787, Lr:0.0001\n",
      "Epoch 20, Step: 66, Loss: 0.8279166221618652, Lr:0.0001\n",
      "Epoch 20, Step: 67, Loss: 0.8936799764633179, Lr:0.0001\n",
      "Epoch 20, Step: 68, Loss: 2.1149942874908447, Lr:0.0001\n",
      "Epoch 20, Step: 69, Loss: 1.1385564804077148, Lr:0.0001\n",
      "Epoch 20, Step: 70, Loss: 0.20614935457706451, Lr:0.0001\n",
      "Epoch 20, Step: 71, Loss: 0.9736964702606201, Lr:0.0001\n",
      "Epoch 20, Step: 72, Loss: 0.4664608836174011, Lr:0.0001\n",
      "Epoch 20, Step: 73, Loss: 1.594021201133728, Lr:0.0001\n",
      "Epoch 20, Step: 74, Loss: 1.1361076831817627, Lr:0.0001\n",
      "Epoch 20, Step: 75, Loss: 0.5953300595283508, Lr:0.0001\n",
      "Epoch 20, Step: 76, Loss: 0.4048619568347931, Lr:0.0001\n",
      "Epoch 20, Step: 77, Loss: 0.45021042227745056, Lr:0.0001\n",
      "Epoch 20, Step: 78, Loss: 0.2943331003189087, Lr:0.0001\n",
      "Epoch 20, Step: 79, Loss: 0.28112342953681946, Lr:0.0001\n",
      "Epoch 20, Step: 80, Loss: 0.5616082549095154, Lr:0.0001\n",
      "Epoch 20, Step: 81, Loss: 0.48883718252182007, Lr:0.0001\n",
      "Epoch 20, Step: 82, Loss: 0.3755844831466675, Lr:0.0001\n",
      "Epoch 20, Step: 83, Loss: 0.4140489399433136, Lr:0.0001\n",
      "Epoch 20, Step: 84, Loss: 0.49772652983665466, Lr:0.0001\n",
      "Epoch 20, Step: 85, Loss: 1.1793522834777832, Lr:0.0001\n",
      "Epoch 20, Step: 86, Loss: 1.8291468620300293, Lr:0.0001\n",
      "Epoch 20, Step: 87, Loss: 0.5690794587135315, Lr:0.0001\n",
      "Epoch 20, Step: 88, Loss: 0.38403213024139404, Lr:0.0001\n",
      "Epoch 20, Step: 89, Loss: 0.6377123594284058, Lr:0.0001\n",
      "Epoch 20, Step: 90, Loss: 1.4531420469284058, Lr:0.0001\n",
      "Epoch 20, Step: 91, Loss: 0.45537325739860535, Lr:0.0001\n",
      "Epoch 20, Step: 92, Loss: 0.4211518168449402, Lr:0.0001\n",
      "Epoch 20, Step: 93, Loss: 0.7454700469970703, Lr:0.0001\n",
      "Epoch 20, Step: 94, Loss: 0.3025251030921936, Lr:0.0001\n",
      "Epoch 20, Step: 95, Loss: 1.1577577590942383, Lr:0.0001\n",
      "Epoch 20, Step: 96, Loss: 1.0572856664657593, Lr:0.0001\n",
      "Epoch 20, Step: 97, Loss: 0.20835018157958984, Lr:0.0001\n",
      "Epoch 20, Step: 98, Loss: 0.7385183572769165, Lr:0.0001\n",
      "Epoch 20, Step: 99, Loss: 0.7430039048194885, Lr:0.0001\n",
      "Epoch 20, Step: 100, Loss: 1.635594129562378, Lr:0.0001\n",
      "Epoch 20, Step: 101, Loss: 1.282059907913208, Lr:0.0001\n",
      "Epoch 20, Step: 102, Loss: 0.6460723280906677, Lr:0.0001\n",
      "Epoch 20, Step: 103, Loss: 0.5533564686775208, Lr:0.0001\n",
      "Epoch 20, Step: 104, Loss: 0.6162576675415039, Lr:0.0001\n",
      "Epoch 20, Step: 105, Loss: 0.856078028678894, Lr:0.0001\n",
      "Epoch 20, Step: 106, Loss: 0.840100884437561, Lr:0.0001\n",
      "Epoch 20, Step: 107, Loss: 0.7417787909507751, Lr:0.0001\n",
      "Epoch 20, Step: 108, Loss: 0.6545225381851196, Lr:0.0001\n",
      "Epoch 20, Step: 109, Loss: 0.8725295066833496, Lr:0.0001\n",
      "Epoch 20, Step: 110, Loss: 0.5738148093223572, Lr:0.0001\n",
      "Epoch 20, Step: 111, Loss: 1.1922378540039062, Lr:0.0001\n",
      "Epoch 20, Step: 112, Loss: 2.064199447631836, Lr:0.0001\n",
      "Epoch 20, Step: 113, Loss: 0.42884087562561035, Lr:0.0001\n",
      "Epoch 20, Step: 114, Loss: 0.6516461968421936, Lr:0.0001\n",
      "Epoch 20, Step: 115, Loss: 1.8246760368347168, Lr:0.0001\n",
      "Epoch 20, Step: 116, Loss: 0.3443014621734619, Lr:0.0001\n",
      "Epoch 20, Step: 117, Loss: 0.772711992263794, Lr:0.0001\n",
      "Epoch 20, Step: 118, Loss: 0.499339759349823, Lr:0.0001\n",
      "Epoch 20, Step: 119, Loss: 0.6001513004302979, Lr:0.0001\n",
      "Epoch 20, Step: 120, Loss: 0.5663636922836304, Lr:0.0001\n",
      "Epoch 20, Step: 121, Loss: 0.8622480630874634, Lr:0.0001\n",
      "Epoch 20, Step: 122, Loss: 1.4995430707931519, Lr:0.0001\n",
      "Epoch 20, Step: 123, Loss: 1.559662938117981, Lr:0.0001\n",
      "Epoch 20, Step: 124, Loss: 0.45267626643180847, Lr:0.0001\n",
      "Epoch 20, Step: 125, Loss: 0.6222977042198181, Lr:0.0001\n",
      "Epoch 20, Step: 126, Loss: 0.6026096343994141, Lr:0.0001\n",
      "Epoch 20, Step: 127, Loss: 0.600776195526123, Lr:0.0001\n",
      "Epoch 20, Step: 128, Loss: 2.550734519958496, Lr:0.0001\n",
      "Epoch 20, Step: 129, Loss: 1.14838707447052, Lr:0.0001\n",
      "Epoch 20, Step: 130, Loss: 0.6972965002059937, Lr:0.0001\n",
      "Epoch 20, Step: 131, Loss: 0.5435845851898193, Lr:0.0001\n",
      "Epoch 20, Step: 132, Loss: 0.6083059906959534, Lr:0.0001\n",
      "Epoch 20, Step: 133, Loss: 0.5282767415046692, Lr:0.0001\n",
      "Epoch 20, Step: 134, Loss: 0.3012545108795166, Lr:0.0001\n",
      "Epoch 20, Step: 135, Loss: 1.1860377788543701, Lr:0.0001\n",
      "Epoch 20, Step: 136, Loss: 0.9111061692237854, Lr:0.0001\n",
      "Epoch 20, Step: 137, Loss: 1.2259166240692139, Lr:0.0001\n",
      "Epoch 20, Step: 138, Loss: 0.7315149307250977, Lr:0.0001\n",
      "Epoch 20, Step: 139, Loss: 0.42841923236846924, Lr:0.0001\n",
      "Epoch 20, Step: 140, Loss: 1.0191231966018677, Lr:0.0001\n",
      "Epoch 20, Step: 141, Loss: 0.4157710671424866, Lr:0.0001\n",
      "Epoch 20, Step: 142, Loss: 0.3925887942314148, Lr:0.0001\n",
      "Epoch 20, Step: 143, Loss: 1.3609473705291748, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 20\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.2030 (0.2030) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0091 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5208 (0.8827) acc1: 75.0000 (63.6364) acc5: 100.0000 (100.0000) time: 0.0081 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0082 s / it)\n",
      "* Acc@1 63.636 Acc@5 100.000 loss 0.883\n",
      "Accuracy of the network on the 33 test image: 63.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 21, Step: 0, Loss: 0.9957748651504517, Lr:0.0001\n",
      "Epoch 21, Step: 1, Loss: 0.7308671474456787, Lr:0.0001\n",
      "Epoch 21, Step: 2, Loss: 0.2838118076324463, Lr:0.0001\n",
      "Epoch 21, Step: 3, Loss: 0.5239883661270142, Lr:0.0001\n",
      "Epoch 21, Step: 4, Loss: 0.48815709352493286, Lr:0.0001\n",
      "Epoch 21, Step: 5, Loss: 0.5113121271133423, Lr:0.0001\n",
      "Epoch 21, Step: 6, Loss: 2.3816421031951904, Lr:0.0001\n",
      "Epoch 21, Step: 7, Loss: 0.6244440078735352, Lr:0.0001\n",
      "Epoch 21, Step: 8, Loss: 0.14635217189788818, Lr:0.0001\n",
      "Epoch 21, Step: 9, Loss: 0.9618723392486572, Lr:0.0001\n",
      "Epoch 21, Step: 10, Loss: 0.18846876919269562, Lr:0.0001\n",
      "Epoch 21, Step: 11, Loss: 0.800159215927124, Lr:0.0001\n",
      "Epoch 21, Step: 12, Loss: 1.179792881011963, Lr:0.0001\n",
      "Epoch 21, Step: 13, Loss: 0.3529714345932007, Lr:0.0001\n",
      "Epoch 21, Step: 14, Loss: 0.8309094309806824, Lr:0.0001\n",
      "Epoch 21, Step: 15, Loss: 0.6658369302749634, Lr:0.0001\n",
      "Epoch 21, Step: 16, Loss: 0.9119657278060913, Lr:0.0001\n",
      "Epoch 21, Step: 17, Loss: 0.5110960006713867, Lr:0.0001\n",
      "Epoch 21, Step: 18, Loss: 0.20608186721801758, Lr:0.0001\n",
      "Epoch 21, Step: 19, Loss: 0.41670501232147217, Lr:0.0001\n",
      "Epoch 21, Step: 20, Loss: 0.9261589050292969, Lr:0.0001\n",
      "Epoch 21, Step: 21, Loss: 0.9909149408340454, Lr:0.0001\n",
      "Epoch 21, Step: 22, Loss: 0.5726739764213562, Lr:0.0001\n",
      "Epoch 21, Step: 23, Loss: 0.8627347946166992, Lr:0.0001\n",
      "Epoch 21, Step: 24, Loss: 1.5676757097244263, Lr:0.0001\n",
      "Epoch 21, Step: 25, Loss: 1.1849606037139893, Lr:0.0001\n",
      "Epoch 21, Step: 26, Loss: 1.0744385719299316, Lr:0.0001\n",
      "Epoch 21, Step: 27, Loss: 1.7079578638076782, Lr:0.0001\n",
      "Epoch 21, Step: 28, Loss: 0.9450506567955017, Lr:0.0001\n",
      "Epoch 21, Step: 29, Loss: 1.0215470790863037, Lr:0.0001\n",
      "Epoch 21, Step: 30, Loss: 0.9186762571334839, Lr:0.0001\n",
      "Epoch 21, Step: 31, Loss: 0.7671473026275635, Lr:0.0001\n",
      "Epoch 21, Step: 32, Loss: 0.7321892976760864, Lr:0.0001\n",
      "Epoch 21, Step: 33, Loss: 0.23786737024784088, Lr:0.0001\n",
      "Epoch 21, Step: 34, Loss: 0.4797099530696869, Lr:0.0001\n",
      "Epoch 21, Step: 35, Loss: 2.256765842437744, Lr:0.0001\n",
      "Epoch 21, Step: 36, Loss: 1.1902165412902832, Lr:0.0001\n",
      "Epoch 21, Step: 37, Loss: 0.45288676023483276, Lr:0.0001\n",
      "Epoch 21, Step: 38, Loss: 0.26915550231933594, Lr:0.0001\n",
      "Epoch 21, Step: 39, Loss: 0.15133383870124817, Lr:0.0001\n",
      "Epoch 21, Step: 40, Loss: 0.8176972270011902, Lr:0.0001\n",
      "Epoch 21, Step: 41, Loss: 0.6418601274490356, Lr:0.0001\n",
      "Epoch 21, Step: 42, Loss: 0.647551953792572, Lr:0.0001\n",
      "Epoch 21, Step: 43, Loss: 0.6156911849975586, Lr:0.0001\n",
      "Epoch 21, Step: 44, Loss: 0.21144331991672516, Lr:0.0001\n",
      "Epoch 21, Step: 45, Loss: 1.0963538885116577, Lr:0.0001\n",
      "Epoch 21, Step: 46, Loss: 0.43878865242004395, Lr:0.0001\n",
      "Epoch 21, Step: 47, Loss: 1.435225009918213, Lr:0.0001\n",
      "Epoch 21, Step: 48, Loss: 0.9784732460975647, Lr:0.0001\n",
      "Epoch 21, Step: 49, Loss: 0.8990355730056763, Lr:0.0001\n",
      "Epoch 21, Step: 50, Loss: 0.4244561493396759, Lr:0.0001\n",
      "Epoch 21, Step: 51, Loss: 0.21141678094863892, Lr:0.0001\n",
      "Epoch 21, Step: 52, Loss: 1.4468696117401123, Lr:0.0001\n",
      "Epoch 21, Step: 53, Loss: 1.1247508525848389, Lr:0.0001\n",
      "Epoch 21, Step: 54, Loss: 0.8663263916969299, Lr:0.0001\n",
      "Epoch 21, Step: 55, Loss: 0.8248293995857239, Lr:0.0001\n",
      "Epoch 21, Step: 56, Loss: 0.7701882123947144, Lr:0.0001\n",
      "Epoch 21, Step: 57, Loss: 1.616560697555542, Lr:0.0001\n",
      "Epoch 21, Step: 58, Loss: 0.20072762668132782, Lr:0.0001\n",
      "Epoch 21, Step: 59, Loss: 0.4740157127380371, Lr:0.0001\n",
      "Epoch 21, Step: 60, Loss: 0.6491080522537231, Lr:0.0001\n",
      "Epoch 21, Step: 61, Loss: 0.6589449644088745, Lr:0.0001\n",
      "Epoch 21, Step: 62, Loss: 0.7802112698554993, Lr:0.0001\n",
      "Epoch 21, Step: 63, Loss: 1.3525090217590332, Lr:0.0001\n",
      "Epoch 21, Step: 64, Loss: 0.4831535518169403, Lr:0.0001\n",
      "Epoch 21, Step: 65, Loss: 0.6195991039276123, Lr:0.0001\n",
      "Epoch 21, Step: 66, Loss: 0.5741053223609924, Lr:0.0001\n",
      "Epoch 21, Step: 67, Loss: 1.187737226486206, Lr:0.0001\n",
      "Epoch 21, Step: 68, Loss: 0.6012178063392639, Lr:0.0001\n",
      "Epoch 21, Step: 69, Loss: 0.4252256751060486, Lr:0.0001\n",
      "Epoch 21, Step: 70, Loss: 0.3540359139442444, Lr:0.0001\n",
      "Epoch 21, Step: 71, Loss: 0.404703289270401, Lr:0.0001\n",
      "Epoch 21, Step: 72, Loss: 0.5789494514465332, Lr:0.0001\n",
      "Epoch 21, Step: 73, Loss: 0.15975189208984375, Lr:0.0001\n",
      "Epoch 21, Step: 74, Loss: 1.4685533046722412, Lr:0.0001\n",
      "Epoch 21, Step: 75, Loss: 0.49360179901123047, Lr:0.0001\n",
      "Epoch 21, Step: 76, Loss: 0.6363082528114319, Lr:0.0001\n",
      "Epoch 21, Step: 77, Loss: 0.8569617867469788, Lr:0.0001\n",
      "Epoch 21, Step: 78, Loss: 0.8528468608856201, Lr:0.0001\n",
      "Epoch 21, Step: 79, Loss: 1.4308832883834839, Lr:0.0001\n",
      "Epoch 21, Step: 80, Loss: 1.2125046253204346, Lr:0.0001\n",
      "Epoch 21, Step: 81, Loss: 0.5612003803253174, Lr:0.0001\n",
      "Epoch 21, Step: 82, Loss: 0.6412457227706909, Lr:0.0001\n",
      "Epoch 21, Step: 83, Loss: 0.117538221180439, Lr:0.0001\n",
      "Epoch 21, Step: 84, Loss: 0.6994773149490356, Lr:0.0001\n",
      "Epoch 21, Step: 85, Loss: 0.8951030969619751, Lr:0.0001\n",
      "Epoch 21, Step: 86, Loss: 0.9043765664100647, Lr:0.0001\n",
      "Epoch 21, Step: 87, Loss: 1.1498360633850098, Lr:0.0001\n",
      "Epoch 21, Step: 88, Loss: 1.4529032707214355, Lr:0.0001\n",
      "Epoch 21, Step: 89, Loss: 0.9312613010406494, Lr:0.0001\n",
      "Epoch 21, Step: 90, Loss: 0.5610073804855347, Lr:0.0001\n",
      "Epoch 21, Step: 91, Loss: 0.8457881808280945, Lr:0.0001\n",
      "Epoch 21, Step: 92, Loss: 0.5574460625648499, Lr:0.0001\n",
      "Epoch 21, Step: 93, Loss: 0.24946413934230804, Lr:0.0001\n",
      "Epoch 21, Step: 94, Loss: 0.6964963674545288, Lr:0.0001\n",
      "Epoch 21, Step: 95, Loss: 0.7612521052360535, Lr:0.0001\n",
      "Epoch 21, Step: 96, Loss: 1.405954122543335, Lr:0.0001\n",
      "Epoch 21, Step: 97, Loss: 0.16358932852745056, Lr:0.0001\n",
      "Epoch 21, Step: 98, Loss: 0.4383688271045685, Lr:0.0001\n",
      "Epoch 21, Step: 99, Loss: 0.2727545499801636, Lr:0.0001\n",
      "Epoch 21, Step: 100, Loss: 1.0374772548675537, Lr:0.0001\n",
      "Epoch 21, Step: 101, Loss: 0.4680842161178589, Lr:0.0001\n",
      "Epoch 21, Step: 102, Loss: 0.5643181800842285, Lr:0.0001\n",
      "Epoch 21, Step: 103, Loss: 0.13498547673225403, Lr:0.0001\n",
      "Epoch 21, Step: 104, Loss: 2.619260311126709, Lr:0.0001\n",
      "Epoch 21, Step: 105, Loss: 0.04858008027076721, Lr:0.0001\n",
      "Epoch 21, Step: 106, Loss: 0.9816531538963318, Lr:0.0001\n",
      "Epoch 21, Step: 107, Loss: 0.45889919996261597, Lr:0.0001\n",
      "Epoch 21, Step: 108, Loss: 0.5618721842765808, Lr:0.0001\n",
      "Epoch 21, Step: 109, Loss: 0.36825522780418396, Lr:0.0001\n",
      "Epoch 21, Step: 110, Loss: 0.4037662446498871, Lr:0.0001\n",
      "Epoch 21, Step: 111, Loss: 0.5149573683738708, Lr:0.0001\n",
      "Epoch 21, Step: 112, Loss: 0.7726134657859802, Lr:0.0001\n",
      "Epoch 21, Step: 113, Loss: 1.8796253204345703, Lr:0.0001\n",
      "Epoch 21, Step: 114, Loss: 0.7594993114471436, Lr:0.0001\n",
      "Epoch 21, Step: 115, Loss: 0.36277300119400024, Lr:0.0001\n",
      "Epoch 21, Step: 116, Loss: 0.039101492613554, Lr:0.0001\n",
      "Epoch 21, Step: 117, Loss: 0.43770506978034973, Lr:0.0001\n",
      "Epoch 21, Step: 118, Loss: 1.3881864547729492, Lr:0.0001\n",
      "Epoch 21, Step: 119, Loss: 0.5223574638366699, Lr:0.0001\n",
      "Epoch 21, Step: 120, Loss: 0.1350998878479004, Lr:0.0001\n",
      "Epoch 21, Step: 121, Loss: 1.7980937957763672, Lr:0.0001\n",
      "Epoch 21, Step: 122, Loss: 0.996124804019928, Lr:0.0001\n",
      "Epoch 21, Step: 123, Loss: 0.41460540890693665, Lr:0.0001\n",
      "Epoch 21, Step: 124, Loss: 1.2172774076461792, Lr:0.0001\n",
      "Epoch 21, Step: 125, Loss: 1.2150335311889648, Lr:0.0001\n",
      "Epoch 21, Step: 126, Loss: 0.8225083947181702, Lr:0.0001\n",
      "Epoch 21, Step: 127, Loss: 0.8662314414978027, Lr:0.0001\n",
      "Epoch 21, Step: 128, Loss: 0.9935890436172485, Lr:0.0001\n",
      "Epoch 21, Step: 129, Loss: 0.6021939516067505, Lr:0.0001\n",
      "Epoch 21, Step: 130, Loss: 0.7457368969917297, Lr:0.0001\n",
      "Epoch 21, Step: 131, Loss: 0.7603922486305237, Lr:0.0001\n",
      "Epoch 21, Step: 132, Loss: 1.0764294862747192, Lr:0.0001\n",
      "Epoch 21, Step: 133, Loss: 1.1230864524841309, Lr:0.0001\n",
      "Epoch 21, Step: 134, Loss: 1.1467914581298828, Lr:0.0001\n",
      "Epoch 21, Step: 135, Loss: 0.2748301029205322, Lr:0.0001\n",
      "Epoch 21, Step: 136, Loss: 0.8428785800933838, Lr:0.0001\n",
      "Epoch 21, Step: 137, Loss: 0.750336766242981, Lr:0.0001\n",
      "Epoch 21, Step: 138, Loss: 1.353118658065796, Lr:0.0001\n",
      "Epoch 21, Step: 139, Loss: 0.7458012104034424, Lr:0.0001\n",
      "Epoch 21, Step: 140, Loss: 0.7787814140319824, Lr:0.0001\n",
      "Epoch 21, Step: 141, Loss: 0.3349703848361969, Lr:0.0001\n",
      "Epoch 21, Step: 142, Loss: 0.6135827302932739, Lr:0.0001\n",
      "Epoch 21, Step: 143, Loss: 0.8819800019264221, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 21\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.3106 (0.3106) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0090 data: 0.0050 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7577 (0.7723) acc1: 75.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0076 data: 0.0038 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0076 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.772\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 22, Step: 0, Loss: 0.5817326307296753, Lr:0.0001\n",
      "Epoch 22, Step: 1, Loss: 1.475127935409546, Lr:0.0001\n",
      "Epoch 22, Step: 2, Loss: 1.1420005559921265, Lr:0.0001\n",
      "Epoch 22, Step: 3, Loss: 0.8371789455413818, Lr:0.0001\n",
      "Epoch 22, Step: 4, Loss: 0.4376043677330017, Lr:0.0001\n",
      "Epoch 22, Step: 5, Loss: 0.6357088088989258, Lr:0.0001\n",
      "Epoch 22, Step: 6, Loss: 0.6271877288818359, Lr:0.0001\n",
      "Epoch 22, Step: 7, Loss: 0.15230698883533478, Lr:0.0001\n",
      "Epoch 22, Step: 8, Loss: 1.853982925415039, Lr:0.0001\n",
      "Epoch 22, Step: 9, Loss: 0.32336482405662537, Lr:0.0001\n",
      "Epoch 22, Step: 10, Loss: 0.6739628314971924, Lr:0.0001\n",
      "Epoch 22, Step: 11, Loss: 1.346282958984375, Lr:0.0001\n",
      "Epoch 22, Step: 12, Loss: 0.4384419023990631, Lr:0.0001\n",
      "Epoch 22, Step: 13, Loss: 0.22582930326461792, Lr:0.0001\n",
      "Epoch 22, Step: 14, Loss: 0.3425266742706299, Lr:0.0001\n",
      "Epoch 22, Step: 15, Loss: 0.49537622928619385, Lr:0.0001\n",
      "Epoch 22, Step: 16, Loss: 0.19755996763706207, Lr:0.0001\n",
      "Epoch 22, Step: 17, Loss: 0.32317298650741577, Lr:0.0001\n",
      "Epoch 22, Step: 18, Loss: 1.1060726642608643, Lr:0.0001\n",
      "Epoch 22, Step: 19, Loss: 0.3217984139919281, Lr:0.0001\n",
      "Epoch 22, Step: 20, Loss: 0.7266936898231506, Lr:0.0001\n",
      "Epoch 22, Step: 21, Loss: 0.22283117473125458, Lr:0.0001\n",
      "Epoch 22, Step: 22, Loss: 1.883090615272522, Lr:0.0001\n",
      "Epoch 22, Step: 23, Loss: 0.6578029990196228, Lr:0.0001\n",
      "Epoch 22, Step: 24, Loss: 0.5475485920906067, Lr:0.0001\n",
      "Epoch 22, Step: 25, Loss: 2.225313901901245, Lr:0.0001\n",
      "Epoch 22, Step: 26, Loss: 1.7727534770965576, Lr:0.0001\n",
      "Epoch 22, Step: 27, Loss: 0.3291090428829193, Lr:0.0001\n",
      "Epoch 22, Step: 28, Loss: 1.1101716756820679, Lr:0.0001\n",
      "Epoch 22, Step: 29, Loss: 0.4057340621948242, Lr:0.0001\n",
      "Epoch 22, Step: 30, Loss: 0.5407673716545105, Lr:0.0001\n",
      "Epoch 22, Step: 31, Loss: 1.9161158800125122, Lr:0.0001\n",
      "Epoch 22, Step: 32, Loss: 0.11295735836029053, Lr:0.0001\n",
      "Epoch 22, Step: 33, Loss: 0.517194390296936, Lr:0.0001\n",
      "Epoch 22, Step: 34, Loss: 1.0274041891098022, Lr:0.0001\n",
      "Epoch 22, Step: 35, Loss: 0.43493083119392395, Lr:0.0001\n",
      "Epoch 22, Step: 36, Loss: 0.7377071380615234, Lr:0.0001\n",
      "Epoch 22, Step: 37, Loss: 0.6614567041397095, Lr:0.0001\n",
      "Epoch 22, Step: 38, Loss: 0.5748879909515381, Lr:0.0001\n",
      "Epoch 22, Step: 39, Loss: 0.5718281865119934, Lr:0.0001\n",
      "Epoch 22, Step: 40, Loss: 0.21095344424247742, Lr:0.0001\n",
      "Epoch 22, Step: 41, Loss: 0.7945948839187622, Lr:0.0001\n",
      "Epoch 22, Step: 42, Loss: 0.3467317223548889, Lr:0.0001\n",
      "Epoch 22, Step: 43, Loss: 0.43610885739326477, Lr:0.0001\n",
      "Epoch 22, Step: 44, Loss: 0.7006588578224182, Lr:0.0001\n",
      "Epoch 22, Step: 45, Loss: 0.2120255082845688, Lr:0.0001\n",
      "Epoch 22, Step: 46, Loss: 0.8844037055969238, Lr:0.0001\n",
      "Epoch 22, Step: 47, Loss: 0.5881233215332031, Lr:0.0001\n",
      "Epoch 22, Step: 48, Loss: 0.317286878824234, Lr:0.0001\n",
      "Epoch 22, Step: 49, Loss: 0.3718615174293518, Lr:0.0001\n",
      "Epoch 22, Step: 50, Loss: 0.788449764251709, Lr:0.0001\n",
      "Epoch 22, Step: 51, Loss: 0.7439387440681458, Lr:0.0001\n",
      "Epoch 22, Step: 52, Loss: 0.1170349195599556, Lr:0.0001\n",
      "Epoch 22, Step: 53, Loss: 0.8035160303115845, Lr:0.0001\n",
      "Epoch 22, Step: 54, Loss: 1.794960618019104, Lr:0.0001\n",
      "Epoch 22, Step: 55, Loss: 1.0401405096054077, Lr:0.0001\n",
      "Epoch 22, Step: 56, Loss: 0.8504258394241333, Lr:0.0001\n",
      "Epoch 22, Step: 57, Loss: 0.10753995180130005, Lr:0.0001\n",
      "Epoch 22, Step: 58, Loss: 1.190248727798462, Lr:0.0001\n",
      "Epoch 22, Step: 59, Loss: 1.2123631238937378, Lr:0.0001\n",
      "Epoch 22, Step: 60, Loss: 1.0740259885787964, Lr:0.0001\n",
      "Epoch 22, Step: 61, Loss: 1.368882656097412, Lr:0.0001\n",
      "Epoch 22, Step: 62, Loss: 0.3266093134880066, Lr:0.0001\n",
      "Epoch 22, Step: 63, Loss: 0.9607458114624023, Lr:0.0001\n",
      "Epoch 22, Step: 64, Loss: 0.39881742000579834, Lr:0.0001\n",
      "Epoch 22, Step: 65, Loss: 0.34878939390182495, Lr:0.0001\n",
      "Epoch 22, Step: 66, Loss: 0.5839254856109619, Lr:0.0001\n",
      "Epoch 22, Step: 67, Loss: 0.937501072883606, Lr:0.0001\n",
      "Epoch 22, Step: 68, Loss: 1.0043479204177856, Lr:0.0001\n",
      "Epoch 22, Step: 69, Loss: 1.6762964725494385, Lr:0.0001\n",
      "Epoch 22, Step: 70, Loss: 0.29049041867256165, Lr:0.0001\n",
      "Epoch 22, Step: 71, Loss: 0.47646650671958923, Lr:0.0001\n",
      "Epoch 22, Step: 72, Loss: 0.6141107082366943, Lr:0.0001\n",
      "Epoch 22, Step: 73, Loss: 0.8258232474327087, Lr:0.0001\n",
      "Epoch 22, Step: 74, Loss: 0.5110402703285217, Lr:0.0001\n",
      "Epoch 22, Step: 75, Loss: 0.2585098147392273, Lr:0.0001\n",
      "Epoch 22, Step: 76, Loss: 0.1289019137620926, Lr:0.0001\n",
      "Epoch 22, Step: 77, Loss: 0.5793150663375854, Lr:0.0001\n",
      "Epoch 22, Step: 78, Loss: 0.7751763463020325, Lr:0.0001\n",
      "Epoch 22, Step: 79, Loss: 0.8229485750198364, Lr:0.0001\n",
      "Epoch 22, Step: 80, Loss: 0.8338050246238708, Lr:0.0001\n",
      "Epoch 22, Step: 81, Loss: 1.4024771451950073, Lr:0.0001\n",
      "Epoch 22, Step: 82, Loss: 0.1338275969028473, Lr:0.0001\n",
      "Epoch 22, Step: 83, Loss: 0.8940690755844116, Lr:0.0001\n",
      "Epoch 22, Step: 84, Loss: 0.7200888991355896, Lr:0.0001\n",
      "Epoch 22, Step: 85, Loss: 1.6118009090423584, Lr:0.0001\n",
      "Epoch 22, Step: 86, Loss: 1.4069536924362183, Lr:0.0001\n",
      "Epoch 22, Step: 87, Loss: 0.49718350172042847, Lr:0.0001\n",
      "Epoch 22, Step: 88, Loss: 1.250130295753479, Lr:0.0001\n",
      "Epoch 22, Step: 89, Loss: 0.19690170884132385, Lr:0.0001\n",
      "Epoch 22, Step: 90, Loss: 0.4682639241218567, Lr:0.0001\n",
      "Epoch 22, Step: 91, Loss: 0.036599092185497284, Lr:0.0001\n",
      "Epoch 22, Step: 92, Loss: 0.922321617603302, Lr:0.0001\n",
      "Epoch 22, Step: 93, Loss: 2.841999053955078, Lr:0.0001\n",
      "Epoch 22, Step: 94, Loss: 1.734348177909851, Lr:0.0001\n",
      "Epoch 22, Step: 95, Loss: 0.4155469834804535, Lr:0.0001\n",
      "Epoch 22, Step: 96, Loss: 0.423971951007843, Lr:0.0001\n",
      "Epoch 22, Step: 97, Loss: 0.436770498752594, Lr:0.0001\n",
      "Epoch 22, Step: 98, Loss: 0.6803829669952393, Lr:0.0001\n",
      "Epoch 22, Step: 99, Loss: 1.389601230621338, Lr:0.0001\n",
      "Epoch 22, Step: 100, Loss: 0.9869441390037537, Lr:0.0001\n",
      "Epoch 22, Step: 101, Loss: 0.5950311422348022, Lr:0.0001\n",
      "Epoch 22, Step: 102, Loss: 0.7269179821014404, Lr:0.0001\n",
      "Epoch 22, Step: 103, Loss: 1.0542348623275757, Lr:0.0001\n",
      "Epoch 22, Step: 104, Loss: 0.702899694442749, Lr:0.0001\n",
      "Epoch 22, Step: 105, Loss: 0.868184506893158, Lr:0.0001\n",
      "Epoch 22, Step: 106, Loss: 0.8680643439292908, Lr:0.0001\n",
      "Epoch 22, Step: 107, Loss: 0.5292251110076904, Lr:0.0001\n",
      "Epoch 22, Step: 108, Loss: 0.5732349157333374, Lr:0.0001\n",
      "Epoch 22, Step: 109, Loss: 1.0809450149536133, Lr:0.0001\n",
      "Epoch 22, Step: 110, Loss: 0.7259195446968079, Lr:0.0001\n",
      "Epoch 22, Step: 111, Loss: 1.1137428283691406, Lr:0.0001\n",
      "Epoch 22, Step: 112, Loss: 0.7633941769599915, Lr:0.0001\n",
      "Epoch 22, Step: 113, Loss: 0.9258426427841187, Lr:0.0001\n",
      "Epoch 22, Step: 114, Loss: 0.14934971928596497, Lr:0.0001\n",
      "Epoch 22, Step: 115, Loss: 0.3856341540813446, Lr:0.0001\n",
      "Epoch 22, Step: 116, Loss: 0.5221170783042908, Lr:0.0001\n",
      "Epoch 22, Step: 117, Loss: 0.467217355966568, Lr:0.0001\n",
      "Epoch 22, Step: 118, Loss: 1.6537022590637207, Lr:0.0001\n",
      "Epoch 22, Step: 119, Loss: 1.1194937229156494, Lr:0.0001\n",
      "Epoch 22, Step: 120, Loss: 0.6682593822479248, Lr:0.0001\n",
      "Epoch 22, Step: 121, Loss: 0.6552733182907104, Lr:0.0001\n",
      "Epoch 22, Step: 122, Loss: 0.5313067436218262, Lr:0.0001\n",
      "Epoch 22, Step: 123, Loss: 0.3943886160850525, Lr:0.0001\n",
      "Epoch 22, Step: 124, Loss: 0.7722147703170776, Lr:0.0001\n",
      "Epoch 22, Step: 125, Loss: 0.3090142011642456, Lr:0.0001\n",
      "Epoch 22, Step: 126, Loss: 0.6979966759681702, Lr:0.0001\n",
      "Epoch 22, Step: 127, Loss: 0.5192128419876099, Lr:0.0001\n",
      "Epoch 22, Step: 128, Loss: 1.6029112339019775, Lr:0.0001\n",
      "Epoch 22, Step: 129, Loss: 0.3142179548740387, Lr:0.0001\n",
      "Epoch 22, Step: 130, Loss: 0.26724350452423096, Lr:0.0001\n",
      "Epoch 22, Step: 131, Loss: 0.8135644197463989, Lr:0.0001\n",
      "Epoch 22, Step: 132, Loss: 0.8010050058364868, Lr:0.0001\n",
      "Epoch 22, Step: 133, Loss: 0.591556191444397, Lr:0.0001\n",
      "Epoch 22, Step: 134, Loss: 0.8476554155349731, Lr:0.0001\n",
      "Epoch 22, Step: 135, Loss: 0.09901298582553864, Lr:0.0001\n",
      "Epoch 22, Step: 136, Loss: 1.3243893384933472, Lr:0.0001\n",
      "Epoch 22, Step: 137, Loss: 0.24156954884529114, Lr:0.0001\n",
      "Epoch 22, Step: 138, Loss: 1.0398030281066895, Lr:0.0001\n",
      "Epoch 22, Step: 139, Loss: 0.7718917727470398, Lr:0.0001\n",
      "Epoch 22, Step: 140, Loss: 1.1023640632629395, Lr:0.0001\n",
      "Epoch 22, Step: 141, Loss: 0.022100698202848434, Lr:0.0001\n",
      "Epoch 22, Step: 142, Loss: 0.7900626063346863, Lr:0.0001\n",
      "Epoch 22, Step: 143, Loss: 0.6227836012840271, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 22\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0378 (0.0378) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0110 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5409 (0.6944) acc1: 75.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0072 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0073 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.694\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 23, Step: 0, Loss: 0.8737679123878479, Lr:0.0001\n",
      "Epoch 23, Step: 1, Loss: 0.8333566188812256, Lr:0.0001\n",
      "Epoch 23, Step: 2, Loss: 1.867081642150879, Lr:0.0001\n",
      "Epoch 23, Step: 3, Loss: 0.589601993560791, Lr:0.0001\n",
      "Epoch 23, Step: 4, Loss: 1.396272897720337, Lr:0.0001\n",
      "Epoch 23, Step: 5, Loss: 1.014975905418396, Lr:0.0001\n",
      "Epoch 23, Step: 6, Loss: 0.5896706581115723, Lr:0.0001\n",
      "Epoch 23, Step: 7, Loss: 0.4811781942844391, Lr:0.0001\n",
      "Epoch 23, Step: 8, Loss: 1.7150537967681885, Lr:0.0001\n",
      "Epoch 23, Step: 9, Loss: 0.655962347984314, Lr:0.0001\n",
      "Epoch 23, Step: 10, Loss: 0.6097095012664795, Lr:0.0001\n",
      "Epoch 23, Step: 11, Loss: 1.077477216720581, Lr:0.0001\n",
      "Epoch 23, Step: 12, Loss: 0.26310425996780396, Lr:0.0001\n",
      "Epoch 23, Step: 13, Loss: 0.48110756278038025, Lr:0.0001\n",
      "Epoch 23, Step: 14, Loss: 0.6068430542945862, Lr:0.0001\n",
      "Epoch 23, Step: 15, Loss: 0.5655491352081299, Lr:0.0001\n",
      "Epoch 23, Step: 16, Loss: 0.2411024272441864, Lr:0.0001\n",
      "Epoch 23, Step: 17, Loss: 0.22579263150691986, Lr:0.0001\n",
      "Epoch 23, Step: 18, Loss: 0.8402906060218811, Lr:0.0001\n",
      "Epoch 23, Step: 19, Loss: 0.12173879146575928, Lr:0.0001\n",
      "Epoch 23, Step: 20, Loss: 1.3100894689559937, Lr:0.0001\n",
      "Epoch 23, Step: 21, Loss: 1.702831506729126, Lr:0.0001\n",
      "Epoch 23, Step: 22, Loss: 0.051126278936862946, Lr:0.0001\n",
      "Epoch 23, Step: 23, Loss: 2.161284923553467, Lr:0.0001\n",
      "Epoch 23, Step: 24, Loss: 0.4347265958786011, Lr:0.0001\n",
      "Epoch 23, Step: 25, Loss: 0.7199961543083191, Lr:0.0001\n",
      "Epoch 23, Step: 26, Loss: 1.048627495765686, Lr:0.0001\n",
      "Epoch 23, Step: 27, Loss: 0.6158822774887085, Lr:0.0001\n",
      "Epoch 23, Step: 28, Loss: 0.48391446471214294, Lr:0.0001\n",
      "Epoch 23, Step: 29, Loss: 0.40490543842315674, Lr:0.0001\n",
      "Epoch 23, Step: 30, Loss: 0.8733850717544556, Lr:0.0001\n",
      "Epoch 23, Step: 31, Loss: 1.3890511989593506, Lr:0.0001\n",
      "Epoch 23, Step: 32, Loss: 2.0873656272888184, Lr:0.0001\n",
      "Epoch 23, Step: 33, Loss: 0.5203070640563965, Lr:0.0001\n",
      "Epoch 23, Step: 34, Loss: 0.1672896444797516, Lr:0.0001\n",
      "Epoch 23, Step: 35, Loss: 0.7069768905639648, Lr:0.0001\n",
      "Epoch 23, Step: 36, Loss: 0.24667270481586456, Lr:0.0001\n",
      "Epoch 23, Step: 37, Loss: 0.7346927523612976, Lr:0.0001\n",
      "Epoch 23, Step: 38, Loss: 0.5023608803749084, Lr:0.0001\n",
      "Epoch 23, Step: 39, Loss: 0.31884315609931946, Lr:0.0001\n",
      "Epoch 23, Step: 40, Loss: 0.7053553462028503, Lr:0.0001\n",
      "Epoch 23, Step: 41, Loss: 0.952556848526001, Lr:0.0001\n",
      "Epoch 23, Step: 42, Loss: 0.5161612033843994, Lr:0.0001\n",
      "Epoch 23, Step: 43, Loss: 0.9843897223472595, Lr:0.0001\n",
      "Epoch 23, Step: 44, Loss: 0.7436906099319458, Lr:0.0001\n",
      "Epoch 23, Step: 45, Loss: 2.2361605167388916, Lr:0.0001\n",
      "Epoch 23, Step: 46, Loss: 0.44975370168685913, Lr:0.0001\n",
      "Epoch 23, Step: 47, Loss: 1.116891622543335, Lr:0.0001\n",
      "Epoch 23, Step: 48, Loss: 0.7023481130599976, Lr:0.0001\n",
      "Epoch 23, Step: 49, Loss: 0.6046209335327148, Lr:0.0001\n",
      "Epoch 23, Step: 50, Loss: 0.5441842675209045, Lr:0.0001\n",
      "Epoch 23, Step: 51, Loss: 0.3624008595943451, Lr:0.0001\n",
      "Epoch 23, Step: 52, Loss: 0.26914751529693604, Lr:0.0001\n",
      "Epoch 23, Step: 53, Loss: 0.784693717956543, Lr:0.0001\n",
      "Epoch 23, Step: 54, Loss: 0.8164599537849426, Lr:0.0001\n",
      "Epoch 23, Step: 55, Loss: 0.6333692669868469, Lr:0.0001\n",
      "Epoch 23, Step: 56, Loss: 1.0187474489212036, Lr:0.0001\n",
      "Epoch 23, Step: 57, Loss: 0.5885416269302368, Lr:0.0001\n",
      "Epoch 23, Step: 58, Loss: 0.6872249841690063, Lr:0.0001\n",
      "Epoch 23, Step: 59, Loss: 0.5456418991088867, Lr:0.0001\n",
      "Epoch 23, Step: 60, Loss: 0.8785883188247681, Lr:0.0001\n",
      "Epoch 23, Step: 61, Loss: 0.7799639105796814, Lr:0.0001\n",
      "Epoch 23, Step: 62, Loss: 0.3834555745124817, Lr:0.0001\n",
      "Epoch 23, Step: 63, Loss: 0.6197590231895447, Lr:0.0001\n",
      "Epoch 23, Step: 64, Loss: 1.3205589056015015, Lr:0.0001\n",
      "Epoch 23, Step: 65, Loss: 0.7081579566001892, Lr:0.0001\n",
      "Epoch 23, Step: 66, Loss: 2.4625470638275146, Lr:0.0001\n",
      "Epoch 23, Step: 67, Loss: 0.9574896693229675, Lr:0.0001\n",
      "Epoch 23, Step: 68, Loss: 0.7810717225074768, Lr:0.0001\n",
      "Epoch 23, Step: 69, Loss: 2.7306079864501953, Lr:0.0001\n",
      "Epoch 23, Step: 70, Loss: 0.9383760690689087, Lr:0.0001\n",
      "Epoch 23, Step: 71, Loss: 0.26750338077545166, Lr:0.0001\n",
      "Epoch 23, Step: 72, Loss: 0.8407261371612549, Lr:0.0001\n",
      "Epoch 23, Step: 73, Loss: 0.32262274622917175, Lr:0.0001\n",
      "Epoch 23, Step: 74, Loss: 0.8409265279769897, Lr:0.0001\n",
      "Epoch 23, Step: 75, Loss: 0.7539318799972534, Lr:0.0001\n",
      "Epoch 23, Step: 76, Loss: 1.3587759733200073, Lr:0.0001\n",
      "Epoch 23, Step: 77, Loss: 0.9182369709014893, Lr:0.0001\n",
      "Epoch 23, Step: 78, Loss: 0.8215370774269104, Lr:0.0001\n",
      "Epoch 23, Step: 79, Loss: 0.7558481693267822, Lr:0.0001\n",
      "Epoch 23, Step: 80, Loss: 0.7021914720535278, Lr:0.0001\n",
      "Epoch 23, Step: 81, Loss: 0.6321549415588379, Lr:0.0001\n",
      "Epoch 23, Step: 82, Loss: 0.4029695391654968, Lr:0.0001\n",
      "Epoch 23, Step: 83, Loss: 1.530731439590454, Lr:0.0001\n",
      "Epoch 23, Step: 84, Loss: 0.6770957708358765, Lr:0.0001\n",
      "Epoch 23, Step: 85, Loss: 1.5883288383483887, Lr:0.0001\n",
      "Epoch 23, Step: 86, Loss: 1.3135126829147339, Lr:0.0001\n",
      "Epoch 23, Step: 87, Loss: 0.8223605751991272, Lr:0.0001\n",
      "Epoch 23, Step: 88, Loss: 0.7480553984642029, Lr:0.0001\n",
      "Epoch 23, Step: 89, Loss: 0.3608511984348297, Lr:0.0001\n",
      "Epoch 23, Step: 90, Loss: 0.541280210018158, Lr:0.0001\n",
      "Epoch 23, Step: 91, Loss: 0.8619937896728516, Lr:0.0001\n",
      "Epoch 23, Step: 92, Loss: 0.14270877838134766, Lr:0.0001\n",
      "Epoch 23, Step: 93, Loss: 0.6256603002548218, Lr:0.0001\n",
      "Epoch 23, Step: 94, Loss: 0.6944947242736816, Lr:0.0001\n",
      "Epoch 23, Step: 95, Loss: 0.5006048679351807, Lr:0.0001\n",
      "Epoch 23, Step: 96, Loss: 0.3332732915878296, Lr:0.0001\n",
      "Epoch 23, Step: 97, Loss: 1.420296311378479, Lr:0.0001\n",
      "Epoch 23, Step: 98, Loss: 0.6946137547492981, Lr:0.0001\n",
      "Epoch 23, Step: 99, Loss: 0.9901008009910583, Lr:0.0001\n",
      "Epoch 23, Step: 100, Loss: 1.0522533655166626, Lr:0.0001\n",
      "Epoch 23, Step: 101, Loss: 0.19721458852291107, Lr:0.0001\n",
      "Epoch 23, Step: 102, Loss: 0.7097168564796448, Lr:0.0001\n",
      "Epoch 23, Step: 103, Loss: 0.38749054074287415, Lr:0.0001\n",
      "Epoch 23, Step: 104, Loss: 0.8934226632118225, Lr:0.0001\n",
      "Epoch 23, Step: 105, Loss: 0.6712951064109802, Lr:0.0001\n",
      "Epoch 23, Step: 106, Loss: 0.4601891338825226, Lr:0.0001\n",
      "Epoch 23, Step: 107, Loss: 0.2480594366788864, Lr:0.0001\n",
      "Epoch 23, Step: 108, Loss: 0.5438635945320129, Lr:0.0001\n",
      "Epoch 23, Step: 109, Loss: 1.0056473016738892, Lr:0.0001\n",
      "Epoch 23, Step: 110, Loss: 0.9237155914306641, Lr:0.0001\n",
      "Epoch 23, Step: 111, Loss: 1.060485601425171, Lr:0.0001\n",
      "Epoch 23, Step: 112, Loss: 2.9399008750915527, Lr:0.0001\n",
      "Epoch 23, Step: 113, Loss: 0.4604504704475403, Lr:0.0001\n",
      "Epoch 23, Step: 114, Loss: 0.8408191800117493, Lr:0.0001\n",
      "Epoch 23, Step: 115, Loss: 0.5876206159591675, Lr:0.0001\n",
      "Epoch 23, Step: 116, Loss: 0.7899317145347595, Lr:0.0001\n",
      "Epoch 23, Step: 117, Loss: 0.6971098184585571, Lr:0.0001\n",
      "Epoch 23, Step: 118, Loss: 0.7644866108894348, Lr:0.0001\n",
      "Epoch 23, Step: 119, Loss: 1.0600868463516235, Lr:0.0001\n",
      "Epoch 23, Step: 120, Loss: 0.6033220291137695, Lr:0.0001\n",
      "Epoch 23, Step: 121, Loss: 0.3621634542942047, Lr:0.0001\n",
      "Epoch 23, Step: 122, Loss: 0.9075546264648438, Lr:0.0001\n",
      "Epoch 23, Step: 123, Loss: 1.134674072265625, Lr:0.0001\n",
      "Epoch 23, Step: 124, Loss: 1.2197571992874146, Lr:0.0001\n",
      "Epoch 23, Step: 125, Loss: 0.5584170818328857, Lr:0.0001\n",
      "Epoch 23, Step: 126, Loss: 0.1554667055606842, Lr:0.0001\n",
      "Epoch 23, Step: 127, Loss: 0.6282591223716736, Lr:0.0001\n",
      "Epoch 23, Step: 128, Loss: 1.0105741024017334, Lr:0.0001\n",
      "Epoch 23, Step: 129, Loss: 0.9622774720191956, Lr:0.0001\n",
      "Epoch 23, Step: 130, Loss: 0.5388041734695435, Lr:0.0001\n",
      "Epoch 23, Step: 131, Loss: 1.0418727397918701, Lr:0.0001\n",
      "Epoch 23, Step: 132, Loss: 0.16414736211299896, Lr:0.0001\n",
      "Epoch 23, Step: 133, Loss: 0.8752275109291077, Lr:0.0001\n",
      "Epoch 23, Step: 134, Loss: 0.5520562529563904, Lr:0.0001\n",
      "Epoch 23, Step: 135, Loss: 0.288987398147583, Lr:0.0001\n",
      "Epoch 23, Step: 136, Loss: 0.6998851299285889, Lr:0.0001\n",
      "Epoch 23, Step: 137, Loss: 0.1524880826473236, Lr:0.0001\n",
      "Epoch 23, Step: 138, Loss: 0.662762463092804, Lr:0.0001\n",
      "Epoch 23, Step: 139, Loss: 0.5224137306213379, Lr:0.0001\n",
      "Epoch 23, Step: 140, Loss: 0.45882245898246765, Lr:0.0001\n",
      "Epoch 23, Step: 141, Loss: 0.662732720375061, Lr:0.0001\n",
      "Epoch 23, Step: 142, Loss: 0.07927870750427246, Lr:0.0001\n",
      "Epoch 23, Step: 143, Loss: 0.3991493284702301, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 23\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0817 (0.0817) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 1.1303 (1.0059) acc1: 75.0000 (63.6364) acc5: 100.0000 (100.0000) time: 0.0068 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0068 s / it)\n",
      "* Acc@1 63.636 Acc@5 100.000 loss 1.006\n",
      "Accuracy of the network on the 33 test image: 63.6%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 24, Step: 0, Loss: 0.4783586263656616, Lr:0.0001\n",
      "Epoch 24, Step: 1, Loss: 0.21822315454483032, Lr:0.0001\n",
      "Epoch 24, Step: 2, Loss: 0.0830300971865654, Lr:0.0001\n",
      "Epoch 24, Step: 3, Loss: 0.5537203550338745, Lr:0.0001\n",
      "Epoch 24, Step: 4, Loss: 0.4530077576637268, Lr:0.0001\n",
      "Epoch 24, Step: 5, Loss: 0.037046167999506, Lr:0.0001\n",
      "Epoch 24, Step: 6, Loss: 0.4278888404369354, Lr:0.0001\n",
      "Epoch 24, Step: 7, Loss: 0.3724950850009918, Lr:0.0001\n",
      "Epoch 24, Step: 8, Loss: 0.4865207374095917, Lr:0.0001\n",
      "Epoch 24, Step: 9, Loss: 0.5775417685508728, Lr:0.0001\n",
      "Epoch 24, Step: 10, Loss: 1.8083091974258423, Lr:0.0001\n",
      "Epoch 24, Step: 11, Loss: 0.8488408923149109, Lr:0.0001\n",
      "Epoch 24, Step: 12, Loss: 0.6958845853805542, Lr:0.0001\n",
      "Epoch 24, Step: 13, Loss: 0.733171284198761, Lr:0.0001\n",
      "Epoch 24, Step: 14, Loss: 1.3176981210708618, Lr:0.0001\n",
      "Epoch 24, Step: 15, Loss: 0.3145579695701599, Lr:0.0001\n",
      "Epoch 24, Step: 16, Loss: 0.2670426666736603, Lr:0.0001\n",
      "Epoch 24, Step: 17, Loss: 0.6275510787963867, Lr:0.0001\n",
      "Epoch 24, Step: 18, Loss: 0.32199323177337646, Lr:0.0001\n",
      "Epoch 24, Step: 19, Loss: 2.2352092266082764, Lr:0.0001\n",
      "Epoch 24, Step: 20, Loss: 0.5076618790626526, Lr:0.0001\n",
      "Epoch 24, Step: 21, Loss: 0.7416432499885559, Lr:0.0001\n",
      "Epoch 24, Step: 22, Loss: 0.5814991593360901, Lr:0.0001\n",
      "Epoch 24, Step: 23, Loss: 0.7564471960067749, Lr:0.0001\n",
      "Epoch 24, Step: 24, Loss: 0.8699891567230225, Lr:0.0001\n",
      "Epoch 24, Step: 25, Loss: 0.8699889183044434, Lr:0.0001\n",
      "Epoch 24, Step: 26, Loss: 0.031401846557855606, Lr:0.0001\n",
      "Epoch 24, Step: 27, Loss: 0.7249503135681152, Lr:0.0001\n",
      "Epoch 24, Step: 28, Loss: 0.31886056065559387, Lr:0.0001\n",
      "Epoch 24, Step: 29, Loss: 0.8061410784721375, Lr:0.0001\n",
      "Epoch 24, Step: 30, Loss: 0.4686869978904724, Lr:0.0001\n",
      "Epoch 24, Step: 31, Loss: 0.37434878945350647, Lr:0.0001\n",
      "Epoch 24, Step: 32, Loss: 0.6113722324371338, Lr:0.0001\n",
      "Epoch 24, Step: 33, Loss: 1.484086036682129, Lr:0.0001\n",
      "Epoch 24, Step: 34, Loss: 0.06556989997625351, Lr:0.0001\n",
      "Epoch 24, Step: 35, Loss: 0.3048899173736572, Lr:0.0001\n",
      "Epoch 24, Step: 36, Loss: 0.6129133701324463, Lr:0.0001\n",
      "Epoch 24, Step: 37, Loss: 0.2814526855945587, Lr:0.0001\n",
      "Epoch 24, Step: 38, Loss: 2.5381898880004883, Lr:0.0001\n",
      "Epoch 24, Step: 39, Loss: 1.334336519241333, Lr:0.0001\n",
      "Epoch 24, Step: 40, Loss: 0.6513731479644775, Lr:0.0001\n",
      "Epoch 24, Step: 41, Loss: 0.6450724005699158, Lr:0.0001\n",
      "Epoch 24, Step: 42, Loss: 0.6255650520324707, Lr:0.0001\n",
      "Epoch 24, Step: 43, Loss: 0.11699974536895752, Lr:0.0001\n",
      "Epoch 24, Step: 44, Loss: 0.758277416229248, Lr:0.0001\n",
      "Epoch 24, Step: 45, Loss: 1.6533324718475342, Lr:0.0001\n",
      "Epoch 24, Step: 46, Loss: 1.8492088317871094, Lr:0.0001\n",
      "Epoch 24, Step: 47, Loss: 0.07970767468214035, Lr:0.0001\n",
      "Epoch 24, Step: 48, Loss: 1.4987845420837402, Lr:0.0001\n",
      "Epoch 24, Step: 49, Loss: 0.7061916589736938, Lr:0.0001\n",
      "Epoch 24, Step: 50, Loss: 1.0812420845031738, Lr:0.0001\n",
      "Epoch 24, Step: 51, Loss: 0.16531071066856384, Lr:0.0001\n",
      "Epoch 24, Step: 52, Loss: 0.6500415802001953, Lr:0.0001\n",
      "Epoch 24, Step: 53, Loss: 0.5712080001831055, Lr:0.0001\n",
      "Epoch 24, Step: 54, Loss: 0.5477690100669861, Lr:0.0001\n",
      "Epoch 24, Step: 55, Loss: 0.5009118914604187, Lr:0.0001\n",
      "Epoch 24, Step: 56, Loss: 0.6596447229385376, Lr:0.0001\n",
      "Epoch 24, Step: 57, Loss: 0.4827157258987427, Lr:0.0001\n",
      "Epoch 24, Step: 58, Loss: 0.2330857813358307, Lr:0.0001\n",
      "Epoch 24, Step: 59, Loss: 0.7566370964050293, Lr:0.0001\n",
      "Epoch 24, Step: 60, Loss: 0.7187590599060059, Lr:0.0001\n",
      "Epoch 24, Step: 61, Loss: 0.8736581802368164, Lr:0.0001\n",
      "Epoch 24, Step: 62, Loss: 0.9583728909492493, Lr:0.0001\n",
      "Epoch 24, Step: 63, Loss: 0.5000372529029846, Lr:0.0001\n",
      "Epoch 24, Step: 64, Loss: 0.46785253286361694, Lr:0.0001\n",
      "Epoch 24, Step: 65, Loss: 0.7837563753128052, Lr:0.0001\n",
      "Epoch 24, Step: 66, Loss: 0.9439611434936523, Lr:0.0001\n",
      "Epoch 24, Step: 67, Loss: 0.8014388084411621, Lr:0.0001\n",
      "Epoch 24, Step: 68, Loss: 0.7419824600219727, Lr:0.0001\n",
      "Epoch 24, Step: 69, Loss: 0.8008924722671509, Lr:0.0001\n",
      "Epoch 24, Step: 70, Loss: 0.7574973106384277, Lr:0.0001\n",
      "Epoch 24, Step: 71, Loss: 0.27473127841949463, Lr:0.0001\n",
      "Epoch 24, Step: 72, Loss: 0.40620726346969604, Lr:0.0001\n",
      "Epoch 24, Step: 73, Loss: 1.6958808898925781, Lr:0.0001\n",
      "Epoch 24, Step: 74, Loss: 1.2083711624145508, Lr:0.0001\n",
      "Epoch 24, Step: 75, Loss: 0.49570128321647644, Lr:0.0001\n",
      "Epoch 24, Step: 76, Loss: 0.7338410019874573, Lr:0.0001\n",
      "Epoch 24, Step: 77, Loss: 0.9674240946769714, Lr:0.0001\n",
      "Epoch 24, Step: 78, Loss: 0.27849718928337097, Lr:0.0001\n",
      "Epoch 24, Step: 79, Loss: 0.6568310260772705, Lr:0.0001\n",
      "Epoch 24, Step: 80, Loss: 0.38515788316726685, Lr:0.0001\n",
      "Epoch 24, Step: 81, Loss: 0.444416880607605, Lr:0.0001\n",
      "Epoch 24, Step: 82, Loss: 0.19639255106449127, Lr:0.0001\n",
      "Epoch 24, Step: 83, Loss: 1.8216906785964966, Lr:0.0001\n",
      "Epoch 24, Step: 84, Loss: 0.7541652321815491, Lr:0.0001\n",
      "Epoch 24, Step: 85, Loss: 0.49438348412513733, Lr:0.0001\n",
      "Epoch 24, Step: 86, Loss: 0.41050300002098083, Lr:0.0001\n",
      "Epoch 24, Step: 87, Loss: 1.079389214515686, Lr:0.0001\n",
      "Epoch 24, Step: 88, Loss: 0.5136010646820068, Lr:0.0001\n",
      "Epoch 24, Step: 89, Loss: 1.1897319555282593, Lr:0.0001\n",
      "Epoch 24, Step: 90, Loss: 0.7721760272979736, Lr:0.0001\n",
      "Epoch 24, Step: 91, Loss: 0.6656494736671448, Lr:0.0001\n",
      "Epoch 24, Step: 92, Loss: 1.192492961883545, Lr:0.0001\n",
      "Epoch 24, Step: 93, Loss: 0.46333351731300354, Lr:0.0001\n",
      "Epoch 24, Step: 94, Loss: 0.6570839285850525, Lr:0.0001\n",
      "Epoch 24, Step: 95, Loss: 0.22424164414405823, Lr:0.0001\n",
      "Epoch 24, Step: 96, Loss: 0.542881965637207, Lr:0.0001\n",
      "Epoch 24, Step: 97, Loss: 1.1717013120651245, Lr:0.0001\n",
      "Epoch 24, Step: 98, Loss: 0.5818080306053162, Lr:0.0001\n",
      "Epoch 24, Step: 99, Loss: 0.7515050172805786, Lr:0.0001\n",
      "Epoch 24, Step: 100, Loss: 0.36082375049591064, Lr:0.0001\n",
      "Epoch 24, Step: 101, Loss: 1.6083821058273315, Lr:0.0001\n",
      "Epoch 24, Step: 102, Loss: 0.5421068072319031, Lr:0.0001\n",
      "Epoch 24, Step: 103, Loss: 0.7800270318984985, Lr:0.0001\n",
      "Epoch 24, Step: 104, Loss: 0.5491787791252136, Lr:0.0001\n",
      "Epoch 24, Step: 105, Loss: 0.5491048693656921, Lr:0.0001\n",
      "Epoch 24, Step: 106, Loss: 1.0435316562652588, Lr:0.0001\n",
      "Epoch 24, Step: 107, Loss: 0.42020440101623535, Lr:0.0001\n",
      "Epoch 24, Step: 108, Loss: 0.9913328289985657, Lr:0.0001\n",
      "Epoch 24, Step: 109, Loss: 1.202543020248413, Lr:0.0001\n",
      "Epoch 24, Step: 110, Loss: 1.52164888381958, Lr:0.0001\n",
      "Epoch 24, Step: 111, Loss: 0.31108570098876953, Lr:0.0001\n",
      "Epoch 24, Step: 112, Loss: 1.7942228317260742, Lr:0.0001\n",
      "Epoch 24, Step: 113, Loss: 0.2527301609516144, Lr:0.0001\n",
      "Epoch 24, Step: 114, Loss: 1.0039981603622437, Lr:0.0001\n",
      "Epoch 24, Step: 115, Loss: 0.2610545754432678, Lr:0.0001\n",
      "Epoch 24, Step: 116, Loss: 2.2688560485839844, Lr:0.0001\n",
      "Epoch 24, Step: 117, Loss: 0.6549410820007324, Lr:0.0001\n",
      "Epoch 24, Step: 118, Loss: 0.44597843289375305, Lr:0.0001\n",
      "Epoch 24, Step: 119, Loss: 0.43005532026290894, Lr:0.0001\n",
      "Epoch 24, Step: 120, Loss: 0.19397510588169098, Lr:0.0001\n",
      "Epoch 24, Step: 121, Loss: 0.5630248188972473, Lr:0.0001\n",
      "Epoch 24, Step: 122, Loss: 0.24312695860862732, Lr:0.0001\n",
      "Epoch 24, Step: 123, Loss: 2.072894334793091, Lr:0.0001\n",
      "Epoch 24, Step: 124, Loss: 1.4697812795639038, Lr:0.0001\n",
      "Epoch 24, Step: 125, Loss: 0.8349053263664246, Lr:0.0001\n",
      "Epoch 24, Step: 126, Loss: 0.3728678524494171, Lr:0.0001\n",
      "Epoch 24, Step: 127, Loss: 0.21030840277671814, Lr:0.0001\n",
      "Epoch 24, Step: 128, Loss: 0.9202419519424438, Lr:0.0001\n",
      "Epoch 24, Step: 129, Loss: 0.5300036668777466, Lr:0.0001\n",
      "Epoch 24, Step: 130, Loss: 0.9515514373779297, Lr:0.0001\n",
      "Epoch 24, Step: 131, Loss: 0.8568463325500488, Lr:0.0001\n",
      "Epoch 24, Step: 132, Loss: 2.2347803115844727, Lr:0.0001\n",
      "Epoch 24, Step: 133, Loss: 1.2335110902786255, Lr:0.0001\n",
      "Epoch 24, Step: 134, Loss: 0.7164021730422974, Lr:0.0001\n",
      "Epoch 24, Step: 135, Loss: 0.18857550621032715, Lr:0.0001\n",
      "Epoch 24, Step: 136, Loss: 0.3162979483604431, Lr:0.0001\n",
      "Epoch 24, Step: 137, Loss: 1.1609883308410645, Lr:0.0001\n",
      "Epoch 24, Step: 138, Loss: 1.6019303798675537, Lr:0.0001\n",
      "Epoch 24, Step: 139, Loss: 0.22677087783813477, Lr:0.0001\n",
      "Epoch 24, Step: 140, Loss: 0.9188170433044434, Lr:0.0001\n",
      "Epoch 24, Step: 141, Loss: 0.5125900506973267, Lr:0.0001\n",
      "Epoch 24, Step: 142, Loss: 0.8273476362228394, Lr:0.0001\n",
      "Epoch 24, Step: 143, Loss: 0.6112440824508667, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 24\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1142 (0.1142) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.9000 (0.8355) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0071 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0073 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 0.835\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 25, Step: 0, Loss: 0.6681249737739563, Lr:0.0001\n",
      "Epoch 25, Step: 1, Loss: 0.42819130420684814, Lr:0.0001\n",
      "Epoch 25, Step: 2, Loss: 0.4317063093185425, Lr:0.0001\n",
      "Epoch 25, Step: 3, Loss: 1.6717915534973145, Lr:0.0001\n",
      "Epoch 25, Step: 4, Loss: 1.3066519498825073, Lr:0.0001\n",
      "Epoch 25, Step: 5, Loss: 1.0292267799377441, Lr:0.0001\n",
      "Epoch 25, Step: 6, Loss: 1.264434814453125, Lr:0.0001\n",
      "Epoch 25, Step: 7, Loss: 0.1895512491464615, Lr:0.0001\n",
      "Epoch 25, Step: 8, Loss: 1.3931806087493896, Lr:0.0001\n",
      "Epoch 25, Step: 9, Loss: 0.457748144865036, Lr:0.0001\n",
      "Epoch 25, Step: 10, Loss: 0.4500218331813812, Lr:0.0001\n",
      "Epoch 25, Step: 11, Loss: 0.4413303732872009, Lr:0.0001\n",
      "Epoch 25, Step: 12, Loss: 2.1252737045288086, Lr:0.0001\n",
      "Epoch 25, Step: 13, Loss: 1.5698161125183105, Lr:0.0001\n",
      "Epoch 25, Step: 14, Loss: 0.6920621395111084, Lr:0.0001\n",
      "Epoch 25, Step: 15, Loss: 0.8571202754974365, Lr:0.0001\n",
      "Epoch 25, Step: 16, Loss: 1.250328779220581, Lr:0.0001\n",
      "Epoch 25, Step: 17, Loss: 0.9531748294830322, Lr:0.0001\n",
      "Epoch 25, Step: 18, Loss: 1.0873355865478516, Lr:0.0001\n",
      "Epoch 25, Step: 19, Loss: 0.59543776512146, Lr:0.0001\n",
      "Epoch 25, Step: 20, Loss: 0.5651749968528748, Lr:0.0001\n",
      "Epoch 25, Step: 21, Loss: 1.118782639503479, Lr:0.0001\n",
      "Epoch 25, Step: 22, Loss: 0.21889367699623108, Lr:0.0001\n",
      "Epoch 25, Step: 23, Loss: 0.2996944785118103, Lr:0.0001\n",
      "Epoch 25, Step: 24, Loss: 1.3254855871200562, Lr:0.0001\n",
      "Epoch 25, Step: 25, Loss: 0.3416746258735657, Lr:0.0001\n",
      "Epoch 25, Step: 26, Loss: 0.22468918561935425, Lr:0.0001\n",
      "Epoch 25, Step: 27, Loss: 0.48730558156967163, Lr:0.0001\n",
      "Epoch 25, Step: 28, Loss: 0.0819723904132843, Lr:0.0001\n",
      "Epoch 25, Step: 29, Loss: 0.09797867387533188, Lr:0.0001\n",
      "Epoch 25, Step: 30, Loss: 0.4237949848175049, Lr:0.0001\n",
      "Epoch 25, Step: 31, Loss: 1.3795479536056519, Lr:0.0001\n",
      "Epoch 25, Step: 32, Loss: 0.2965162992477417, Lr:0.0001\n",
      "Epoch 25, Step: 33, Loss: 0.42290833592414856, Lr:0.0001\n",
      "Epoch 25, Step: 34, Loss: 1.5710852146148682, Lr:0.0001\n",
      "Epoch 25, Step: 35, Loss: 0.7007356882095337, Lr:0.0001\n",
      "Epoch 25, Step: 36, Loss: 0.6846928000450134, Lr:0.0001\n",
      "Epoch 25, Step: 37, Loss: 0.23543933033943176, Lr:0.0001\n",
      "Epoch 25, Step: 38, Loss: 0.713248610496521, Lr:0.0001\n",
      "Epoch 25, Step: 39, Loss: 1.4434093236923218, Lr:0.0001\n",
      "Epoch 25, Step: 40, Loss: 0.44414791464805603, Lr:0.0001\n",
      "Epoch 25, Step: 41, Loss: 0.9558205604553223, Lr:0.0001\n",
      "Epoch 25, Step: 42, Loss: 0.3424142003059387, Lr:0.0001\n",
      "Epoch 25, Step: 43, Loss: 1.2921143770217896, Lr:0.0001\n",
      "Epoch 25, Step: 44, Loss: 0.415073424577713, Lr:0.0001\n",
      "Epoch 25, Step: 45, Loss: 0.868030309677124, Lr:0.0001\n",
      "Epoch 25, Step: 46, Loss: 0.7233753204345703, Lr:0.0001\n",
      "Epoch 25, Step: 47, Loss: 0.8043871521949768, Lr:0.0001\n",
      "Epoch 25, Step: 48, Loss: 1.0593955516815186, Lr:0.0001\n",
      "Epoch 25, Step: 49, Loss: 0.29097819328308105, Lr:0.0001\n",
      "Epoch 25, Step: 50, Loss: 0.4524244964122772, Lr:0.0001\n",
      "Epoch 25, Step: 51, Loss: 0.5753543376922607, Lr:0.0001\n",
      "Epoch 25, Step: 52, Loss: 1.2688546180725098, Lr:0.0001\n",
      "Epoch 25, Step: 53, Loss: 0.4771723449230194, Lr:0.0001\n",
      "Epoch 25, Step: 54, Loss: 0.4777476489543915, Lr:0.0001\n",
      "Epoch 25, Step: 55, Loss: 0.47609442472457886, Lr:0.0001\n",
      "Epoch 25, Step: 56, Loss: 1.2226442098617554, Lr:0.0001\n",
      "Epoch 25, Step: 57, Loss: 0.8158478736877441, Lr:0.0001\n",
      "Epoch 25, Step: 58, Loss: 0.4622989594936371, Lr:0.0001\n",
      "Epoch 25, Step: 59, Loss: 0.3253564238548279, Lr:0.0001\n",
      "Epoch 25, Step: 60, Loss: 0.6614015102386475, Lr:0.0001\n",
      "Epoch 25, Step: 61, Loss: 0.6932944059371948, Lr:0.0001\n",
      "Epoch 25, Step: 62, Loss: 0.7268904447555542, Lr:0.0001\n",
      "Epoch 25, Step: 63, Loss: 1.4767190217971802, Lr:0.0001\n",
      "Epoch 25, Step: 64, Loss: 0.6727597117424011, Lr:0.0001\n",
      "Epoch 25, Step: 65, Loss: 0.5791985392570496, Lr:0.0001\n",
      "Epoch 25, Step: 66, Loss: 0.6132075786590576, Lr:0.0001\n",
      "Epoch 25, Step: 67, Loss: 0.37556323409080505, Lr:0.0001\n",
      "Epoch 25, Step: 68, Loss: 0.6755627393722534, Lr:0.0001\n",
      "Epoch 25, Step: 69, Loss: 0.32229411602020264, Lr:0.0001\n",
      "Epoch 25, Step: 70, Loss: 0.623834490776062, Lr:0.0001\n",
      "Epoch 25, Step: 71, Loss: 0.6264928579330444, Lr:0.0001\n",
      "Epoch 25, Step: 72, Loss: 0.9262971878051758, Lr:0.0001\n",
      "Epoch 25, Step: 73, Loss: 0.7909626960754395, Lr:0.0001\n",
      "Epoch 25, Step: 74, Loss: 0.9445035457611084, Lr:0.0001\n",
      "Epoch 25, Step: 75, Loss: 0.39060619473457336, Lr:0.0001\n",
      "Epoch 25, Step: 76, Loss: 0.721760630607605, Lr:0.0001\n",
      "Epoch 25, Step: 77, Loss: 0.8015875220298767, Lr:0.0001\n",
      "Epoch 25, Step: 78, Loss: 0.563360333442688, Lr:0.0001\n",
      "Epoch 25, Step: 79, Loss: 1.2115634679794312, Lr:0.0001\n",
      "Epoch 25, Step: 80, Loss: 1.3910627365112305, Lr:0.0001\n",
      "Epoch 25, Step: 81, Loss: 0.13179627060890198, Lr:0.0001\n",
      "Epoch 25, Step: 82, Loss: 0.22609560191631317, Lr:0.0001\n",
      "Epoch 25, Step: 83, Loss: 1.859130620956421, Lr:0.0001\n",
      "Epoch 25, Step: 84, Loss: 0.928241491317749, Lr:0.0001\n",
      "Epoch 25, Step: 85, Loss: 0.44533342123031616, Lr:0.0001\n",
      "Epoch 25, Step: 86, Loss: 0.3536531925201416, Lr:0.0001\n",
      "Epoch 25, Step: 87, Loss: 0.5204735994338989, Lr:0.0001\n",
      "Epoch 25, Step: 88, Loss: 0.46835869550704956, Lr:0.0001\n",
      "Epoch 25, Step: 89, Loss: 0.634360671043396, Lr:0.0001\n",
      "Epoch 25, Step: 90, Loss: 0.4612686038017273, Lr:0.0001\n",
      "Epoch 25, Step: 91, Loss: 0.239673912525177, Lr:0.0001\n",
      "Epoch 25, Step: 92, Loss: 1.190170168876648, Lr:0.0001\n",
      "Epoch 25, Step: 93, Loss: 0.7172236442565918, Lr:0.0001\n",
      "Epoch 25, Step: 94, Loss: 0.6330936551094055, Lr:0.0001\n",
      "Epoch 25, Step: 95, Loss: 0.5411481857299805, Lr:0.0001\n",
      "Epoch 25, Step: 96, Loss: 0.28176549077033997, Lr:0.0001\n",
      "Epoch 25, Step: 97, Loss: 1.1243072748184204, Lr:0.0001\n",
      "Epoch 25, Step: 98, Loss: 0.9172685146331787, Lr:0.0001\n",
      "Epoch 25, Step: 99, Loss: 1.0247550010681152, Lr:0.0001\n",
      "Epoch 25, Step: 100, Loss: 0.9595546722412109, Lr:0.0001\n",
      "Epoch 25, Step: 101, Loss: 0.1602652221918106, Lr:0.0001\n",
      "Epoch 25, Step: 102, Loss: 1.2641658782958984, Lr:0.0001\n",
      "Epoch 25, Step: 103, Loss: 0.32828518748283386, Lr:0.0001\n",
      "Epoch 25, Step: 104, Loss: 0.15932980179786682, Lr:0.0001\n",
      "Epoch 25, Step: 105, Loss: 0.9617294073104858, Lr:0.0001\n",
      "Epoch 25, Step: 106, Loss: 1.143670916557312, Lr:0.0001\n",
      "Epoch 25, Step: 107, Loss: 0.650131106376648, Lr:0.0001\n",
      "Epoch 25, Step: 108, Loss: 1.0840725898742676, Lr:0.0001\n",
      "Epoch 25, Step: 109, Loss: 0.12943804264068604, Lr:0.0001\n",
      "Epoch 25, Step: 110, Loss: 1.2094385623931885, Lr:0.0001\n",
      "Epoch 25, Step: 111, Loss: 0.5356160402297974, Lr:0.0001\n",
      "Epoch 25, Step: 112, Loss: 1.3389549255371094, Lr:0.0001\n",
      "Epoch 25, Step: 113, Loss: 0.6114399433135986, Lr:0.0001\n",
      "Epoch 25, Step: 114, Loss: 0.19603830575942993, Lr:0.0001\n",
      "Epoch 25, Step: 115, Loss: 0.8923028111457825, Lr:0.0001\n",
      "Epoch 25, Step: 116, Loss: 1.0996519327163696, Lr:0.0001\n",
      "Epoch 25, Step: 117, Loss: 2.0331790447235107, Lr:0.0001\n",
      "Epoch 25, Step: 118, Loss: 0.5568297505378723, Lr:0.0001\n",
      "Epoch 25, Step: 119, Loss: 0.4424837827682495, Lr:0.0001\n",
      "Epoch 25, Step: 120, Loss: 0.25475430488586426, Lr:0.0001\n",
      "Epoch 25, Step: 121, Loss: 0.7154726982116699, Lr:0.0001\n",
      "Epoch 25, Step: 122, Loss: 0.7498106956481934, Lr:0.0001\n",
      "Epoch 25, Step: 123, Loss: 0.5934754610061646, Lr:0.0001\n",
      "Epoch 25, Step: 124, Loss: 0.37391963601112366, Lr:0.0001\n",
      "Epoch 25, Step: 125, Loss: 0.9014890193939209, Lr:0.0001\n",
      "Epoch 25, Step: 126, Loss: 0.9940028786659241, Lr:0.0001\n",
      "Epoch 25, Step: 127, Loss: 0.6710412502288818, Lr:0.0001\n",
      "Epoch 25, Step: 128, Loss: 0.969200074672699, Lr:0.0001\n",
      "Epoch 25, Step: 129, Loss: 0.19387970864772797, Lr:0.0001\n",
      "Epoch 25, Step: 130, Loss: 0.06969397515058517, Lr:0.0001\n",
      "Epoch 25, Step: 131, Loss: 1.3342258930206299, Lr:0.0001\n",
      "Epoch 25, Step: 132, Loss: 0.42607831954956055, Lr:0.0001\n",
      "Epoch 25, Step: 133, Loss: 1.0407664775848389, Lr:0.0001\n",
      "Epoch 25, Step: 134, Loss: 0.7778717279434204, Lr:0.0001\n",
      "Epoch 25, Step: 135, Loss: 0.8376555442810059, Lr:0.0001\n",
      "Epoch 25, Step: 136, Loss: 0.6588224172592163, Lr:0.0001\n",
      "Epoch 25, Step: 137, Loss: 0.7162073254585266, Lr:0.0001\n",
      "Epoch 25, Step: 138, Loss: 0.7377611994743347, Lr:0.0001\n",
      "Epoch 25, Step: 139, Loss: 0.5587077736854553, Lr:0.0001\n",
      "Epoch 25, Step: 140, Loss: 0.5769847631454468, Lr:0.0001\n",
      "Epoch 25, Step: 141, Loss: 0.453543484210968, Lr:0.0001\n",
      "Epoch 25, Step: 142, Loss: 0.19389213621616364, Lr:0.0001\n",
      "Epoch 25, Step: 143, Loss: 1.8725905418395996, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 25\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.2309 (0.2309) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5627 (0.8437) acc1: 100.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0062 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0064 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.844\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 26, Step: 0, Loss: 0.6233762502670288, Lr:0.0001\n",
      "Epoch 26, Step: 1, Loss: 2.402348041534424, Lr:0.0001\n",
      "Epoch 26, Step: 2, Loss: 0.9025108218193054, Lr:0.0001\n",
      "Epoch 26, Step: 3, Loss: 0.6383102536201477, Lr:0.0001\n",
      "Epoch 26, Step: 4, Loss: 1.3013527393341064, Lr:0.0001\n",
      "Epoch 26, Step: 5, Loss: 1.249289870262146, Lr:0.0001\n",
      "Epoch 26, Step: 6, Loss: 0.03785961493849754, Lr:0.0001\n",
      "Epoch 26, Step: 7, Loss: 0.4098489582538605, Lr:0.0001\n",
      "Epoch 26, Step: 8, Loss: 0.2442861944437027, Lr:0.0001\n",
      "Epoch 26, Step: 9, Loss: 0.682012677192688, Lr:0.0001\n",
      "Epoch 26, Step: 10, Loss: 1.135745644569397, Lr:0.0001\n",
      "Epoch 26, Step: 11, Loss: 0.3844093680381775, Lr:0.0001\n",
      "Epoch 26, Step: 12, Loss: 0.43288615345954895, Lr:0.0001\n",
      "Epoch 26, Step: 13, Loss: 0.21882843971252441, Lr:0.0001\n",
      "Epoch 26, Step: 14, Loss: 0.24958308041095734, Lr:0.0001\n",
      "Epoch 26, Step: 15, Loss: 0.5437797904014587, Lr:0.0001\n",
      "Epoch 26, Step: 16, Loss: 0.6699846982955933, Lr:0.0001\n",
      "Epoch 26, Step: 17, Loss: 0.8153631687164307, Lr:0.0001\n",
      "Epoch 26, Step: 18, Loss: 0.7968779802322388, Lr:0.0001\n",
      "Epoch 26, Step: 19, Loss: 0.318987101316452, Lr:0.0001\n",
      "Epoch 26, Step: 20, Loss: 0.3122493326663971, Lr:0.0001\n",
      "Epoch 26, Step: 21, Loss: 0.3844432830810547, Lr:0.0001\n",
      "Epoch 26, Step: 22, Loss: 0.225918710231781, Lr:0.0001\n",
      "Epoch 26, Step: 23, Loss: 0.5223121047019958, Lr:0.0001\n",
      "Epoch 26, Step: 24, Loss: 0.708916425704956, Lr:0.0001\n",
      "Epoch 26, Step: 25, Loss: 0.8734332323074341, Lr:0.0001\n",
      "Epoch 26, Step: 26, Loss: 0.5444779396057129, Lr:0.0001\n",
      "Epoch 26, Step: 27, Loss: 3.0420947074890137, Lr:0.0001\n",
      "Epoch 26, Step: 28, Loss: 0.273777037858963, Lr:0.0001\n",
      "Epoch 26, Step: 29, Loss: 0.3393042981624603, Lr:0.0001\n",
      "Epoch 26, Step: 30, Loss: 0.3952551484107971, Lr:0.0001\n",
      "Epoch 26, Step: 31, Loss: 0.32233232259750366, Lr:0.0001\n",
      "Epoch 26, Step: 32, Loss: 1.0330321788787842, Lr:0.0001\n",
      "Epoch 26, Step: 33, Loss: 0.6357136964797974, Lr:0.0001\n",
      "Epoch 26, Step: 34, Loss: 0.5589224100112915, Lr:0.0001\n",
      "Epoch 26, Step: 35, Loss: 0.36376863718032837, Lr:0.0001\n",
      "Epoch 26, Step: 36, Loss: 0.9125330448150635, Lr:0.0001\n",
      "Epoch 26, Step: 37, Loss: 1.1236132383346558, Lr:0.0001\n",
      "Epoch 26, Step: 38, Loss: 0.46076399087905884, Lr:0.0001\n",
      "Epoch 26, Step: 39, Loss: 0.28941279649734497, Lr:0.0001\n",
      "Epoch 26, Step: 40, Loss: 0.26637840270996094, Lr:0.0001\n",
      "Epoch 26, Step: 41, Loss: 1.0682027339935303, Lr:0.0001\n",
      "Epoch 26, Step: 42, Loss: 0.9126060605049133, Lr:0.0001\n",
      "Epoch 26, Step: 43, Loss: 0.5456419587135315, Lr:0.0001\n",
      "Epoch 26, Step: 44, Loss: 0.07376705855131149, Lr:0.0001\n",
      "Epoch 26, Step: 45, Loss: 0.530383825302124, Lr:0.0001\n",
      "Epoch 26, Step: 46, Loss: 0.747308611869812, Lr:0.0001\n",
      "Epoch 26, Step: 47, Loss: 0.40213528275489807, Lr:0.0001\n",
      "Epoch 26, Step: 48, Loss: 0.9008488655090332, Lr:0.0001\n",
      "Epoch 26, Step: 49, Loss: 0.9715143442153931, Lr:0.0001\n",
      "Epoch 26, Step: 50, Loss: 0.6022258996963501, Lr:0.0001\n",
      "Epoch 26, Step: 51, Loss: 0.35299474000930786, Lr:0.0001\n",
      "Epoch 26, Step: 52, Loss: 2.0523009300231934, Lr:0.0001\n",
      "Epoch 26, Step: 53, Loss: 0.2952590882778168, Lr:0.0001\n",
      "Epoch 26, Step: 54, Loss: 0.27854377031326294, Lr:0.0001\n",
      "Epoch 26, Step: 55, Loss: 1.2186915874481201, Lr:0.0001\n",
      "Epoch 26, Step: 56, Loss: 0.45957717299461365, Lr:0.0001\n",
      "Epoch 26, Step: 57, Loss: 0.2740393877029419, Lr:0.0001\n",
      "Epoch 26, Step: 58, Loss: 0.8664511442184448, Lr:0.0001\n",
      "Epoch 26, Step: 59, Loss: 0.5119143724441528, Lr:0.0001\n",
      "Epoch 26, Step: 60, Loss: 0.2194872498512268, Lr:0.0001\n",
      "Epoch 26, Step: 61, Loss: 0.5405640006065369, Lr:0.0001\n",
      "Epoch 26, Step: 62, Loss: 0.617722749710083, Lr:0.0001\n",
      "Epoch 26, Step: 63, Loss: 0.7616360187530518, Lr:0.0001\n",
      "Epoch 26, Step: 64, Loss: 0.569770097732544, Lr:0.0001\n",
      "Epoch 26, Step: 65, Loss: 0.4064323306083679, Lr:0.0001\n",
      "Epoch 26, Step: 66, Loss: 0.6818027496337891, Lr:0.0001\n",
      "Epoch 26, Step: 67, Loss: 0.5969280004501343, Lr:0.0001\n",
      "Epoch 26, Step: 68, Loss: 1.0169858932495117, Lr:0.0001\n",
      "Epoch 26, Step: 69, Loss: 1.6722307205200195, Lr:0.0001\n",
      "Epoch 26, Step: 70, Loss: 1.1520353555679321, Lr:0.0001\n",
      "Epoch 26, Step: 71, Loss: 1.0945907831192017, Lr:0.0001\n",
      "Epoch 26, Step: 72, Loss: 1.0181987285614014, Lr:0.0001\n",
      "Epoch 26, Step: 73, Loss: 0.47574418783187866, Lr:0.0001\n",
      "Epoch 26, Step: 74, Loss: 0.30033671855926514, Lr:0.0001\n",
      "Epoch 26, Step: 75, Loss: 0.5014094114303589, Lr:0.0001\n",
      "Epoch 26, Step: 76, Loss: 0.9326061010360718, Lr:0.0001\n",
      "Epoch 26, Step: 77, Loss: 0.8958202004432678, Lr:0.0001\n",
      "Epoch 26, Step: 78, Loss: 0.44663336873054504, Lr:0.0001\n",
      "Epoch 26, Step: 79, Loss: 1.7022123336791992, Lr:0.0001\n",
      "Epoch 26, Step: 80, Loss: 0.32459932565689087, Lr:0.0001\n",
      "Epoch 26, Step: 81, Loss: 0.7889101505279541, Lr:0.0001\n",
      "Epoch 26, Step: 82, Loss: 0.9979553818702698, Lr:0.0001\n",
      "Epoch 26, Step: 83, Loss: 0.6908711194992065, Lr:0.0001\n",
      "Epoch 26, Step: 84, Loss: 0.1603516936302185, Lr:0.0001\n",
      "Epoch 26, Step: 85, Loss: 1.524590015411377, Lr:0.0001\n",
      "Epoch 26, Step: 86, Loss: 1.2449512481689453, Lr:0.0001\n",
      "Epoch 26, Step: 87, Loss: 0.40903350710868835, Lr:0.0001\n",
      "Epoch 26, Step: 88, Loss: 0.029835321009159088, Lr:0.0001\n",
      "Epoch 26, Step: 89, Loss: 0.3293713927268982, Lr:0.0001\n",
      "Epoch 26, Step: 90, Loss: 0.2577981650829315, Lr:0.0001\n",
      "Epoch 26, Step: 91, Loss: 0.5113945007324219, Lr:0.0001\n",
      "Epoch 26, Step: 92, Loss: 0.8488360047340393, Lr:0.0001\n",
      "Epoch 26, Step: 93, Loss: 0.568533182144165, Lr:0.0001\n",
      "Epoch 26, Step: 94, Loss: 0.21113654971122742, Lr:0.0001\n",
      "Epoch 26, Step: 95, Loss: 0.5702173709869385, Lr:0.0001\n",
      "Epoch 26, Step: 96, Loss: 1.0820841789245605, Lr:0.0001\n",
      "Epoch 26, Step: 97, Loss: 0.5813590288162231, Lr:0.0001\n",
      "Epoch 26, Step: 98, Loss: 0.911217451095581, Lr:0.0001\n",
      "Epoch 26, Step: 99, Loss: 0.7868945598602295, Lr:0.0001\n",
      "Epoch 26, Step: 100, Loss: 0.6372860074043274, Lr:0.0001\n",
      "Epoch 26, Step: 101, Loss: 0.4181273877620697, Lr:0.0001\n",
      "Epoch 26, Step: 102, Loss: 0.700416624546051, Lr:0.0001\n",
      "Epoch 26, Step: 103, Loss: 0.8103959560394287, Lr:0.0001\n",
      "Epoch 26, Step: 104, Loss: 0.694901704788208, Lr:0.0001\n",
      "Epoch 26, Step: 105, Loss: 0.3710247278213501, Lr:0.0001\n",
      "Epoch 26, Step: 106, Loss: 0.3869571089744568, Lr:0.0001\n",
      "Epoch 26, Step: 107, Loss: 1.435654640197754, Lr:0.0001\n",
      "Epoch 26, Step: 108, Loss: 1.3751049041748047, Lr:0.0001\n",
      "Epoch 26, Step: 109, Loss: 0.7121589183807373, Lr:0.0001\n",
      "Epoch 26, Step: 110, Loss: 0.6489753127098083, Lr:0.0001\n",
      "Epoch 26, Step: 111, Loss: 0.4877019226551056, Lr:0.0001\n",
      "Epoch 26, Step: 112, Loss: 2.0561299324035645, Lr:0.0001\n",
      "Epoch 26, Step: 113, Loss: 0.49497777223587036, Lr:0.0001\n",
      "Epoch 26, Step: 114, Loss: 0.38905981183052063, Lr:0.0001\n",
      "Epoch 26, Step: 115, Loss: 1.1336305141448975, Lr:0.0001\n",
      "Epoch 26, Step: 116, Loss: 0.9056841135025024, Lr:0.0001\n",
      "Epoch 26, Step: 117, Loss: 1.10978364944458, Lr:0.0001\n",
      "Epoch 26, Step: 118, Loss: 1.171156406402588, Lr:0.0001\n",
      "Epoch 26, Step: 119, Loss: 0.03310757130384445, Lr:0.0001\n",
      "Epoch 26, Step: 120, Loss: 1.2321486473083496, Lr:0.0001\n",
      "Epoch 26, Step: 121, Loss: 0.8993018865585327, Lr:0.0001\n",
      "Epoch 26, Step: 122, Loss: 0.23793914914131165, Lr:0.0001\n",
      "Epoch 26, Step: 123, Loss: 0.4751797914505005, Lr:0.0001\n",
      "Epoch 26, Step: 124, Loss: 0.04740377515554428, Lr:0.0001\n",
      "Epoch 26, Step: 125, Loss: 0.4267466068267822, Lr:0.0001\n",
      "Epoch 26, Step: 126, Loss: 1.375399112701416, Lr:0.0001\n",
      "Epoch 26, Step: 127, Loss: 0.18875563144683838, Lr:0.0001\n",
      "Epoch 26, Step: 128, Loss: 1.0198627710342407, Lr:0.0001\n",
      "Epoch 26, Step: 129, Loss: 0.28165552020072937, Lr:0.0001\n",
      "Epoch 26, Step: 130, Loss: 0.4632571339607239, Lr:0.0001\n",
      "Epoch 26, Step: 131, Loss: 0.40276041626930237, Lr:0.0001\n",
      "Epoch 26, Step: 132, Loss: 0.4463360905647278, Lr:0.0001\n",
      "Epoch 26, Step: 133, Loss: 0.2536952793598175, Lr:0.0001\n",
      "Epoch 26, Step: 134, Loss: 0.7053794860839844, Lr:0.0001\n",
      "Epoch 26, Step: 135, Loss: 0.495533287525177, Lr:0.0001\n",
      "Epoch 26, Step: 136, Loss: 0.9816500544548035, Lr:0.0001\n",
      "Epoch 26, Step: 137, Loss: 1.2718112468719482, Lr:0.0001\n",
      "Epoch 26, Step: 138, Loss: 0.6309027671813965, Lr:0.0001\n",
      "Epoch 26, Step: 139, Loss: 0.5253541469573975, Lr:0.0001\n",
      "Epoch 26, Step: 140, Loss: 0.04173098877072334, Lr:0.0001\n",
      "Epoch 26, Step: 141, Loss: 0.7861126065254211, Lr:0.0001\n",
      "Epoch 26, Step: 142, Loss: 1.1910996437072754, Lr:0.0001\n",
      "Epoch 26, Step: 143, Loss: 2.1652090549468994, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 26\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1038 (0.1038) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.6242 (1.0687) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0062 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0062 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 1.069\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 27, Step: 0, Loss: 1.2464648485183716, Lr:0.0001\n",
      "Epoch 27, Step: 1, Loss: 0.412112295627594, Lr:0.0001\n",
      "Epoch 27, Step: 2, Loss: 0.7888628244400024, Lr:0.0001\n",
      "Epoch 27, Step: 3, Loss: 1.2407277822494507, Lr:0.0001\n",
      "Epoch 27, Step: 4, Loss: 0.6524540185928345, Lr:0.0001\n",
      "Epoch 27, Step: 5, Loss: 0.6186791062355042, Lr:0.0001\n",
      "Epoch 27, Step: 6, Loss: 0.4732339680194855, Lr:0.0001\n",
      "Epoch 27, Step: 7, Loss: 0.8632168769836426, Lr:0.0001\n",
      "Epoch 27, Step: 8, Loss: 0.10580042004585266, Lr:0.0001\n",
      "Epoch 27, Step: 9, Loss: 0.1900181621313095, Lr:0.0001\n",
      "Epoch 27, Step: 10, Loss: 0.2747022211551666, Lr:0.0001\n",
      "Epoch 27, Step: 11, Loss: 0.46184536814689636, Lr:0.0001\n",
      "Epoch 27, Step: 12, Loss: 1.2123804092407227, Lr:0.0001\n",
      "Epoch 27, Step: 13, Loss: 0.8040658235549927, Lr:0.0001\n",
      "Epoch 27, Step: 14, Loss: 0.8822269439697266, Lr:0.0001\n",
      "Epoch 27, Step: 15, Loss: 0.611295759677887, Lr:0.0001\n",
      "Epoch 27, Step: 16, Loss: 0.02649136260151863, Lr:0.0001\n",
      "Epoch 27, Step: 17, Loss: 0.750927209854126, Lr:0.0001\n",
      "Epoch 27, Step: 18, Loss: 0.48219576478004456, Lr:0.0001\n",
      "Epoch 27, Step: 19, Loss: 0.4159471392631531, Lr:0.0001\n",
      "Epoch 27, Step: 20, Loss: 0.8764830827713013, Lr:0.0001\n",
      "Epoch 27, Step: 21, Loss: 0.9582704305648804, Lr:0.0001\n",
      "Epoch 27, Step: 22, Loss: 1.5944080352783203, Lr:0.0001\n",
      "Epoch 27, Step: 23, Loss: 0.2823326587677002, Lr:0.0001\n",
      "Epoch 27, Step: 24, Loss: 0.7729899287223816, Lr:0.0001\n",
      "Epoch 27, Step: 25, Loss: 0.4297047257423401, Lr:0.0001\n",
      "Epoch 27, Step: 26, Loss: 1.1051316261291504, Lr:0.0001\n",
      "Epoch 27, Step: 27, Loss: 0.45925354957580566, Lr:0.0001\n",
      "Epoch 27, Step: 28, Loss: 0.7490030527114868, Lr:0.0001\n",
      "Epoch 27, Step: 29, Loss: 0.2672611176967621, Lr:0.0001\n",
      "Epoch 27, Step: 30, Loss: 0.7285439968109131, Lr:0.0001\n",
      "Epoch 27, Step: 31, Loss: 0.8739035129547119, Lr:0.0001\n",
      "Epoch 27, Step: 32, Loss: 0.7288087606430054, Lr:0.0001\n",
      "Epoch 27, Step: 33, Loss: 0.5514355301856995, Lr:0.0001\n",
      "Epoch 27, Step: 34, Loss: 0.26692280173301697, Lr:0.0001\n",
      "Epoch 27, Step: 35, Loss: 0.6429515480995178, Lr:0.0001\n",
      "Epoch 27, Step: 36, Loss: 0.7612432837486267, Lr:0.0001\n",
      "Epoch 27, Step: 37, Loss: 1.8737704753875732, Lr:0.0001\n",
      "Epoch 27, Step: 38, Loss: 0.2569156885147095, Lr:0.0001\n",
      "Epoch 27, Step: 39, Loss: 0.8350557684898376, Lr:0.0001\n",
      "Epoch 27, Step: 40, Loss: 1.5925226211547852, Lr:0.0001\n",
      "Epoch 27, Step: 41, Loss: 0.04511328041553497, Lr:0.0001\n",
      "Epoch 27, Step: 42, Loss: 0.48429539799690247, Lr:0.0001\n",
      "Epoch 27, Step: 43, Loss: 0.4625803530216217, Lr:0.0001\n",
      "Epoch 27, Step: 44, Loss: 0.9105724096298218, Lr:0.0001\n",
      "Epoch 27, Step: 45, Loss: 2.1919641494750977, Lr:0.0001\n",
      "Epoch 27, Step: 46, Loss: 1.68288254737854, Lr:0.0001\n",
      "Epoch 27, Step: 47, Loss: 0.7224816679954529, Lr:0.0001\n",
      "Epoch 27, Step: 48, Loss: 0.2257961630821228, Lr:0.0001\n",
      "Epoch 27, Step: 49, Loss: 0.6158597469329834, Lr:0.0001\n",
      "Epoch 27, Step: 50, Loss: 0.34057921171188354, Lr:0.0001\n",
      "Epoch 27, Step: 51, Loss: 0.740401029586792, Lr:0.0001\n",
      "Epoch 27, Step: 52, Loss: 0.3831496834754944, Lr:0.0001\n",
      "Epoch 27, Step: 53, Loss: 0.8152943849563599, Lr:0.0001\n",
      "Epoch 27, Step: 54, Loss: 0.5144143104553223, Lr:0.0001\n",
      "Epoch 27, Step: 55, Loss: 0.6273886561393738, Lr:0.0001\n",
      "Epoch 27, Step: 56, Loss: 1.0237855911254883, Lr:0.0001\n",
      "Epoch 27, Step: 57, Loss: 0.08255165815353394, Lr:0.0001\n",
      "Epoch 27, Step: 58, Loss: 0.5794073939323425, Lr:0.0001\n",
      "Epoch 27, Step: 59, Loss: 0.5147858262062073, Lr:0.0001\n",
      "Epoch 27, Step: 60, Loss: 0.7168893814086914, Lr:0.0001\n",
      "Epoch 27, Step: 61, Loss: 1.9362457990646362, Lr:0.0001\n",
      "Epoch 27, Step: 62, Loss: 2.300163507461548, Lr:0.0001\n",
      "Epoch 27, Step: 63, Loss: 1.322979211807251, Lr:0.0001\n",
      "Epoch 27, Step: 64, Loss: 1.3175315856933594, Lr:0.0001\n",
      "Epoch 27, Step: 65, Loss: 0.6614112257957458, Lr:0.0001\n",
      "Epoch 27, Step: 66, Loss: 0.5698295831680298, Lr:0.0001\n",
      "Epoch 27, Step: 67, Loss: 1.0577726364135742, Lr:0.0001\n",
      "Epoch 27, Step: 68, Loss: 0.49962055683135986, Lr:0.0001\n",
      "Epoch 27, Step: 69, Loss: 0.9645848274230957, Lr:0.0001\n",
      "Epoch 27, Step: 70, Loss: 0.4329532980918884, Lr:0.0001\n",
      "Epoch 27, Step: 71, Loss: 0.3293321430683136, Lr:0.0001\n",
      "Epoch 27, Step: 72, Loss: 0.2322966456413269, Lr:0.0001\n",
      "Epoch 27, Step: 73, Loss: 0.6089534759521484, Lr:0.0001\n",
      "Epoch 27, Step: 74, Loss: 1.1685802936553955, Lr:0.0001\n",
      "Epoch 27, Step: 75, Loss: 0.7462639808654785, Lr:0.0001\n",
      "Epoch 27, Step: 76, Loss: 0.6533762216567993, Lr:0.0001\n",
      "Epoch 27, Step: 77, Loss: 0.44267362356185913, Lr:0.0001\n",
      "Epoch 27, Step: 78, Loss: 0.718773365020752, Lr:0.0001\n",
      "Epoch 27, Step: 79, Loss: 0.4615630507469177, Lr:0.0001\n",
      "Epoch 27, Step: 80, Loss: 0.916968822479248, Lr:0.0001\n",
      "Epoch 27, Step: 81, Loss: 1.2902424335479736, Lr:0.0001\n",
      "Epoch 27, Step: 82, Loss: 0.6188193559646606, Lr:0.0001\n",
      "Epoch 27, Step: 83, Loss: 0.6479533314704895, Lr:0.0001\n",
      "Epoch 27, Step: 84, Loss: 0.14384378492832184, Lr:0.0001\n",
      "Epoch 27, Step: 85, Loss: 0.7385869026184082, Lr:0.0001\n",
      "Epoch 27, Step: 86, Loss: 0.21921038627624512, Lr:0.0001\n",
      "Epoch 27, Step: 87, Loss: 1.0142207145690918, Lr:0.0001\n",
      "Epoch 27, Step: 88, Loss: 0.22529847919940948, Lr:0.0001\n",
      "Epoch 27, Step: 89, Loss: 0.7239190340042114, Lr:0.0001\n",
      "Epoch 27, Step: 90, Loss: 0.3790825307369232, Lr:0.0001\n",
      "Epoch 27, Step: 91, Loss: 0.6398048996925354, Lr:0.0001\n",
      "Epoch 27, Step: 92, Loss: 0.991020679473877, Lr:0.0001\n",
      "Epoch 27, Step: 93, Loss: 0.4839330315589905, Lr:0.0001\n",
      "Epoch 27, Step: 94, Loss: 1.0005643367767334, Lr:0.0001\n",
      "Epoch 27, Step: 95, Loss: 0.36931461095809937, Lr:0.0001\n",
      "Epoch 27, Step: 96, Loss: 0.15897542238235474, Lr:0.0001\n",
      "Epoch 27, Step: 97, Loss: 0.5684428215026855, Lr:0.0001\n",
      "Epoch 27, Step: 98, Loss: 0.8922117352485657, Lr:0.0001\n",
      "Epoch 27, Step: 99, Loss: 0.7802467346191406, Lr:0.0001\n",
      "Epoch 27, Step: 100, Loss: 1.028747797012329, Lr:0.0001\n",
      "Epoch 27, Step: 101, Loss: 0.04155253618955612, Lr:0.0001\n",
      "Epoch 27, Step: 102, Loss: 0.9355190396308899, Lr:0.0001\n",
      "Epoch 27, Step: 103, Loss: 1.2929316759109497, Lr:0.0001\n",
      "Epoch 27, Step: 104, Loss: 0.4507313668727875, Lr:0.0001\n",
      "Epoch 27, Step: 105, Loss: 0.6067433953285217, Lr:0.0001\n",
      "Epoch 27, Step: 106, Loss: 0.4382374882698059, Lr:0.0001\n",
      "Epoch 27, Step: 107, Loss: 0.6132992506027222, Lr:0.0001\n",
      "Epoch 27, Step: 108, Loss: 0.37920230627059937, Lr:0.0001\n",
      "Epoch 27, Step: 109, Loss: 0.6810265779495239, Lr:0.0001\n",
      "Epoch 27, Step: 110, Loss: 0.09559277445077896, Lr:0.0001\n",
      "Epoch 27, Step: 111, Loss: 0.6202340126037598, Lr:0.0001\n",
      "Epoch 27, Step: 112, Loss: 0.5687797665596008, Lr:0.0001\n",
      "Epoch 27, Step: 113, Loss: 1.4939676523208618, Lr:0.0001\n",
      "Epoch 27, Step: 114, Loss: 0.28000539541244507, Lr:0.0001\n",
      "Epoch 27, Step: 115, Loss: 0.5569763779640198, Lr:0.0001\n",
      "Epoch 27, Step: 116, Loss: 0.2239227443933487, Lr:0.0001\n",
      "Epoch 27, Step: 117, Loss: 0.5145136117935181, Lr:0.0001\n",
      "Epoch 27, Step: 118, Loss: 0.5774659514427185, Lr:0.0001\n",
      "Epoch 27, Step: 119, Loss: 0.14412927627563477, Lr:0.0001\n",
      "Epoch 27, Step: 120, Loss: 0.5077146887779236, Lr:0.0001\n",
      "Epoch 27, Step: 121, Loss: 0.7715353965759277, Lr:0.0001\n",
      "Epoch 27, Step: 122, Loss: 0.49074867367744446, Lr:0.0001\n",
      "Epoch 27, Step: 123, Loss: 1.656585454940796, Lr:0.0001\n",
      "Epoch 27, Step: 124, Loss: 0.3830772638320923, Lr:0.0001\n",
      "Epoch 27, Step: 125, Loss: 1.3627687692642212, Lr:0.0001\n",
      "Epoch 27, Step: 126, Loss: 0.9668271541595459, Lr:0.0001\n",
      "Epoch 27, Step: 127, Loss: 0.5212312936782837, Lr:0.0001\n",
      "Epoch 27, Step: 128, Loss: 1.689530372619629, Lr:0.0001\n",
      "Epoch 27, Step: 129, Loss: 0.8969462513923645, Lr:0.0001\n",
      "Epoch 27, Step: 130, Loss: 0.23678211867809296, Lr:0.0001\n",
      "Epoch 27, Step: 131, Loss: 0.5318170189857483, Lr:0.0001\n",
      "Epoch 27, Step: 132, Loss: 0.5183858871459961, Lr:0.0001\n",
      "Epoch 27, Step: 133, Loss: 0.5815122723579407, Lr:0.0001\n",
      "Epoch 27, Step: 134, Loss: 0.4380561113357544, Lr:0.0001\n",
      "Epoch 27, Step: 135, Loss: 0.7657516002655029, Lr:0.0001\n",
      "Epoch 27, Step: 136, Loss: 1.0909953117370605, Lr:0.0001\n",
      "Epoch 27, Step: 137, Loss: 0.28350356221199036, Lr:0.0001\n",
      "Epoch 27, Step: 138, Loss: 0.7136490345001221, Lr:0.0001\n",
      "Epoch 27, Step: 139, Loss: 0.47966769337654114, Lr:0.0001\n",
      "Epoch 27, Step: 140, Loss: 0.7686408758163452, Lr:0.0001\n",
      "Epoch 27, Step: 141, Loss: 0.47621238231658936, Lr:0.0001\n",
      "Epoch 27, Step: 142, Loss: 0.9649713635444641, Lr:0.0001\n",
      "Epoch 27, Step: 143, Loss: 1.1965728998184204, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 27\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0392 (0.0392) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5690 (0.8618) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0065 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0065 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 0.862\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 28, Step: 0, Loss: 0.27461445331573486, Lr:0.0001\n",
      "Epoch 28, Step: 1, Loss: 0.4437955617904663, Lr:0.0001\n",
      "Epoch 28, Step: 2, Loss: 0.7044392228126526, Lr:0.0001\n",
      "Epoch 28, Step: 3, Loss: 0.7928666472434998, Lr:0.0001\n",
      "Epoch 28, Step: 4, Loss: 0.36728930473327637, Lr:0.0001\n",
      "Epoch 28, Step: 5, Loss: 1.7264525890350342, Lr:0.0001\n",
      "Epoch 28, Step: 6, Loss: 0.6341923475265503, Lr:0.0001\n",
      "Epoch 28, Step: 7, Loss: 1.1770625114440918, Lr:0.0001\n",
      "Epoch 28, Step: 8, Loss: 0.22746901214122772, Lr:0.0001\n",
      "Epoch 28, Step: 9, Loss: 0.6384789943695068, Lr:0.0001\n",
      "Epoch 28, Step: 10, Loss: 0.37050265073776245, Lr:0.0001\n",
      "Epoch 28, Step: 11, Loss: 0.5888022184371948, Lr:0.0001\n",
      "Epoch 28, Step: 12, Loss: 0.20687662065029144, Lr:0.0001\n",
      "Epoch 28, Step: 13, Loss: 0.1691257506608963, Lr:0.0001\n",
      "Epoch 28, Step: 14, Loss: 0.9010602831840515, Lr:0.0001\n",
      "Epoch 28, Step: 15, Loss: 0.3242076635360718, Lr:0.0001\n",
      "Epoch 28, Step: 16, Loss: 0.44051557779312134, Lr:0.0001\n",
      "Epoch 28, Step: 17, Loss: 0.23366332054138184, Lr:0.0001\n",
      "Epoch 28, Step: 18, Loss: 0.3268701136112213, Lr:0.0001\n",
      "Epoch 28, Step: 19, Loss: 0.47862932085990906, Lr:0.0001\n",
      "Epoch 28, Step: 20, Loss: 0.8493919372558594, Lr:0.0001\n",
      "Epoch 28, Step: 21, Loss: 0.4362978935241699, Lr:0.0001\n",
      "Epoch 28, Step: 22, Loss: 0.4465368986129761, Lr:0.0001\n",
      "Epoch 28, Step: 23, Loss: 0.3014058470726013, Lr:0.0001\n",
      "Epoch 28, Step: 24, Loss: 0.589891254901886, Lr:0.0001\n",
      "Epoch 28, Step: 25, Loss: 1.0352935791015625, Lr:0.0001\n",
      "Epoch 28, Step: 26, Loss: 1.096299648284912, Lr:0.0001\n",
      "Epoch 28, Step: 27, Loss: 0.9656795263290405, Lr:0.0001\n",
      "Epoch 28, Step: 28, Loss: 0.5021127462387085, Lr:0.0001\n",
      "Epoch 28, Step: 29, Loss: 0.4187661111354828, Lr:0.0001\n",
      "Epoch 28, Step: 30, Loss: 0.41343262791633606, Lr:0.0001\n",
      "Epoch 28, Step: 31, Loss: 0.19299694895744324, Lr:0.0001\n",
      "Epoch 28, Step: 32, Loss: 0.3866637647151947, Lr:0.0001\n",
      "Epoch 28, Step: 33, Loss: 0.8537502884864807, Lr:0.0001\n",
      "Epoch 28, Step: 34, Loss: 0.5412386655807495, Lr:0.0001\n",
      "Epoch 28, Step: 35, Loss: 0.9898077845573425, Lr:0.0001\n",
      "Epoch 28, Step: 36, Loss: 0.7519726753234863, Lr:0.0001\n",
      "Epoch 28, Step: 37, Loss: 0.4342409074306488, Lr:0.0001\n",
      "Epoch 28, Step: 38, Loss: 0.3260895609855652, Lr:0.0001\n",
      "Epoch 28, Step: 39, Loss: 1.0109158754348755, Lr:0.0001\n",
      "Epoch 28, Step: 40, Loss: 0.4685351848602295, Lr:0.0001\n",
      "Epoch 28, Step: 41, Loss: 2.004271984100342, Lr:0.0001\n",
      "Epoch 28, Step: 42, Loss: 0.9111273288726807, Lr:0.0001\n",
      "Epoch 28, Step: 43, Loss: 0.632969319820404, Lr:0.0001\n",
      "Epoch 28, Step: 44, Loss: 1.1038891077041626, Lr:0.0001\n",
      "Epoch 28, Step: 45, Loss: 0.8376622796058655, Lr:0.0001\n",
      "Epoch 28, Step: 46, Loss: 0.7685655355453491, Lr:0.0001\n",
      "Epoch 28, Step: 47, Loss: 0.6232191920280457, Lr:0.0001\n",
      "Epoch 28, Step: 48, Loss: 0.7882534265518188, Lr:0.0001\n",
      "Epoch 28, Step: 49, Loss: 0.3852185606956482, Lr:0.0001\n",
      "Epoch 28, Step: 50, Loss: 0.6800419688224792, Lr:0.0001\n",
      "Epoch 28, Step: 51, Loss: 1.5513806343078613, Lr:0.0001\n",
      "Epoch 28, Step: 52, Loss: 0.8364355564117432, Lr:0.0001\n",
      "Epoch 28, Step: 53, Loss: 0.32227158546447754, Lr:0.0001\n",
      "Epoch 28, Step: 54, Loss: 0.6016872525215149, Lr:0.0001\n",
      "Epoch 28, Step: 55, Loss: 0.8310053944587708, Lr:0.0001\n",
      "Epoch 28, Step: 56, Loss: 0.6071496605873108, Lr:0.0001\n",
      "Epoch 28, Step: 57, Loss: 0.9301391839981079, Lr:0.0001\n",
      "Epoch 28, Step: 58, Loss: 0.2636811137199402, Lr:0.0001\n",
      "Epoch 28, Step: 59, Loss: 0.1260056048631668, Lr:0.0001\n",
      "Epoch 28, Step: 60, Loss: 1.1335639953613281, Lr:0.0001\n",
      "Epoch 28, Step: 61, Loss: 0.4938983619213104, Lr:0.0001\n",
      "Epoch 28, Step: 62, Loss: 0.24134299159049988, Lr:0.0001\n",
      "Epoch 28, Step: 63, Loss: 0.7786043882369995, Lr:0.0001\n",
      "Epoch 28, Step: 64, Loss: 0.695827841758728, Lr:0.0001\n",
      "Epoch 28, Step: 65, Loss: 0.5331100821495056, Lr:0.0001\n",
      "Epoch 28, Step: 66, Loss: 0.31304413080215454, Lr:0.0001\n",
      "Epoch 28, Step: 67, Loss: 1.4122674465179443, Lr:0.0001\n",
      "Epoch 28, Step: 68, Loss: 0.6047711372375488, Lr:0.0001\n",
      "Epoch 28, Step: 69, Loss: 1.454861044883728, Lr:0.0001\n",
      "Epoch 28, Step: 70, Loss: 0.2981882691383362, Lr:0.0001\n",
      "Epoch 28, Step: 71, Loss: 0.48800814151763916, Lr:0.0001\n",
      "Epoch 28, Step: 72, Loss: 0.09703414887189865, Lr:0.0001\n",
      "Epoch 28, Step: 73, Loss: 0.39059165120124817, Lr:0.0001\n",
      "Epoch 28, Step: 74, Loss: 2.0576746463775635, Lr:0.0001\n",
      "Epoch 28, Step: 75, Loss: 1.2431342601776123, Lr:0.0001\n",
      "Epoch 28, Step: 76, Loss: 0.12784847617149353, Lr:0.0001\n",
      "Epoch 28, Step: 77, Loss: 2.6434502601623535, Lr:0.0001\n",
      "Epoch 28, Step: 78, Loss: 0.44208601117134094, Lr:0.0001\n",
      "Epoch 28, Step: 79, Loss: 0.37706950306892395, Lr:0.0001\n",
      "Epoch 28, Step: 80, Loss: 0.11501239985227585, Lr:0.0001\n",
      "Epoch 28, Step: 81, Loss: 0.8829382658004761, Lr:0.0001\n",
      "Epoch 28, Step: 82, Loss: 1.799080491065979, Lr:0.0001\n",
      "Epoch 28, Step: 83, Loss: 1.2480669021606445, Lr:0.0001\n",
      "Epoch 28, Step: 84, Loss: 0.4876556098461151, Lr:0.0001\n",
      "Epoch 28, Step: 85, Loss: 0.6488906145095825, Lr:0.0001\n",
      "Epoch 28, Step: 86, Loss: 1.1220152378082275, Lr:0.0001\n",
      "Epoch 28, Step: 87, Loss: 0.42296817898750305, Lr:0.0001\n",
      "Epoch 28, Step: 88, Loss: 1.2186191082000732, Lr:0.0001\n",
      "Epoch 28, Step: 89, Loss: 1.064096450805664, Lr:0.0001\n",
      "Epoch 28, Step: 90, Loss: 0.4984227120876312, Lr:0.0001\n",
      "Epoch 28, Step: 91, Loss: 0.30178025364875793, Lr:0.0001\n",
      "Epoch 28, Step: 92, Loss: 0.473170667886734, Lr:0.0001\n",
      "Epoch 28, Step: 93, Loss: 1.3300023078918457, Lr:0.0001\n",
      "Epoch 28, Step: 94, Loss: 0.30297714471817017, Lr:0.0001\n",
      "Epoch 28, Step: 95, Loss: 1.5513551235198975, Lr:0.0001\n",
      "Epoch 28, Step: 96, Loss: 0.8915213346481323, Lr:0.0001\n",
      "Epoch 28, Step: 97, Loss: 0.7461535334587097, Lr:0.0001\n",
      "Epoch 28, Step: 98, Loss: 0.6009887456893921, Lr:0.0001\n",
      "Epoch 28, Step: 99, Loss: 0.9784955978393555, Lr:0.0001\n",
      "Epoch 28, Step: 100, Loss: 0.8265320658683777, Lr:0.0001\n",
      "Epoch 28, Step: 101, Loss: 2.8620851039886475, Lr:0.0001\n",
      "Epoch 28, Step: 102, Loss: 0.658734917640686, Lr:0.0001\n",
      "Epoch 28, Step: 103, Loss: 0.7873955965042114, Lr:0.0001\n",
      "Epoch 28, Step: 104, Loss: 0.3841365873813629, Lr:0.0001\n",
      "Epoch 28, Step: 105, Loss: 0.12064199894666672, Lr:0.0001\n",
      "Epoch 28, Step: 106, Loss: 2.106332302093506, Lr:0.0001\n",
      "Epoch 28, Step: 107, Loss: 1.1367926597595215, Lr:0.0001\n",
      "Epoch 28, Step: 108, Loss: 0.2877817451953888, Lr:0.0001\n",
      "Epoch 28, Step: 109, Loss: 0.073897585272789, Lr:0.0001\n",
      "Epoch 28, Step: 110, Loss: 0.7894611358642578, Lr:0.0001\n",
      "Epoch 28, Step: 111, Loss: 2.3696534633636475, Lr:0.0001\n",
      "Epoch 28, Step: 112, Loss: 0.38793298602104187, Lr:0.0001\n",
      "Epoch 28, Step: 113, Loss: 1.0641045570373535, Lr:0.0001\n",
      "Epoch 28, Step: 114, Loss: 0.3507779538631439, Lr:0.0001\n",
      "Epoch 28, Step: 115, Loss: 1.1930806636810303, Lr:0.0001\n",
      "Epoch 28, Step: 116, Loss: 0.2426374852657318, Lr:0.0001\n",
      "Epoch 28, Step: 117, Loss: 0.7932721972465515, Lr:0.0001\n",
      "Epoch 28, Step: 118, Loss: 0.4209144711494446, Lr:0.0001\n",
      "Epoch 28, Step: 119, Loss: 0.3414800465106964, Lr:0.0001\n",
      "Epoch 28, Step: 120, Loss: 0.3337527811527252, Lr:0.0001\n",
      "Epoch 28, Step: 121, Loss: 0.6770646572113037, Lr:0.0001\n",
      "Epoch 28, Step: 122, Loss: 1.2072224617004395, Lr:0.0001\n",
      "Epoch 28, Step: 123, Loss: 0.393311470746994, Lr:0.0001\n",
      "Epoch 28, Step: 124, Loss: 1.298964500427246, Lr:0.0001\n",
      "Epoch 28, Step: 125, Loss: 0.42295247316360474, Lr:0.0001\n",
      "Epoch 28, Step: 126, Loss: 0.5216549634933472, Lr:0.0001\n",
      "Epoch 28, Step: 127, Loss: 0.23311391472816467, Lr:0.0001\n",
      "Epoch 28, Step: 128, Loss: 0.2947309911251068, Lr:0.0001\n",
      "Epoch 28, Step: 129, Loss: 0.8800776600837708, Lr:0.0001\n",
      "Epoch 28, Step: 130, Loss: 0.7438904643058777, Lr:0.0001\n",
      "Epoch 28, Step: 131, Loss: 0.8024328351020813, Lr:0.0001\n",
      "Epoch 28, Step: 132, Loss: 0.487861692905426, Lr:0.0001\n",
      "Epoch 28, Step: 133, Loss: 0.7033544182777405, Lr:0.0001\n",
      "Epoch 28, Step: 134, Loss: 0.6049161553382874, Lr:0.0001\n",
      "Epoch 28, Step: 135, Loss: 0.4488994777202606, Lr:0.0001\n",
      "Epoch 28, Step: 136, Loss: 0.3179074823856354, Lr:0.0001\n",
      "Epoch 28, Step: 137, Loss: 1.8036656379699707, Lr:0.0001\n",
      "Epoch 28, Step: 138, Loss: 0.07320745289325714, Lr:0.0001\n",
      "Epoch 28, Step: 139, Loss: 0.40315669775009155, Lr:0.0001\n",
      "Epoch 28, Step: 140, Loss: 0.8305344581604004, Lr:0.0001\n",
      "Epoch 28, Step: 141, Loss: 0.8492448329925537, Lr:0.0001\n",
      "Epoch 28, Step: 142, Loss: 0.8549612164497375, Lr:0.0001\n",
      "Epoch 28, Step: 143, Loss: 0.6900036334991455, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 28\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1213 (0.1213) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.6258 (1.0032) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0061 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0062 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 1.003\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 29, Step: 0, Loss: 0.5084518194198608, Lr:0.0001\n",
      "Epoch 29, Step: 1, Loss: 0.6582303643226624, Lr:0.0001\n",
      "Epoch 29, Step: 2, Loss: 0.14000245928764343, Lr:0.0001\n",
      "Epoch 29, Step: 3, Loss: 0.5168668031692505, Lr:0.0001\n",
      "Epoch 29, Step: 4, Loss: 0.6851373910903931, Lr:0.0001\n",
      "Epoch 29, Step: 5, Loss: 0.7711238265037537, Lr:0.0001\n",
      "Epoch 29, Step: 6, Loss: 0.4741317927837372, Lr:0.0001\n",
      "Epoch 29, Step: 7, Loss: 0.24201524257659912, Lr:0.0001\n",
      "Epoch 29, Step: 8, Loss: 0.5168979167938232, Lr:0.0001\n",
      "Epoch 29, Step: 9, Loss: 0.847970724105835, Lr:0.0001\n",
      "Epoch 29, Step: 10, Loss: 0.6678063273429871, Lr:0.0001\n",
      "Epoch 29, Step: 11, Loss: 0.8439925909042358, Lr:0.0001\n",
      "Epoch 29, Step: 12, Loss: 0.33303189277648926, Lr:0.0001\n",
      "Epoch 29, Step: 13, Loss: 0.3952316641807556, Lr:0.0001\n",
      "Epoch 29, Step: 14, Loss: 0.233342245221138, Lr:0.0001\n",
      "Epoch 29, Step: 15, Loss: 0.6312240958213806, Lr:0.0001\n",
      "Epoch 29, Step: 16, Loss: 0.7505334615707397, Lr:0.0001\n",
      "Epoch 29, Step: 17, Loss: 0.3975503146648407, Lr:0.0001\n",
      "Epoch 29, Step: 18, Loss: 1.3630928993225098, Lr:0.0001\n",
      "Epoch 29, Step: 19, Loss: 0.42873814702033997, Lr:0.0001\n",
      "Epoch 29, Step: 20, Loss: 0.01211514137685299, Lr:0.0001\n",
      "Epoch 29, Step: 21, Loss: 0.4338752031326294, Lr:0.0001\n",
      "Epoch 29, Step: 22, Loss: 0.6115959286689758, Lr:0.0001\n",
      "Epoch 29, Step: 23, Loss: 0.23496927320957184, Lr:0.0001\n",
      "Epoch 29, Step: 24, Loss: 0.39534494280815125, Lr:0.0001\n",
      "Epoch 29, Step: 25, Loss: 0.6562100052833557, Lr:0.0001\n",
      "Epoch 29, Step: 26, Loss: 0.2869446277618408, Lr:0.0001\n",
      "Epoch 29, Step: 27, Loss: 0.41378355026245117, Lr:0.0001\n",
      "Epoch 29, Step: 28, Loss: 1.0589579343795776, Lr:0.0001\n",
      "Epoch 29, Step: 29, Loss: 1.3392863273620605, Lr:0.0001\n",
      "Epoch 29, Step: 30, Loss: 0.33696112036705017, Lr:0.0001\n",
      "Epoch 29, Step: 31, Loss: 1.1243606805801392, Lr:0.0001\n",
      "Epoch 29, Step: 32, Loss: 0.790160596370697, Lr:0.0001\n",
      "Epoch 29, Step: 33, Loss: 0.39627155661582947, Lr:0.0001\n",
      "Epoch 29, Step: 34, Loss: 2.3132164478302, Lr:0.0001\n",
      "Epoch 29, Step: 35, Loss: 0.3203538656234741, Lr:0.0001\n",
      "Epoch 29, Step: 36, Loss: 0.6963497400283813, Lr:0.0001\n",
      "Epoch 29, Step: 37, Loss: 0.4172780215740204, Lr:0.0001\n",
      "Epoch 29, Step: 38, Loss: 0.6836612820625305, Lr:0.0001\n",
      "Epoch 29, Step: 39, Loss: 0.40543654561042786, Lr:0.0001\n",
      "Epoch 29, Step: 40, Loss: 0.888710618019104, Lr:0.0001\n",
      "Epoch 29, Step: 41, Loss: 0.23796352744102478, Lr:0.0001\n",
      "Epoch 29, Step: 42, Loss: 0.6597803235054016, Lr:0.0001\n",
      "Epoch 29, Step: 43, Loss: 1.2772564888000488, Lr:0.0001\n",
      "Epoch 29, Step: 44, Loss: 0.45480582118034363, Lr:0.0001\n",
      "Epoch 29, Step: 45, Loss: 0.28995949029922485, Lr:0.0001\n",
      "Epoch 29, Step: 46, Loss: 0.39389970898628235, Lr:0.0001\n",
      "Epoch 29, Step: 47, Loss: 0.8985710144042969, Lr:0.0001\n",
      "Epoch 29, Step: 48, Loss: 0.1146039143204689, Lr:0.0001\n",
      "Epoch 29, Step: 49, Loss: 1.0375375747680664, Lr:0.0001\n",
      "Epoch 29, Step: 50, Loss: 0.556204617023468, Lr:0.0001\n",
      "Epoch 29, Step: 51, Loss: 0.4856197237968445, Lr:0.0001\n",
      "Epoch 29, Step: 52, Loss: 1.5535348653793335, Lr:0.0001\n",
      "Epoch 29, Step: 53, Loss: 0.16192317008972168, Lr:0.0001\n",
      "Epoch 29, Step: 54, Loss: 1.0813692808151245, Lr:0.0001\n",
      "Epoch 29, Step: 55, Loss: 0.13703228533267975, Lr:0.0001\n",
      "Epoch 29, Step: 56, Loss: 1.7306666374206543, Lr:0.0001\n",
      "Epoch 29, Step: 57, Loss: 1.2718851566314697, Lr:0.0001\n",
      "Epoch 29, Step: 58, Loss: 0.8556768894195557, Lr:0.0001\n",
      "Epoch 29, Step: 59, Loss: 0.9095265865325928, Lr:0.0001\n",
      "Epoch 29, Step: 60, Loss: 0.5235316157341003, Lr:0.0001\n",
      "Epoch 29, Step: 61, Loss: 0.09587246179580688, Lr:0.0001\n",
      "Epoch 29, Step: 62, Loss: 0.40522247552871704, Lr:0.0001\n",
      "Epoch 29, Step: 63, Loss: 1.420008659362793, Lr:0.0001\n",
      "Epoch 29, Step: 64, Loss: 1.1346793174743652, Lr:0.0001\n",
      "Epoch 29, Step: 65, Loss: 0.6174610257148743, Lr:0.0001\n",
      "Epoch 29, Step: 66, Loss: 0.3987470865249634, Lr:0.0001\n",
      "Epoch 29, Step: 67, Loss: 0.9241213798522949, Lr:0.0001\n",
      "Epoch 29, Step: 68, Loss: 1.0439057350158691, Lr:0.0001\n",
      "Epoch 29, Step: 69, Loss: 0.2900915741920471, Lr:0.0001\n",
      "Epoch 29, Step: 70, Loss: 0.42249900102615356, Lr:0.0001\n",
      "Epoch 29, Step: 71, Loss: 0.538272500038147, Lr:0.0001\n",
      "Epoch 29, Step: 72, Loss: 0.7745267748832703, Lr:0.0001\n",
      "Epoch 29, Step: 73, Loss: 0.6002324819564819, Lr:0.0001\n",
      "Epoch 29, Step: 74, Loss: 0.37668076157569885, Lr:0.0001\n",
      "Epoch 29, Step: 75, Loss: 0.5162704586982727, Lr:0.0001\n",
      "Epoch 29, Step: 76, Loss: 0.9989879131317139, Lr:0.0001\n",
      "Epoch 29, Step: 77, Loss: 0.5717829465866089, Lr:0.0001\n",
      "Epoch 29, Step: 78, Loss: 0.12239456176757812, Lr:0.0001\n",
      "Epoch 29, Step: 79, Loss: 0.5117599368095398, Lr:0.0001\n",
      "Epoch 29, Step: 80, Loss: 1.4860478639602661, Lr:0.0001\n",
      "Epoch 29, Step: 81, Loss: 0.8423214554786682, Lr:0.0001\n",
      "Epoch 29, Step: 82, Loss: 0.1513388752937317, Lr:0.0001\n",
      "Epoch 29, Step: 83, Loss: 0.49847885966300964, Lr:0.0001\n",
      "Epoch 29, Step: 84, Loss: 3.2855639457702637, Lr:0.0001\n",
      "Epoch 29, Step: 85, Loss: 1.2439591884613037, Lr:0.0001\n",
      "Epoch 29, Step: 86, Loss: 1.7072956562042236, Lr:0.0001\n",
      "Epoch 29, Step: 87, Loss: 0.42433661222457886, Lr:0.0001\n",
      "Epoch 29, Step: 88, Loss: 0.3110707700252533, Lr:0.0001\n",
      "Epoch 29, Step: 89, Loss: 0.4426727890968323, Lr:0.0001\n",
      "Epoch 29, Step: 90, Loss: 0.048516131937503815, Lr:0.0001\n",
      "Epoch 29, Step: 91, Loss: 0.9847875833511353, Lr:0.0001\n",
      "Epoch 29, Step: 92, Loss: 1.299129605293274, Lr:0.0001\n",
      "Epoch 29, Step: 93, Loss: 0.17360307276248932, Lr:0.0001\n",
      "Epoch 29, Step: 94, Loss: 0.6125825643539429, Lr:0.0001\n",
      "Epoch 29, Step: 95, Loss: 0.6718113422393799, Lr:0.0001\n",
      "Epoch 29, Step: 96, Loss: 1.0043730735778809, Lr:0.0001\n",
      "Epoch 29, Step: 97, Loss: 0.17788918316364288, Lr:0.0001\n",
      "Epoch 29, Step: 98, Loss: 0.7916368842124939, Lr:0.0001\n",
      "Epoch 29, Step: 99, Loss: 2.2183947563171387, Lr:0.0001\n",
      "Epoch 29, Step: 100, Loss: 0.6852819919586182, Lr:0.0001\n",
      "Epoch 29, Step: 101, Loss: 0.32903411984443665, Lr:0.0001\n",
      "Epoch 29, Step: 102, Loss: 0.5819636583328247, Lr:0.0001\n",
      "Epoch 29, Step: 103, Loss: 2.1114768981933594, Lr:0.0001\n",
      "Epoch 29, Step: 104, Loss: 0.700080931186676, Lr:0.0001\n",
      "Epoch 29, Step: 105, Loss: 0.8521823883056641, Lr:0.0001\n",
      "Epoch 29, Step: 106, Loss: 0.2568608522415161, Lr:0.0001\n",
      "Epoch 29, Step: 107, Loss: 0.5724282264709473, Lr:0.0001\n",
      "Epoch 29, Step: 108, Loss: 0.5810673236846924, Lr:0.0001\n",
      "Epoch 29, Step: 109, Loss: 0.11967090517282486, Lr:0.0001\n",
      "Epoch 29, Step: 110, Loss: 0.12578760087490082, Lr:0.0001\n",
      "Epoch 29, Step: 111, Loss: 0.5654066801071167, Lr:0.0001\n",
      "Epoch 29, Step: 112, Loss: 0.862011194229126, Lr:0.0001\n",
      "Epoch 29, Step: 113, Loss: 0.4136139750480652, Lr:0.0001\n",
      "Epoch 29, Step: 114, Loss: 0.8094683289527893, Lr:0.0001\n",
      "Epoch 29, Step: 115, Loss: 1.0317156314849854, Lr:0.0001\n",
      "Epoch 29, Step: 116, Loss: 1.0117532014846802, Lr:0.0001\n",
      "Epoch 29, Step: 117, Loss: 0.46642738580703735, Lr:0.0001\n",
      "Epoch 29, Step: 118, Loss: 0.5157895088195801, Lr:0.0001\n",
      "Epoch 29, Step: 119, Loss: 0.30112746357917786, Lr:0.0001\n",
      "Epoch 29, Step: 120, Loss: 0.376071035861969, Lr:0.0001\n",
      "Epoch 29, Step: 121, Loss: 0.3585076630115509, Lr:0.0001\n",
      "Epoch 29, Step: 122, Loss: 1.5973985195159912, Lr:0.0001\n",
      "Epoch 29, Step: 123, Loss: 1.370195746421814, Lr:0.0001\n",
      "Epoch 29, Step: 124, Loss: 0.35889580845832825, Lr:0.0001\n",
      "Epoch 29, Step: 125, Loss: 0.26074695587158203, Lr:0.0001\n",
      "Epoch 29, Step: 126, Loss: 0.7781622409820557, Lr:0.0001\n",
      "Epoch 29, Step: 127, Loss: 0.6692548394203186, Lr:0.0001\n",
      "Epoch 29, Step: 128, Loss: 0.3729494512081146, Lr:0.0001\n",
      "Epoch 29, Step: 129, Loss: 0.5826596021652222, Lr:0.0001\n",
      "Epoch 29, Step: 130, Loss: 0.3891105651855469, Lr:0.0001\n",
      "Epoch 29, Step: 131, Loss: 0.2435780018568039, Lr:0.0001\n",
      "Epoch 29, Step: 132, Loss: 0.5718328952789307, Lr:0.0001\n",
      "Epoch 29, Step: 133, Loss: 0.7216851711273193, Lr:0.0001\n",
      "Epoch 29, Step: 134, Loss: 1.1403419971466064, Lr:0.0001\n",
      "Epoch 29, Step: 135, Loss: 0.6355232000350952, Lr:0.0001\n",
      "Epoch 29, Step: 136, Loss: 0.5608878135681152, Lr:0.0001\n",
      "Epoch 29, Step: 137, Loss: 1.0086995363235474, Lr:0.0001\n",
      "Epoch 29, Step: 138, Loss: 0.0695226639509201, Lr:0.0001\n",
      "Epoch 29, Step: 139, Loss: 0.24878008663654327, Lr:0.0001\n",
      "Epoch 29, Step: 140, Loss: 0.2020975649356842, Lr:0.0001\n",
      "Epoch 29, Step: 141, Loss: 0.09933409094810486, Lr:0.0001\n",
      "Epoch 29, Step: 142, Loss: 0.48265597224235535, Lr:0.0001\n",
      "Epoch 29, Step: 143, Loss: 0.9707506895065308, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 29\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0691 (0.0691) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0090 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4470 (0.9881) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0062 data: 0.0032 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0063 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 0.988\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 30, Step: 0, Loss: 0.7643052935600281, Lr:0.0001\n",
      "Epoch 30, Step: 1, Loss: 1.1054352521896362, Lr:0.0001\n",
      "Epoch 30, Step: 2, Loss: 1.5495516061782837, Lr:0.0001\n",
      "Epoch 30, Step: 3, Loss: 0.22473956644535065, Lr:0.0001\n",
      "Epoch 30, Step: 4, Loss: 1.4193373918533325, Lr:0.0001\n",
      "Epoch 30, Step: 5, Loss: 1.3481786251068115, Lr:0.0001\n",
      "Epoch 30, Step: 6, Loss: 0.21136219799518585, Lr:0.0001\n",
      "Epoch 30, Step: 7, Loss: 0.28552305698394775, Lr:0.0001\n",
      "Epoch 30, Step: 8, Loss: 1.2562892436981201, Lr:0.0001\n",
      "Epoch 30, Step: 9, Loss: 0.3588356673717499, Lr:0.0001\n",
      "Epoch 30, Step: 10, Loss: 0.7419413924217224, Lr:0.0001\n",
      "Epoch 30, Step: 11, Loss: 0.8403958082199097, Lr:0.0001\n",
      "Epoch 30, Step: 12, Loss: 1.1383068561553955, Lr:0.0001\n",
      "Epoch 30, Step: 13, Loss: 0.8742823004722595, Lr:0.0001\n",
      "Epoch 30, Step: 14, Loss: 1.1212570667266846, Lr:0.0001\n",
      "Epoch 30, Step: 15, Loss: 0.08923811465501785, Lr:0.0001\n",
      "Epoch 30, Step: 16, Loss: 1.0303033590316772, Lr:0.0001\n",
      "Epoch 30, Step: 17, Loss: 0.49007362127304077, Lr:0.0001\n",
      "Epoch 30, Step: 18, Loss: 0.405467689037323, Lr:0.0001\n",
      "Epoch 30, Step: 19, Loss: 0.19024381041526794, Lr:0.0001\n",
      "Epoch 30, Step: 20, Loss: 0.1375676840543747, Lr:0.0001\n",
      "Epoch 30, Step: 21, Loss: 0.5543146133422852, Lr:0.0001\n",
      "Epoch 30, Step: 22, Loss: 0.5413916110992432, Lr:0.0001\n",
      "Epoch 30, Step: 23, Loss: 0.8311176896095276, Lr:0.0001\n",
      "Epoch 30, Step: 24, Loss: 0.5936631560325623, Lr:0.0001\n",
      "Epoch 30, Step: 25, Loss: 0.13566654920578003, Lr:0.0001\n",
      "Epoch 30, Step: 26, Loss: 0.26829463243484497, Lr:0.0001\n",
      "Epoch 30, Step: 27, Loss: 0.4135017991065979, Lr:0.0001\n",
      "Epoch 30, Step: 28, Loss: 0.49596744775772095, Lr:0.0001\n",
      "Epoch 30, Step: 29, Loss: 0.19514355063438416, Lr:0.0001\n",
      "Epoch 30, Step: 30, Loss: 0.23074647784233093, Lr:0.0001\n",
      "Epoch 30, Step: 31, Loss: 0.18712544441223145, Lr:0.0001\n",
      "Epoch 30, Step: 32, Loss: 0.485263466835022, Lr:0.0001\n",
      "Epoch 30, Step: 33, Loss: 0.21071457862854004, Lr:0.0001\n",
      "Epoch 30, Step: 34, Loss: 0.27727019786834717, Lr:0.0001\n",
      "Epoch 30, Step: 35, Loss: 1.2078896760940552, Lr:0.0001\n",
      "Epoch 30, Step: 36, Loss: 0.11789944767951965, Lr:0.0001\n",
      "Epoch 30, Step: 37, Loss: 0.08547784388065338, Lr:0.0001\n",
      "Epoch 30, Step: 38, Loss: 0.8280270099639893, Lr:0.0001\n",
      "Epoch 30, Step: 39, Loss: 0.759260892868042, Lr:0.0001\n",
      "Epoch 30, Step: 40, Loss: 0.156007319688797, Lr:0.0001\n",
      "Epoch 30, Step: 41, Loss: 0.09700608998537064, Lr:0.0001\n",
      "Epoch 30, Step: 42, Loss: 0.6657198071479797, Lr:0.0001\n",
      "Epoch 30, Step: 43, Loss: 1.1081807613372803, Lr:0.0001\n",
      "Epoch 30, Step: 44, Loss: 1.195837378501892, Lr:0.0001\n",
      "Epoch 30, Step: 45, Loss: 0.9612557888031006, Lr:0.0001\n",
      "Epoch 30, Step: 46, Loss: 0.23370225727558136, Lr:0.0001\n",
      "Epoch 30, Step: 47, Loss: 0.44030287861824036, Lr:0.0001\n",
      "Epoch 30, Step: 48, Loss: 0.6399376392364502, Lr:0.0001\n",
      "Epoch 30, Step: 49, Loss: 0.4625561833381653, Lr:0.0001\n",
      "Epoch 30, Step: 50, Loss: 1.2076774835586548, Lr:0.0001\n",
      "Epoch 30, Step: 51, Loss: 1.783593773841858, Lr:0.0001\n",
      "Epoch 30, Step: 52, Loss: 0.09971112012863159, Lr:0.0001\n",
      "Epoch 30, Step: 53, Loss: 0.425802081823349, Lr:0.0001\n",
      "Epoch 30, Step: 54, Loss: 1.153140902519226, Lr:0.0001\n",
      "Epoch 30, Step: 55, Loss: 2.0417890548706055, Lr:0.0001\n",
      "Epoch 30, Step: 56, Loss: 0.988751232624054, Lr:0.0001\n",
      "Epoch 30, Step: 57, Loss: 0.4287276864051819, Lr:0.0001\n",
      "Epoch 30, Step: 58, Loss: 0.4954242706298828, Lr:0.0001\n",
      "Epoch 30, Step: 59, Loss: 0.6201678514480591, Lr:0.0001\n",
      "Epoch 30, Step: 60, Loss: 0.4836799204349518, Lr:0.0001\n",
      "Epoch 30, Step: 61, Loss: 0.6170039176940918, Lr:0.0001\n",
      "Epoch 30, Step: 62, Loss: 1.6698311567306519, Lr:0.0001\n",
      "Epoch 30, Step: 63, Loss: 0.8264297246932983, Lr:0.0001\n",
      "Epoch 30, Step: 64, Loss: 0.2960992455482483, Lr:0.0001\n",
      "Epoch 30, Step: 65, Loss: 1.4951136112213135, Lr:0.0001\n",
      "Epoch 30, Step: 66, Loss: 1.0520646572113037, Lr:0.0001\n",
      "Epoch 30, Step: 67, Loss: 0.8077830076217651, Lr:0.0001\n",
      "Epoch 30, Step: 68, Loss: 0.37167561054229736, Lr:0.0001\n",
      "Epoch 30, Step: 69, Loss: 0.2331278920173645, Lr:0.0001\n",
      "Epoch 30, Step: 70, Loss: 0.5150004625320435, Lr:0.0001\n",
      "Epoch 30, Step: 71, Loss: 0.07263563573360443, Lr:0.0001\n",
      "Epoch 30, Step: 72, Loss: 0.7429729700088501, Lr:0.0001\n",
      "Epoch 30, Step: 73, Loss: 0.4754406809806824, Lr:0.0001\n",
      "Epoch 30, Step: 74, Loss: 1.2312678098678589, Lr:0.0001\n",
      "Epoch 30, Step: 75, Loss: 0.5881121158599854, Lr:0.0001\n",
      "Epoch 30, Step: 76, Loss: 0.623716413974762, Lr:0.0001\n",
      "Epoch 30, Step: 77, Loss: 0.3262779712677002, Lr:0.0001\n",
      "Epoch 30, Step: 78, Loss: 1.3292268514633179, Lr:0.0001\n",
      "Epoch 30, Step: 79, Loss: 0.7561107873916626, Lr:0.0001\n",
      "Epoch 30, Step: 80, Loss: 0.4158780574798584, Lr:0.0001\n",
      "Epoch 30, Step: 81, Loss: 0.9065732359886169, Lr:0.0001\n",
      "Epoch 30, Step: 82, Loss: 0.46780890226364136, Lr:0.0001\n",
      "Epoch 30, Step: 83, Loss: 0.2118513435125351, Lr:0.0001\n",
      "Epoch 30, Step: 84, Loss: 0.5368534326553345, Lr:0.0001\n",
      "Epoch 30, Step: 85, Loss: 0.713826060295105, Lr:0.0001\n",
      "Epoch 30, Step: 86, Loss: 1.547105073928833, Lr:0.0001\n",
      "Epoch 30, Step: 87, Loss: 1.2706513404846191, Lr:0.0001\n",
      "Epoch 30, Step: 88, Loss: 0.46434372663497925, Lr:0.0001\n",
      "Epoch 30, Step: 89, Loss: 0.5604106783866882, Lr:0.0001\n",
      "Epoch 30, Step: 90, Loss: 1.6540476083755493, Lr:0.0001\n",
      "Epoch 30, Step: 91, Loss: 0.5365913510322571, Lr:0.0001\n",
      "Epoch 30, Step: 92, Loss: 0.49571430683135986, Lr:0.0001\n",
      "Epoch 30, Step: 93, Loss: 0.8426340818405151, Lr:0.0001\n",
      "Epoch 30, Step: 94, Loss: 0.42580530047416687, Lr:0.0001\n",
      "Epoch 30, Step: 95, Loss: 1.0224748849868774, Lr:0.0001\n",
      "Epoch 30, Step: 96, Loss: 0.7714245319366455, Lr:0.0001\n",
      "Epoch 30, Step: 97, Loss: 0.26450854539871216, Lr:0.0001\n",
      "Epoch 30, Step: 98, Loss: 0.3974566161632538, Lr:0.0001\n",
      "Epoch 30, Step: 99, Loss: 2.3946614265441895, Lr:0.0001\n",
      "Epoch 30, Step: 100, Loss: 0.5287655591964722, Lr:0.0001\n",
      "Epoch 30, Step: 101, Loss: 0.9507144689559937, Lr:0.0001\n",
      "Epoch 30, Step: 102, Loss: 1.1446830034255981, Lr:0.0001\n",
      "Epoch 30, Step: 103, Loss: 0.499437153339386, Lr:0.0001\n",
      "Epoch 30, Step: 104, Loss: 0.5561214685440063, Lr:0.0001\n",
      "Epoch 30, Step: 105, Loss: 0.5041569471359253, Lr:0.0001\n",
      "Epoch 30, Step: 106, Loss: 0.4326336979866028, Lr:0.0001\n",
      "Epoch 30, Step: 107, Loss: 0.7699123024940491, Lr:0.0001\n",
      "Epoch 30, Step: 108, Loss: 0.5496910214424133, Lr:0.0001\n",
      "Epoch 30, Step: 109, Loss: 0.36590149998664856, Lr:0.0001\n",
      "Epoch 30, Step: 110, Loss: 0.2557533383369446, Lr:0.0001\n",
      "Epoch 30, Step: 111, Loss: 0.6617095470428467, Lr:0.0001\n",
      "Epoch 30, Step: 112, Loss: 0.9195079803466797, Lr:0.0001\n",
      "Epoch 30, Step: 113, Loss: 0.16346298158168793, Lr:0.0001\n",
      "Epoch 30, Step: 114, Loss: 0.26180514693260193, Lr:0.0001\n",
      "Epoch 30, Step: 115, Loss: 1.1035668849945068, Lr:0.0001\n",
      "Epoch 30, Step: 116, Loss: 1.1205313205718994, Lr:0.0001\n",
      "Epoch 30, Step: 117, Loss: 0.29029521346092224, Lr:0.0001\n",
      "Epoch 30, Step: 118, Loss: 0.375262588262558, Lr:0.0001\n",
      "Epoch 30, Step: 119, Loss: 0.26209062337875366, Lr:0.0001\n",
      "Epoch 30, Step: 120, Loss: 0.15745007991790771, Lr:0.0001\n",
      "Epoch 30, Step: 121, Loss: 1.076370120048523, Lr:0.0001\n",
      "Epoch 30, Step: 122, Loss: 0.5580685138702393, Lr:0.0001\n",
      "Epoch 30, Step: 123, Loss: 0.8113845586776733, Lr:0.0001\n",
      "Epoch 30, Step: 124, Loss: 0.36355558037757874, Lr:0.0001\n",
      "Epoch 30, Step: 125, Loss: 1.0677707195281982, Lr:0.0001\n",
      "Epoch 30, Step: 126, Loss: 0.9626893997192383, Lr:0.0001\n",
      "Epoch 30, Step: 127, Loss: 1.4727168083190918, Lr:0.0001\n",
      "Epoch 30, Step: 128, Loss: 0.613493025302887, Lr:0.0001\n",
      "Epoch 30, Step: 129, Loss: 0.45584288239479065, Lr:0.0001\n",
      "Epoch 30, Step: 130, Loss: 0.25965243577957153, Lr:0.0001\n",
      "Epoch 30, Step: 131, Loss: 0.6218206882476807, Lr:0.0001\n",
      "Epoch 30, Step: 132, Loss: 0.1723262518644333, Lr:0.0001\n",
      "Epoch 30, Step: 133, Loss: 0.7063769102096558, Lr:0.0001\n",
      "Epoch 30, Step: 134, Loss: 0.26591312885284424, Lr:0.0001\n",
      "Epoch 30, Step: 135, Loss: 0.2354707568883896, Lr:0.0001\n",
      "Epoch 30, Step: 136, Loss: 0.04693257436156273, Lr:0.0001\n",
      "Epoch 30, Step: 137, Loss: 1.0671782493591309, Lr:0.0001\n",
      "Epoch 30, Step: 138, Loss: 0.9530979990959167, Lr:0.0001\n",
      "Epoch 30, Step: 139, Loss: 0.8653528690338135, Lr:0.0001\n",
      "Epoch 30, Step: 140, Loss: 1.8656072616577148, Lr:0.0001\n",
      "Epoch 30, Step: 141, Loss: 0.49778425693511963, Lr:0.0001\n",
      "Epoch 30, Step: 142, Loss: 1.346726655960083, Lr:0.0001\n",
      "Epoch 30, Step: 143, Loss: 0.8403455018997192, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 30\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0086 (0.0086) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0120 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7050 (0.7302) acc1: 75.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0093 data: 0.0039 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0093 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.730\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 31, Step: 0, Loss: 1.2925444841384888, Lr:0.0001\n",
      "Epoch 31, Step: 1, Loss: 0.3669271767139435, Lr:0.0001\n",
      "Epoch 31, Step: 2, Loss: 0.7103284597396851, Lr:0.0001\n",
      "Epoch 31, Step: 3, Loss: 0.6265371441841125, Lr:0.0001\n",
      "Epoch 31, Step: 4, Loss: 0.7565933465957642, Lr:0.0001\n",
      "Epoch 31, Step: 5, Loss: 2.183757781982422, Lr:0.0001\n",
      "Epoch 31, Step: 6, Loss: 0.4083976745605469, Lr:0.0001\n",
      "Epoch 31, Step: 7, Loss: 0.5122030973434448, Lr:0.0001\n",
      "Epoch 31, Step: 8, Loss: 1.6943610906600952, Lr:0.0001\n",
      "Epoch 31, Step: 9, Loss: 0.28938165307044983, Lr:0.0001\n",
      "Epoch 31, Step: 10, Loss: 0.9146471619606018, Lr:0.0001\n",
      "Epoch 31, Step: 11, Loss: 1.0562549829483032, Lr:0.0001\n",
      "Epoch 31, Step: 12, Loss: 0.3404659926891327, Lr:0.0001\n",
      "Epoch 31, Step: 13, Loss: 0.5670614242553711, Lr:0.0001\n",
      "Epoch 31, Step: 14, Loss: 0.41431745886802673, Lr:0.0001\n",
      "Epoch 31, Step: 15, Loss: 0.6062350869178772, Lr:0.0001\n",
      "Epoch 31, Step: 16, Loss: 0.8003901839256287, Lr:0.0001\n",
      "Epoch 31, Step: 17, Loss: 0.4274890124797821, Lr:0.0001\n",
      "Epoch 31, Step: 18, Loss: 0.8687905073165894, Lr:0.0001\n",
      "Epoch 31, Step: 19, Loss: 0.3812304437160492, Lr:0.0001\n",
      "Epoch 31, Step: 20, Loss: 1.1302279233932495, Lr:0.0001\n",
      "Epoch 31, Step: 21, Loss: 0.10384195297956467, Lr:0.0001\n",
      "Epoch 31, Step: 22, Loss: 0.11560611426830292, Lr:0.0001\n",
      "Epoch 31, Step: 23, Loss: 0.5654992461204529, Lr:0.0001\n",
      "Epoch 31, Step: 24, Loss: 0.6368123292922974, Lr:0.0001\n",
      "Epoch 31, Step: 25, Loss: 0.8552417159080505, Lr:0.0001\n",
      "Epoch 31, Step: 26, Loss: 0.7259024381637573, Lr:0.0001\n",
      "Epoch 31, Step: 27, Loss: 1.6128801107406616, Lr:0.0001\n",
      "Epoch 31, Step: 28, Loss: 1.1241986751556396, Lr:0.0001\n",
      "Epoch 31, Step: 29, Loss: 0.2759798467159271, Lr:0.0001\n",
      "Epoch 31, Step: 30, Loss: 0.25416114926338196, Lr:0.0001\n",
      "Epoch 31, Step: 31, Loss: 0.28679358959198, Lr:0.0001\n",
      "Epoch 31, Step: 32, Loss: 0.5350764989852905, Lr:0.0001\n",
      "Epoch 31, Step: 33, Loss: 0.44105976819992065, Lr:0.0001\n",
      "Epoch 31, Step: 34, Loss: 0.5385814309120178, Lr:0.0001\n",
      "Epoch 31, Step: 35, Loss: 1.410420298576355, Lr:0.0001\n",
      "Epoch 31, Step: 36, Loss: 0.5052564144134521, Lr:0.0001\n",
      "Epoch 31, Step: 37, Loss: 0.9949067831039429, Lr:0.0001\n",
      "Epoch 31, Step: 38, Loss: 1.3918375968933105, Lr:0.0001\n",
      "Epoch 31, Step: 39, Loss: 0.5053900480270386, Lr:0.0001\n",
      "Epoch 31, Step: 40, Loss: 0.5163211822509766, Lr:0.0001\n",
      "Epoch 31, Step: 41, Loss: 0.38048437237739563, Lr:0.0001\n",
      "Epoch 31, Step: 42, Loss: 1.2059216499328613, Lr:0.0001\n",
      "Epoch 31, Step: 43, Loss: 0.5786833763122559, Lr:0.0001\n",
      "Epoch 31, Step: 44, Loss: 0.8064729571342468, Lr:0.0001\n",
      "Epoch 31, Step: 45, Loss: 0.7133126854896545, Lr:0.0001\n",
      "Epoch 31, Step: 46, Loss: 0.5408157110214233, Lr:0.0001\n",
      "Epoch 31, Step: 47, Loss: 1.1174137592315674, Lr:0.0001\n",
      "Epoch 31, Step: 48, Loss: 0.8297760486602783, Lr:0.0001\n",
      "Epoch 31, Step: 49, Loss: 1.3386424779891968, Lr:0.0001\n",
      "Epoch 31, Step: 50, Loss: 0.7827237844467163, Lr:0.0001\n",
      "Epoch 31, Step: 51, Loss: 0.18484526872634888, Lr:0.0001\n",
      "Epoch 31, Step: 52, Loss: 0.9890302419662476, Lr:0.0001\n",
      "Epoch 31, Step: 53, Loss: 0.24572841823101044, Lr:0.0001\n",
      "Epoch 31, Step: 54, Loss: 0.4474684000015259, Lr:0.0001\n",
      "Epoch 31, Step: 55, Loss: 0.38334575295448303, Lr:0.0001\n",
      "Epoch 31, Step: 56, Loss: 0.19652223587036133, Lr:0.0001\n",
      "Epoch 31, Step: 57, Loss: 0.3818579912185669, Lr:0.0001\n",
      "Epoch 31, Step: 58, Loss: 0.7618515491485596, Lr:0.0001\n",
      "Epoch 31, Step: 59, Loss: 0.7693538665771484, Lr:0.0001\n",
      "Epoch 31, Step: 60, Loss: 0.49166738986968994, Lr:0.0001\n",
      "Epoch 31, Step: 61, Loss: 0.571929931640625, Lr:0.0001\n",
      "Epoch 31, Step: 62, Loss: 0.4420543313026428, Lr:0.0001\n",
      "Epoch 31, Step: 63, Loss: 0.125443235039711, Lr:0.0001\n",
      "Epoch 31, Step: 64, Loss: 0.38594597578048706, Lr:0.0001\n",
      "Epoch 31, Step: 65, Loss: 0.645131528377533, Lr:0.0001\n",
      "Epoch 31, Step: 66, Loss: 0.4111451208591461, Lr:0.0001\n",
      "Epoch 31, Step: 67, Loss: 2.212468385696411, Lr:0.0001\n",
      "Epoch 31, Step: 68, Loss: 0.8474348783493042, Lr:0.0001\n",
      "Epoch 31, Step: 69, Loss: 0.9090427756309509, Lr:0.0001\n",
      "Epoch 31, Step: 70, Loss: 0.4866024851799011, Lr:0.0001\n",
      "Epoch 31, Step: 71, Loss: 0.9448482394218445, Lr:0.0001\n",
      "Epoch 31, Step: 72, Loss: 1.3112080097198486, Lr:0.0001\n",
      "Epoch 31, Step: 73, Loss: 0.19480520486831665, Lr:0.0001\n",
      "Epoch 31, Step: 74, Loss: 0.5551618337631226, Lr:0.0001\n",
      "Epoch 31, Step: 75, Loss: 0.6240449547767639, Lr:0.0001\n",
      "Epoch 31, Step: 76, Loss: 0.28174495697021484, Lr:0.0001\n",
      "Epoch 31, Step: 77, Loss: 0.5961702466011047, Lr:0.0001\n",
      "Epoch 31, Step: 78, Loss: 0.34171900153160095, Lr:0.0001\n",
      "Epoch 31, Step: 79, Loss: 0.9360252618789673, Lr:0.0001\n",
      "Epoch 31, Step: 80, Loss: 1.0729012489318848, Lr:0.0001\n",
      "Epoch 31, Step: 81, Loss: 0.09670212119817734, Lr:0.0001\n",
      "Epoch 31, Step: 82, Loss: 0.3996688425540924, Lr:0.0001\n",
      "Epoch 31, Step: 83, Loss: 0.5723601579666138, Lr:0.0001\n",
      "Epoch 31, Step: 84, Loss: 1.1806268692016602, Lr:0.0001\n",
      "Epoch 31, Step: 85, Loss: 1.2402993440628052, Lr:0.0001\n",
      "Epoch 31, Step: 86, Loss: 0.2641298770904541, Lr:0.0001\n",
      "Epoch 31, Step: 87, Loss: 0.4919147789478302, Lr:0.0001\n",
      "Epoch 31, Step: 88, Loss: 0.7875300645828247, Lr:0.0001\n",
      "Epoch 31, Step: 89, Loss: 0.880892276763916, Lr:0.0001\n",
      "Epoch 31, Step: 90, Loss: 0.12454742193222046, Lr:0.0001\n",
      "Epoch 31, Step: 91, Loss: 0.9512032866477966, Lr:0.0001\n",
      "Epoch 31, Step: 92, Loss: 0.1646035611629486, Lr:0.0001\n",
      "Epoch 31, Step: 93, Loss: 1.317871332168579, Lr:0.0001\n",
      "Epoch 31, Step: 94, Loss: 0.30827590823173523, Lr:0.0001\n",
      "Epoch 31, Step: 95, Loss: 0.6940440535545349, Lr:0.0001\n",
      "Epoch 31, Step: 96, Loss: 0.6197383999824524, Lr:0.0001\n",
      "Epoch 31, Step: 97, Loss: 0.15206143260002136, Lr:0.0001\n",
      "Epoch 31, Step: 98, Loss: 0.5032817721366882, Lr:0.0001\n",
      "Epoch 31, Step: 99, Loss: 1.0809345245361328, Lr:0.0001\n",
      "Epoch 31, Step: 100, Loss: 0.8897486925125122, Lr:0.0001\n",
      "Epoch 31, Step: 101, Loss: 0.9697125554084778, Lr:0.0001\n",
      "Epoch 31, Step: 102, Loss: 0.8396918177604675, Lr:0.0001\n",
      "Epoch 31, Step: 103, Loss: 0.7880463004112244, Lr:0.0001\n",
      "Epoch 31, Step: 104, Loss: 0.3692033290863037, Lr:0.0001\n",
      "Epoch 31, Step: 105, Loss: 0.3076370656490326, Lr:0.0001\n",
      "Epoch 31, Step: 106, Loss: 0.8166337013244629, Lr:0.0001\n",
      "Epoch 31, Step: 107, Loss: 0.8596718907356262, Lr:0.0001\n",
      "Epoch 31, Step: 108, Loss: 0.29354143142700195, Lr:0.0001\n",
      "Epoch 31, Step: 109, Loss: 0.8733695149421692, Lr:0.0001\n",
      "Epoch 31, Step: 110, Loss: 0.49098068475723267, Lr:0.0001\n",
      "Epoch 31, Step: 111, Loss: 0.8677549362182617, Lr:0.0001\n",
      "Epoch 31, Step: 112, Loss: 0.4177440106868744, Lr:0.0001\n",
      "Epoch 31, Step: 113, Loss: 0.25760409235954285, Lr:0.0001\n",
      "Epoch 31, Step: 114, Loss: 1.6450083255767822, Lr:0.0001\n",
      "Epoch 31, Step: 115, Loss: 0.8312623500823975, Lr:0.0001\n",
      "Epoch 31, Step: 116, Loss: 0.9540096521377563, Lr:0.0001\n",
      "Epoch 31, Step: 117, Loss: 0.5108545422554016, Lr:0.0001\n",
      "Epoch 31, Step: 118, Loss: 0.8323964476585388, Lr:0.0001\n",
      "Epoch 31, Step: 119, Loss: 0.1841564029455185, Lr:0.0001\n",
      "Epoch 31, Step: 120, Loss: 0.40716058015823364, Lr:0.0001\n",
      "Epoch 31, Step: 121, Loss: 0.22355058789253235, Lr:0.0001\n",
      "Epoch 31, Step: 122, Loss: 0.4400967061519623, Lr:0.0001\n",
      "Epoch 31, Step: 123, Loss: 0.7174827456474304, Lr:0.0001\n",
      "Epoch 31, Step: 124, Loss: 0.4352913498878479, Lr:0.0001\n",
      "Epoch 31, Step: 125, Loss: 0.6868358850479126, Lr:0.0001\n",
      "Epoch 31, Step: 126, Loss: 0.7510331869125366, Lr:0.0001\n",
      "Epoch 31, Step: 127, Loss: 2.2082929611206055, Lr:0.0001\n",
      "Epoch 31, Step: 128, Loss: 0.13236288726329803, Lr:0.0001\n",
      "Epoch 31, Step: 129, Loss: 0.23180724680423737, Lr:0.0001\n",
      "Epoch 31, Step: 130, Loss: 1.0667580366134644, Lr:0.0001\n",
      "Epoch 31, Step: 131, Loss: 0.45901140570640564, Lr:0.0001\n",
      "Epoch 31, Step: 132, Loss: 0.9914921522140503, Lr:0.0001\n",
      "Epoch 31, Step: 133, Loss: 0.2503049075603485, Lr:0.0001\n",
      "Epoch 31, Step: 134, Loss: 1.5992847681045532, Lr:0.0001\n",
      "Epoch 31, Step: 135, Loss: 0.8323429226875305, Lr:0.0001\n",
      "Epoch 31, Step: 136, Loss: 1.503340244293213, Lr:0.0001\n",
      "Epoch 31, Step: 137, Loss: 0.26792511343955994, Lr:0.0001\n",
      "Epoch 31, Step: 138, Loss: 0.3996407985687256, Lr:0.0001\n",
      "Epoch 31, Step: 139, Loss: 1.463610053062439, Lr:0.0001\n",
      "Epoch 31, Step: 140, Loss: 2.062647581100464, Lr:0.0001\n",
      "Epoch 31, Step: 141, Loss: 0.34301501512527466, Lr:0.0001\n",
      "Epoch 31, Step: 142, Loss: 1.4763792753219604, Lr:0.0001\n",
      "Epoch 31, Step: 143, Loss: 1.1814552545547485, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 31\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0504 (0.0504) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0110 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4522 (0.7365) acc1: 100.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0077 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0078 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.736\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 32, Step: 0, Loss: 0.5892335772514343, Lr:0.0001\n",
      "Epoch 32, Step: 1, Loss: 0.33688804507255554, Lr:0.0001\n",
      "Epoch 32, Step: 2, Loss: 1.010761022567749, Lr:0.0001\n",
      "Epoch 32, Step: 3, Loss: 0.9431701898574829, Lr:0.0001\n",
      "Epoch 32, Step: 4, Loss: 0.47318440675735474, Lr:0.0001\n",
      "Epoch 32, Step: 5, Loss: 0.7938397526741028, Lr:0.0001\n",
      "Epoch 32, Step: 6, Loss: 1.2705785036087036, Lr:0.0001\n",
      "Epoch 32, Step: 7, Loss: 0.8203369379043579, Lr:0.0001\n",
      "Epoch 32, Step: 8, Loss: 0.29775476455688477, Lr:0.0001\n",
      "Epoch 32, Step: 9, Loss: 0.33245858550071716, Lr:0.0001\n",
      "Epoch 32, Step: 10, Loss: 0.5941544771194458, Lr:0.0001\n",
      "Epoch 32, Step: 11, Loss: 0.6329879760742188, Lr:0.0001\n",
      "Epoch 32, Step: 12, Loss: 0.6084427237510681, Lr:0.0001\n",
      "Epoch 32, Step: 13, Loss: 0.871578574180603, Lr:0.0001\n",
      "Epoch 32, Step: 14, Loss: 0.4842807352542877, Lr:0.0001\n",
      "Epoch 32, Step: 15, Loss: 0.6859909892082214, Lr:0.0001\n",
      "Epoch 32, Step: 16, Loss: 0.13367491960525513, Lr:0.0001\n",
      "Epoch 32, Step: 17, Loss: 0.359841525554657, Lr:0.0001\n",
      "Epoch 32, Step: 18, Loss: 0.9983218312263489, Lr:0.0001\n",
      "Epoch 32, Step: 19, Loss: 1.4181979894638062, Lr:0.0001\n",
      "Epoch 32, Step: 20, Loss: 0.3913293182849884, Lr:0.0001\n",
      "Epoch 32, Step: 21, Loss: 0.946010947227478, Lr:0.0001\n",
      "Epoch 32, Step: 22, Loss: 0.26790231466293335, Lr:0.0001\n",
      "Epoch 32, Step: 23, Loss: 0.4525962173938751, Lr:0.0001\n",
      "Epoch 32, Step: 24, Loss: 0.5534877777099609, Lr:0.0001\n",
      "Epoch 32, Step: 25, Loss: 0.24825051426887512, Lr:0.0001\n",
      "Epoch 32, Step: 26, Loss: 1.193361520767212, Lr:0.0001\n",
      "Epoch 32, Step: 27, Loss: 0.26049450039863586, Lr:0.0001\n",
      "Epoch 32, Step: 28, Loss: 0.16259954869747162, Lr:0.0001\n",
      "Epoch 32, Step: 29, Loss: 0.792620062828064, Lr:0.0001\n",
      "Epoch 32, Step: 30, Loss: 1.094214677810669, Lr:0.0001\n",
      "Epoch 32, Step: 31, Loss: 0.3004452884197235, Lr:0.0001\n",
      "Epoch 32, Step: 32, Loss: 0.08889556676149368, Lr:0.0001\n",
      "Epoch 32, Step: 33, Loss: 0.6598621010780334, Lr:0.0001\n",
      "Epoch 32, Step: 34, Loss: 0.7830170392990112, Lr:0.0001\n",
      "Epoch 32, Step: 35, Loss: 0.7572579979896545, Lr:0.0001\n",
      "Epoch 32, Step: 36, Loss: 0.5957232713699341, Lr:0.0001\n",
      "Epoch 32, Step: 37, Loss: 0.7409026622772217, Lr:0.0001\n",
      "Epoch 32, Step: 38, Loss: 0.28714919090270996, Lr:0.0001\n",
      "Epoch 32, Step: 39, Loss: 1.21356201171875, Lr:0.0001\n",
      "Epoch 32, Step: 40, Loss: 1.4401243925094604, Lr:0.0001\n",
      "Epoch 32, Step: 41, Loss: 0.5257333517074585, Lr:0.0001\n",
      "Epoch 32, Step: 42, Loss: 1.1475019454956055, Lr:0.0001\n",
      "Epoch 32, Step: 43, Loss: 0.33624064922332764, Lr:0.0001\n",
      "Epoch 32, Step: 44, Loss: 1.5688090324401855, Lr:0.0001\n",
      "Epoch 32, Step: 45, Loss: 1.5556424856185913, Lr:0.0001\n",
      "Epoch 32, Step: 46, Loss: 0.5408979058265686, Lr:0.0001\n",
      "Epoch 32, Step: 47, Loss: 0.7378146648406982, Lr:0.0001\n",
      "Epoch 32, Step: 48, Loss: 1.0810014009475708, Lr:0.0001\n",
      "Epoch 32, Step: 49, Loss: 0.5649354457855225, Lr:0.0001\n",
      "Epoch 32, Step: 50, Loss: 0.06754843890666962, Lr:0.0001\n",
      "Epoch 32, Step: 51, Loss: 0.894945502281189, Lr:0.0001\n",
      "Epoch 32, Step: 52, Loss: 0.6422291994094849, Lr:0.0001\n",
      "Epoch 32, Step: 53, Loss: 1.4943549633026123, Lr:0.0001\n",
      "Epoch 32, Step: 54, Loss: 1.0270938873291016, Lr:0.0001\n",
      "Epoch 32, Step: 55, Loss: 1.0269776582717896, Lr:0.0001\n",
      "Epoch 32, Step: 56, Loss: 0.6435116529464722, Lr:0.0001\n",
      "Epoch 32, Step: 57, Loss: 0.6703034043312073, Lr:0.0001\n",
      "Epoch 32, Step: 58, Loss: 0.6370896100997925, Lr:0.0001\n",
      "Epoch 32, Step: 59, Loss: 0.455687552690506, Lr:0.0001\n",
      "Epoch 32, Step: 60, Loss: 0.4664955139160156, Lr:0.0001\n",
      "Epoch 32, Step: 61, Loss: 0.8035521507263184, Lr:0.0001\n",
      "Epoch 32, Step: 62, Loss: 0.4151146709918976, Lr:0.0001\n",
      "Epoch 32, Step: 63, Loss: 0.47019922733306885, Lr:0.0001\n",
      "Epoch 32, Step: 64, Loss: 0.6562212705612183, Lr:0.0001\n",
      "Epoch 32, Step: 65, Loss: 0.4014129936695099, Lr:0.0001\n",
      "Epoch 32, Step: 66, Loss: 1.4753433465957642, Lr:0.0001\n",
      "Epoch 32, Step: 67, Loss: 0.3910820484161377, Lr:0.0001\n",
      "Epoch 32, Step: 68, Loss: 0.09012576192617416, Lr:0.0001\n",
      "Epoch 32, Step: 69, Loss: 0.2173917442560196, Lr:0.0001\n",
      "Epoch 32, Step: 70, Loss: 0.7923873662948608, Lr:0.0001\n",
      "Epoch 32, Step: 71, Loss: 0.9854027628898621, Lr:0.0001\n",
      "Epoch 32, Step: 72, Loss: 1.8171855211257935, Lr:0.0001\n",
      "Epoch 32, Step: 73, Loss: 0.3104611337184906, Lr:0.0001\n",
      "Epoch 32, Step: 74, Loss: 0.6635593175888062, Lr:0.0001\n",
      "Epoch 32, Step: 75, Loss: 0.1438925564289093, Lr:0.0001\n",
      "Epoch 32, Step: 76, Loss: 1.1044633388519287, Lr:0.0001\n",
      "Epoch 32, Step: 77, Loss: 0.11383933573961258, Lr:0.0001\n",
      "Epoch 32, Step: 78, Loss: 0.8070477843284607, Lr:0.0001\n",
      "Epoch 32, Step: 79, Loss: 0.46038007736206055, Lr:0.0001\n",
      "Epoch 32, Step: 80, Loss: 0.7726280093193054, Lr:0.0001\n",
      "Epoch 32, Step: 81, Loss: 0.597069263458252, Lr:0.0001\n",
      "Epoch 32, Step: 82, Loss: 0.6275069117546082, Lr:0.0001\n",
      "Epoch 32, Step: 83, Loss: 0.26372161507606506, Lr:0.0001\n",
      "Epoch 32, Step: 84, Loss: 0.9370542764663696, Lr:0.0001\n",
      "Epoch 32, Step: 85, Loss: 0.37006914615631104, Lr:0.0001\n",
      "Epoch 32, Step: 86, Loss: 0.3399137854576111, Lr:0.0001\n",
      "Epoch 32, Step: 87, Loss: 0.3380000591278076, Lr:0.0001\n",
      "Epoch 32, Step: 88, Loss: 2.2881643772125244, Lr:0.0001\n",
      "Epoch 32, Step: 89, Loss: 0.8080883622169495, Lr:0.0001\n",
      "Epoch 32, Step: 90, Loss: 0.5599156618118286, Lr:0.0001\n",
      "Epoch 32, Step: 91, Loss: 2.215838670730591, Lr:0.0001\n",
      "Epoch 32, Step: 92, Loss: 0.7051151990890503, Lr:0.0001\n",
      "Epoch 32, Step: 93, Loss: 0.527619481086731, Lr:0.0001\n",
      "Epoch 32, Step: 94, Loss: 0.33529725670814514, Lr:0.0001\n",
      "Epoch 32, Step: 95, Loss: 0.32315871119499207, Lr:0.0001\n",
      "Epoch 32, Step: 96, Loss: 0.28031831979751587, Lr:0.0001\n",
      "Epoch 32, Step: 97, Loss: 1.5097301006317139, Lr:0.0001\n",
      "Epoch 32, Step: 98, Loss: 1.3274047374725342, Lr:0.0001\n",
      "Epoch 32, Step: 99, Loss: 0.5122714638710022, Lr:0.0001\n",
      "Epoch 32, Step: 100, Loss: 0.8143815398216248, Lr:0.0001\n",
      "Epoch 32, Step: 101, Loss: 1.0237798690795898, Lr:0.0001\n",
      "Epoch 32, Step: 102, Loss: 0.5136030316352844, Lr:0.0001\n",
      "Epoch 32, Step: 103, Loss: 0.5196534991264343, Lr:0.0001\n",
      "Epoch 32, Step: 104, Loss: 1.0761642456054688, Lr:0.0001\n",
      "Epoch 32, Step: 105, Loss: 0.5489162802696228, Lr:0.0001\n",
      "Epoch 32, Step: 106, Loss: 0.4175637364387512, Lr:0.0001\n",
      "Epoch 32, Step: 107, Loss: 0.8447175025939941, Lr:0.0001\n",
      "Epoch 32, Step: 108, Loss: 0.660040020942688, Lr:0.0001\n",
      "Epoch 32, Step: 109, Loss: 0.5579289197921753, Lr:0.0001\n",
      "Epoch 32, Step: 110, Loss: 0.7806422710418701, Lr:0.0001\n",
      "Epoch 32, Step: 111, Loss: 0.549683153629303, Lr:0.0001\n",
      "Epoch 32, Step: 112, Loss: 0.3331611454486847, Lr:0.0001\n",
      "Epoch 32, Step: 113, Loss: 1.6830812692642212, Lr:0.0001\n",
      "Epoch 32, Step: 114, Loss: 0.6640141010284424, Lr:0.0001\n",
      "Epoch 32, Step: 115, Loss: 0.8104720711708069, Lr:0.0001\n",
      "Epoch 32, Step: 116, Loss: 0.55118727684021, Lr:0.0001\n",
      "Epoch 32, Step: 117, Loss: 0.592765748500824, Lr:0.0001\n",
      "Epoch 32, Step: 118, Loss: 0.15666064620018005, Lr:0.0001\n",
      "Epoch 32, Step: 119, Loss: 1.0751526355743408, Lr:0.0001\n",
      "Epoch 32, Step: 120, Loss: 0.640995979309082, Lr:0.0001\n",
      "Epoch 32, Step: 121, Loss: 1.1574859619140625, Lr:0.0001\n",
      "Epoch 32, Step: 122, Loss: 0.5309494137763977, Lr:0.0001\n",
      "Epoch 32, Step: 123, Loss: 0.2114383429288864, Lr:0.0001\n",
      "Epoch 32, Step: 124, Loss: 0.3156452476978302, Lr:0.0001\n",
      "Epoch 32, Step: 125, Loss: 0.13738934695720673, Lr:0.0001\n",
      "Epoch 32, Step: 126, Loss: 0.2745473086833954, Lr:0.0001\n",
      "Epoch 32, Step: 127, Loss: 0.8709805011749268, Lr:0.0001\n",
      "Epoch 32, Step: 128, Loss: 0.35538777709007263, Lr:0.0001\n",
      "Epoch 32, Step: 129, Loss: 1.8358830213546753, Lr:0.0001\n",
      "Epoch 32, Step: 130, Loss: 0.6188618540763855, Lr:0.0001\n",
      "Epoch 32, Step: 131, Loss: 0.2603910565376282, Lr:0.0001\n",
      "Epoch 32, Step: 132, Loss: 1.7803360223770142, Lr:0.0001\n",
      "Epoch 32, Step: 133, Loss: 0.8382517099380493, Lr:0.0001\n",
      "Epoch 32, Step: 134, Loss: 0.4993193447589874, Lr:0.0001\n",
      "Epoch 32, Step: 135, Loss: 0.2542637884616852, Lr:0.0001\n",
      "Epoch 32, Step: 136, Loss: 0.3058834373950958, Lr:0.0001\n",
      "Epoch 32, Step: 137, Loss: 0.24262617528438568, Lr:0.0001\n",
      "Epoch 32, Step: 138, Loss: 1.4010812044143677, Lr:0.0001\n",
      "Epoch 32, Step: 139, Loss: 0.6548184156417847, Lr:0.0001\n",
      "Epoch 32, Step: 140, Loss: 0.44485437870025635, Lr:0.0001\n",
      "Epoch 32, Step: 141, Loss: 0.06653451174497604, Lr:0.0001\n",
      "Epoch 32, Step: 142, Loss: 0.5841455459594727, Lr:0.0001\n",
      "Epoch 32, Step: 143, Loss: 0.6840137839317322, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 32\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.6500 (0.6500) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0106 data: 0.0055 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.9466 (1.1238) acc1: 75.0000 (66.6667) acc5: 100.0000 (100.0000) time: 0.0080 data: 0.0037 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0081 s / it)\n",
      "* Acc@1 66.667 Acc@5 100.000 loss 1.124\n",
      "Accuracy of the network on the 33 test image: 66.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 33, Step: 0, Loss: 0.9049263596534729, Lr:0.0001\n",
      "Epoch 33, Step: 1, Loss: 1.3362650871276855, Lr:0.0001\n",
      "Epoch 33, Step: 2, Loss: 0.4586419463157654, Lr:0.0001\n",
      "Epoch 33, Step: 3, Loss: 0.0995074063539505, Lr:0.0001\n",
      "Epoch 33, Step: 4, Loss: 0.15919440984725952, Lr:0.0001\n",
      "Epoch 33, Step: 5, Loss: 0.5044735074043274, Lr:0.0001\n",
      "Epoch 33, Step: 6, Loss: 0.6407189965248108, Lr:0.0001\n",
      "Epoch 33, Step: 7, Loss: 0.8295706510543823, Lr:0.0001\n",
      "Epoch 33, Step: 8, Loss: 0.30982303619384766, Lr:0.0001\n",
      "Epoch 33, Step: 9, Loss: 0.8624102473258972, Lr:0.0001\n",
      "Epoch 33, Step: 10, Loss: 0.8525426387786865, Lr:0.0001\n",
      "Epoch 33, Step: 11, Loss: 0.6764997839927673, Lr:0.0001\n",
      "Epoch 33, Step: 12, Loss: 0.4635421335697174, Lr:0.0001\n",
      "Epoch 33, Step: 13, Loss: 0.9958052039146423, Lr:0.0001\n",
      "Epoch 33, Step: 14, Loss: 0.9166854023933411, Lr:0.0001\n",
      "Epoch 33, Step: 15, Loss: 1.6615955829620361, Lr:0.0001\n",
      "Epoch 33, Step: 16, Loss: 0.21902436017990112, Lr:0.0001\n",
      "Epoch 33, Step: 17, Loss: 0.3200005292892456, Lr:0.0001\n",
      "Epoch 33, Step: 18, Loss: 1.093161702156067, Lr:0.0001\n",
      "Epoch 33, Step: 19, Loss: 0.7137495875358582, Lr:0.0001\n",
      "Epoch 33, Step: 20, Loss: 0.436359167098999, Lr:0.0001\n",
      "Epoch 33, Step: 21, Loss: 0.4169357419013977, Lr:0.0001\n",
      "Epoch 33, Step: 22, Loss: 0.7167724967002869, Lr:0.0001\n",
      "Epoch 33, Step: 23, Loss: 0.5329903960227966, Lr:0.0001\n",
      "Epoch 33, Step: 24, Loss: 0.5045445561408997, Lr:0.0001\n",
      "Epoch 33, Step: 25, Loss: 0.9308298826217651, Lr:0.0001\n",
      "Epoch 33, Step: 26, Loss: 0.4321313500404358, Lr:0.0001\n",
      "Epoch 33, Step: 27, Loss: 0.3688463270664215, Lr:0.0001\n",
      "Epoch 33, Step: 28, Loss: 0.5274747610092163, Lr:0.0001\n",
      "Epoch 33, Step: 29, Loss: 0.6782504916191101, Lr:0.0001\n",
      "Epoch 33, Step: 30, Loss: 1.5115981101989746, Lr:0.0001\n",
      "Epoch 33, Step: 31, Loss: 0.8368237614631653, Lr:0.0001\n",
      "Epoch 33, Step: 32, Loss: 0.9878871440887451, Lr:0.0001\n",
      "Epoch 33, Step: 33, Loss: 0.8081574440002441, Lr:0.0001\n",
      "Epoch 33, Step: 34, Loss: 0.9254581928253174, Lr:0.0001\n",
      "Epoch 33, Step: 35, Loss: 0.6631691455841064, Lr:0.0001\n",
      "Epoch 33, Step: 36, Loss: 1.2494192123413086, Lr:0.0001\n",
      "Epoch 33, Step: 37, Loss: 1.1018614768981934, Lr:0.0001\n",
      "Epoch 33, Step: 38, Loss: 0.5387682914733887, Lr:0.0001\n",
      "Epoch 33, Step: 39, Loss: 0.9386345148086548, Lr:0.0001\n",
      "Epoch 33, Step: 40, Loss: 0.5641175508499146, Lr:0.0001\n",
      "Epoch 33, Step: 41, Loss: 0.07763190567493439, Lr:0.0001\n",
      "Epoch 33, Step: 42, Loss: 1.014887809753418, Lr:0.0001\n",
      "Epoch 33, Step: 43, Loss: 0.9261864423751831, Lr:0.0001\n",
      "Epoch 33, Step: 44, Loss: 0.689088761806488, Lr:0.0001\n",
      "Epoch 33, Step: 45, Loss: 0.803185760974884, Lr:0.0001\n",
      "Epoch 33, Step: 46, Loss: 0.7066469788551331, Lr:0.0001\n",
      "Epoch 33, Step: 47, Loss: 0.29692983627319336, Lr:0.0001\n",
      "Epoch 33, Step: 48, Loss: 0.6405140161514282, Lr:0.0001\n",
      "Epoch 33, Step: 49, Loss: 1.0956406593322754, Lr:0.0001\n",
      "Epoch 33, Step: 50, Loss: 0.298536479473114, Lr:0.0001\n",
      "Epoch 33, Step: 51, Loss: 1.0497801303863525, Lr:0.0001\n",
      "Epoch 33, Step: 52, Loss: 0.7880454063415527, Lr:0.0001\n",
      "Epoch 33, Step: 53, Loss: 2.1861038208007812, Lr:0.0001\n",
      "Epoch 33, Step: 54, Loss: 0.6764184832572937, Lr:0.0001\n",
      "Epoch 33, Step: 55, Loss: 0.7591249942779541, Lr:0.0001\n",
      "Epoch 33, Step: 56, Loss: 1.1813842058181763, Lr:0.0001\n",
      "Epoch 33, Step: 57, Loss: 0.05915680527687073, Lr:0.0001\n",
      "Epoch 33, Step: 58, Loss: 0.32053935527801514, Lr:0.0001\n",
      "Epoch 33, Step: 59, Loss: 0.49971356987953186, Lr:0.0001\n",
      "Epoch 33, Step: 60, Loss: 0.2806687355041504, Lr:0.0001\n",
      "Epoch 33, Step: 61, Loss: 0.5663920044898987, Lr:0.0001\n",
      "Epoch 33, Step: 62, Loss: 1.4467971324920654, Lr:0.0001\n",
      "Epoch 33, Step: 63, Loss: 0.5486815571784973, Lr:0.0001\n",
      "Epoch 33, Step: 64, Loss: 0.8219581842422485, Lr:0.0001\n",
      "Epoch 33, Step: 65, Loss: 0.5675773024559021, Lr:0.0001\n",
      "Epoch 33, Step: 66, Loss: 2.3229897022247314, Lr:0.0001\n",
      "Epoch 33, Step: 67, Loss: 1.0325870513916016, Lr:0.0001\n",
      "Epoch 33, Step: 68, Loss: 0.7821453213691711, Lr:0.0001\n",
      "Epoch 33, Step: 69, Loss: 0.9908367395401001, Lr:0.0001\n",
      "Epoch 33, Step: 70, Loss: 1.1129482984542847, Lr:0.0001\n",
      "Epoch 33, Step: 71, Loss: 0.5652631521224976, Lr:0.0001\n",
      "Epoch 33, Step: 72, Loss: 2.5560364723205566, Lr:0.0001\n",
      "Epoch 33, Step: 73, Loss: 0.8197420239448547, Lr:0.0001\n",
      "Epoch 33, Step: 74, Loss: 0.9968279004096985, Lr:0.0001\n",
      "Epoch 33, Step: 75, Loss: 1.281164288520813, Lr:0.0001\n",
      "Epoch 33, Step: 76, Loss: 0.6223157644271851, Lr:0.0001\n",
      "Epoch 33, Step: 77, Loss: 0.5757015347480774, Lr:0.0001\n",
      "Epoch 33, Step: 78, Loss: 0.9579489231109619, Lr:0.0001\n",
      "Epoch 33, Step: 79, Loss: 1.1427685022354126, Lr:0.0001\n",
      "Epoch 33, Step: 80, Loss: 1.7333459854125977, Lr:0.0001\n",
      "Epoch 33, Step: 81, Loss: 0.5298407077789307, Lr:0.0001\n",
      "Epoch 33, Step: 82, Loss: 0.7004382610321045, Lr:0.0001\n",
      "Epoch 33, Step: 83, Loss: 0.26799118518829346, Lr:0.0001\n",
      "Epoch 33, Step: 84, Loss: 1.083954095840454, Lr:0.0001\n",
      "Epoch 33, Step: 85, Loss: 0.5821822881698608, Lr:0.0001\n",
      "Epoch 33, Step: 86, Loss: 0.2710552513599396, Lr:0.0001\n",
      "Epoch 33, Step: 87, Loss: 1.0212101936340332, Lr:0.0001\n",
      "Epoch 33, Step: 88, Loss: 0.03820405155420303, Lr:0.0001\n",
      "Epoch 33, Step: 89, Loss: 0.7373147010803223, Lr:0.0001\n",
      "Epoch 33, Step: 90, Loss: 0.04489826783537865, Lr:0.0001\n",
      "Epoch 33, Step: 91, Loss: 0.3512336015701294, Lr:0.0001\n",
      "Epoch 33, Step: 92, Loss: 0.7331661581993103, Lr:0.0001\n",
      "Epoch 33, Step: 93, Loss: 0.03427527844905853, Lr:0.0001\n",
      "Epoch 33, Step: 94, Loss: 0.8984306454658508, Lr:0.0001\n",
      "Epoch 33, Step: 95, Loss: 1.108062744140625, Lr:0.0001\n",
      "Epoch 33, Step: 96, Loss: 0.8328769207000732, Lr:0.0001\n",
      "Epoch 33, Step: 97, Loss: 0.022281400859355927, Lr:0.0001\n",
      "Epoch 33, Step: 98, Loss: 1.8270385265350342, Lr:0.0001\n",
      "Epoch 33, Step: 99, Loss: 0.3684329390525818, Lr:0.0001\n",
      "Epoch 33, Step: 100, Loss: 0.3022938668727875, Lr:0.0001\n",
      "Epoch 33, Step: 101, Loss: 0.07889058440923691, Lr:0.0001\n",
      "Epoch 33, Step: 102, Loss: 0.024020163342356682, Lr:0.0001\n",
      "Epoch 33, Step: 103, Loss: 0.6681579351425171, Lr:0.0001\n",
      "Epoch 33, Step: 104, Loss: 0.46241772174835205, Lr:0.0001\n",
      "Epoch 33, Step: 105, Loss: 0.27562379837036133, Lr:0.0001\n",
      "Epoch 33, Step: 106, Loss: 0.873492956161499, Lr:0.0001\n",
      "Epoch 33, Step: 107, Loss: 0.6984103918075562, Lr:0.0001\n",
      "Epoch 33, Step: 108, Loss: 0.6063048839569092, Lr:0.0001\n",
      "Epoch 33, Step: 109, Loss: 0.36861714720726013, Lr:0.0001\n",
      "Epoch 33, Step: 110, Loss: 0.11170728504657745, Lr:0.0001\n",
      "Epoch 33, Step: 111, Loss: 0.7216421365737915, Lr:0.0001\n",
      "Epoch 33, Step: 112, Loss: 1.1111512184143066, Lr:0.0001\n",
      "Epoch 33, Step: 113, Loss: 0.4382569193840027, Lr:0.0001\n",
      "Epoch 33, Step: 114, Loss: 1.2364424467086792, Lr:0.0001\n",
      "Epoch 33, Step: 115, Loss: 0.17848211526870728, Lr:0.0001\n",
      "Epoch 33, Step: 116, Loss: 1.288529634475708, Lr:0.0001\n",
      "Epoch 33, Step: 117, Loss: 1.0697696208953857, Lr:0.0001\n",
      "Epoch 33, Step: 118, Loss: 0.7873004674911499, Lr:0.0001\n",
      "Epoch 33, Step: 119, Loss: 0.3993591070175171, Lr:0.0001\n",
      "Epoch 33, Step: 120, Loss: 0.5371361970901489, Lr:0.0001\n",
      "Epoch 33, Step: 121, Loss: 0.9013580083847046, Lr:0.0001\n",
      "Epoch 33, Step: 122, Loss: 0.20808357000350952, Lr:0.0001\n",
      "Epoch 33, Step: 123, Loss: 0.7218101024627686, Lr:0.0001\n",
      "Epoch 33, Step: 124, Loss: 0.42855775356292725, Lr:0.0001\n",
      "Epoch 33, Step: 125, Loss: 0.05525016784667969, Lr:0.0001\n",
      "Epoch 33, Step: 126, Loss: 0.3856765329837799, Lr:0.0001\n",
      "Epoch 33, Step: 127, Loss: 0.6584179997444153, Lr:0.0001\n",
      "Epoch 33, Step: 128, Loss: 0.4601038694381714, Lr:0.0001\n",
      "Epoch 33, Step: 129, Loss: 0.6811124682426453, Lr:0.0001\n",
      "Epoch 33, Step: 130, Loss: 0.7976593971252441, Lr:0.0001\n",
      "Epoch 33, Step: 131, Loss: 0.9436035752296448, Lr:0.0001\n",
      "Epoch 33, Step: 132, Loss: 0.1828492134809494, Lr:0.0001\n",
      "Epoch 33, Step: 133, Loss: 0.6515946388244629, Lr:0.0001\n",
      "Epoch 33, Step: 134, Loss: 0.7645525932312012, Lr:0.0001\n",
      "Epoch 33, Step: 135, Loss: 0.39858478307724, Lr:0.0001\n",
      "Epoch 33, Step: 136, Loss: 0.6164979338645935, Lr:0.0001\n",
      "Epoch 33, Step: 137, Loss: 1.80667245388031, Lr:0.0001\n",
      "Epoch 33, Step: 138, Loss: 0.8046793341636658, Lr:0.0001\n",
      "Epoch 33, Step: 139, Loss: 0.16882574558258057, Lr:0.0001\n",
      "Epoch 33, Step: 140, Loss: 0.3474840521812439, Lr:0.0001\n",
      "Epoch 33, Step: 141, Loss: 0.060009174048900604, Lr:0.0001\n",
      "Epoch 33, Step: 142, Loss: 0.5049001574516296, Lr:0.0001\n",
      "Epoch 33, Step: 143, Loss: 0.10290343314409256, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 33\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0400 (0.0400) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0090 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.7222 (0.8653) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0075 data: 0.0039 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0076 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 0.865\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 34, Step: 0, Loss: 0.8195405602455139, Lr:0.0001\n",
      "Epoch 34, Step: 1, Loss: 0.18841101229190826, Lr:0.0001\n",
      "Epoch 34, Step: 2, Loss: 1.7315534353256226, Lr:0.0001\n",
      "Epoch 34, Step: 3, Loss: 0.3496779799461365, Lr:0.0001\n",
      "Epoch 34, Step: 4, Loss: 0.0854412093758583, Lr:0.0001\n",
      "Epoch 34, Step: 5, Loss: 0.1918269246816635, Lr:0.0001\n",
      "Epoch 34, Step: 6, Loss: 2.461857318878174, Lr:0.0001\n",
      "Epoch 34, Step: 7, Loss: 1.2567542791366577, Lr:0.0001\n",
      "Epoch 34, Step: 8, Loss: 0.8528041243553162, Lr:0.0001\n",
      "Epoch 34, Step: 9, Loss: 0.5152547359466553, Lr:0.0001\n",
      "Epoch 34, Step: 10, Loss: 0.7522808313369751, Lr:0.0001\n",
      "Epoch 34, Step: 11, Loss: 0.4621274173259735, Lr:0.0001\n",
      "Epoch 34, Step: 12, Loss: 0.5864280462265015, Lr:0.0001\n",
      "Epoch 34, Step: 13, Loss: 0.6759135127067566, Lr:0.0001\n",
      "Epoch 34, Step: 14, Loss: 0.32479262351989746, Lr:0.0001\n",
      "Epoch 34, Step: 15, Loss: 0.6442317962646484, Lr:0.0001\n",
      "Epoch 34, Step: 16, Loss: 0.21327434480190277, Lr:0.0001\n",
      "Epoch 34, Step: 17, Loss: 0.8051953315734863, Lr:0.0001\n",
      "Epoch 34, Step: 18, Loss: 0.744739830493927, Lr:0.0001\n",
      "Epoch 34, Step: 19, Loss: 0.1219426691532135, Lr:0.0001\n",
      "Epoch 34, Step: 20, Loss: 0.5709835290908813, Lr:0.0001\n",
      "Epoch 34, Step: 21, Loss: 0.8143672943115234, Lr:0.0001\n",
      "Epoch 34, Step: 22, Loss: 0.0732518807053566, Lr:0.0001\n",
      "Epoch 34, Step: 23, Loss: 0.24122053384780884, Lr:0.0001\n",
      "Epoch 34, Step: 24, Loss: 0.26598402857780457, Lr:0.0001\n",
      "Epoch 34, Step: 25, Loss: 0.7531002759933472, Lr:0.0001\n",
      "Epoch 34, Step: 26, Loss: 0.3838968575000763, Lr:0.0001\n",
      "Epoch 34, Step: 27, Loss: 0.32300594449043274, Lr:0.0001\n",
      "Epoch 34, Step: 28, Loss: 0.4391951560974121, Lr:0.0001\n",
      "Epoch 34, Step: 29, Loss: 0.6376373767852783, Lr:0.0001\n",
      "Epoch 34, Step: 30, Loss: 0.6131798624992371, Lr:0.0001\n",
      "Epoch 34, Step: 31, Loss: 0.4386121928691864, Lr:0.0001\n",
      "Epoch 34, Step: 32, Loss: 1.097361445426941, Lr:0.0001\n",
      "Epoch 34, Step: 33, Loss: 0.9143901467323303, Lr:0.0001\n",
      "Epoch 34, Step: 34, Loss: 0.3576725423336029, Lr:0.0001\n",
      "Epoch 34, Step: 35, Loss: 0.2589882016181946, Lr:0.0001\n",
      "Epoch 34, Step: 36, Loss: 0.3605974316596985, Lr:0.0001\n",
      "Epoch 34, Step: 37, Loss: 0.5618851780891418, Lr:0.0001\n",
      "Epoch 34, Step: 38, Loss: 0.7773808240890503, Lr:0.0001\n",
      "Epoch 34, Step: 39, Loss: 0.6280034184455872, Lr:0.0001\n",
      "Epoch 34, Step: 40, Loss: 0.5950562953948975, Lr:0.0001\n",
      "Epoch 34, Step: 41, Loss: 1.9089939594268799, Lr:0.0001\n",
      "Epoch 34, Step: 42, Loss: 0.3455040454864502, Lr:0.0001\n",
      "Epoch 34, Step: 43, Loss: 0.38932377099990845, Lr:0.0001\n",
      "Epoch 34, Step: 44, Loss: 0.34468185901641846, Lr:0.0001\n",
      "Epoch 34, Step: 45, Loss: 0.6890819072723389, Lr:0.0001\n",
      "Epoch 34, Step: 46, Loss: 0.44463974237442017, Lr:0.0001\n",
      "Epoch 34, Step: 47, Loss: 0.9157870411872864, Lr:0.0001\n",
      "Epoch 34, Step: 48, Loss: 1.115075707435608, Lr:0.0001\n",
      "Epoch 34, Step: 49, Loss: 0.4137799143791199, Lr:0.0001\n",
      "Epoch 34, Step: 50, Loss: 0.584579586982727, Lr:0.0001\n",
      "Epoch 34, Step: 51, Loss: 0.10712167620658875, Lr:0.0001\n",
      "Epoch 34, Step: 52, Loss: 0.3343605399131775, Lr:0.0001\n",
      "Epoch 34, Step: 53, Loss: 1.3713499307632446, Lr:0.0001\n",
      "Epoch 34, Step: 54, Loss: 0.17147599160671234, Lr:0.0001\n",
      "Epoch 34, Step: 55, Loss: 1.4197176694869995, Lr:0.0001\n",
      "Epoch 34, Step: 56, Loss: 0.8851537108421326, Lr:0.0001\n",
      "Epoch 34, Step: 57, Loss: 0.3905904293060303, Lr:0.0001\n",
      "Epoch 34, Step: 58, Loss: 0.08930517733097076, Lr:0.0001\n",
      "Epoch 34, Step: 59, Loss: 0.3538806438446045, Lr:0.0001\n",
      "Epoch 34, Step: 60, Loss: 0.43903982639312744, Lr:0.0001\n",
      "Epoch 34, Step: 61, Loss: 0.7843114137649536, Lr:0.0001\n",
      "Epoch 34, Step: 62, Loss: 0.6285214424133301, Lr:0.0001\n",
      "Epoch 34, Step: 63, Loss: 1.0532159805297852, Lr:0.0001\n",
      "Epoch 34, Step: 64, Loss: 0.5822967886924744, Lr:0.0001\n",
      "Epoch 34, Step: 65, Loss: 0.44372275471687317, Lr:0.0001\n",
      "Epoch 34, Step: 66, Loss: 0.2574450373649597, Lr:0.0001\n",
      "Epoch 34, Step: 67, Loss: 0.6977766752243042, Lr:0.0001\n",
      "Epoch 34, Step: 68, Loss: 0.39012324810028076, Lr:0.0001\n",
      "Epoch 34, Step: 69, Loss: 0.29213428497314453, Lr:0.0001\n",
      "Epoch 34, Step: 70, Loss: 0.12585526704788208, Lr:0.0001\n",
      "Epoch 34, Step: 71, Loss: 0.22392264008522034, Lr:0.0001\n",
      "Epoch 34, Step: 72, Loss: 0.849704384803772, Lr:0.0001\n",
      "Epoch 34, Step: 73, Loss: 1.400206208229065, Lr:0.0001\n",
      "Epoch 34, Step: 74, Loss: 0.5734558701515198, Lr:0.0001\n",
      "Epoch 34, Step: 75, Loss: 0.5883787274360657, Lr:0.0001\n",
      "Epoch 34, Step: 76, Loss: 0.885874330997467, Lr:0.0001\n",
      "Epoch 34, Step: 77, Loss: 0.17928601801395416, Lr:0.0001\n",
      "Epoch 34, Step: 78, Loss: 0.06656362116336823, Lr:0.0001\n",
      "Epoch 34, Step: 79, Loss: 0.3364585340023041, Lr:0.0001\n",
      "Epoch 34, Step: 80, Loss: 0.5522034168243408, Lr:0.0001\n",
      "Epoch 34, Step: 81, Loss: 0.7676239609718323, Lr:0.0001\n",
      "Epoch 34, Step: 82, Loss: 0.8779046535491943, Lr:0.0001\n",
      "Epoch 34, Step: 83, Loss: 0.939283013343811, Lr:0.0001\n",
      "Epoch 34, Step: 84, Loss: 0.15817232429981232, Lr:0.0001\n",
      "Epoch 34, Step: 85, Loss: 0.47874146699905396, Lr:0.0001\n",
      "Epoch 34, Step: 86, Loss: 0.6292598843574524, Lr:0.0001\n",
      "Epoch 34, Step: 87, Loss: 0.4502304196357727, Lr:0.0001\n",
      "Epoch 34, Step: 88, Loss: 1.2859481573104858, Lr:0.0001\n",
      "Epoch 34, Step: 89, Loss: 0.3637505769729614, Lr:0.0001\n",
      "Epoch 34, Step: 90, Loss: 0.42671453952789307, Lr:0.0001\n",
      "Epoch 34, Step: 91, Loss: 0.3477966785430908, Lr:0.0001\n",
      "Epoch 34, Step: 92, Loss: 0.4701419770717621, Lr:0.0001\n",
      "Epoch 34, Step: 93, Loss: 0.5946381688117981, Lr:0.0001\n",
      "Epoch 34, Step: 94, Loss: 1.0290414094924927, Lr:0.0001\n",
      "Epoch 34, Step: 95, Loss: 0.4593002200126648, Lr:0.0001\n",
      "Epoch 34, Step: 96, Loss: 0.29971402883529663, Lr:0.0001\n",
      "Epoch 34, Step: 97, Loss: 0.6325491666793823, Lr:0.0001\n",
      "Epoch 34, Step: 98, Loss: 0.6769436597824097, Lr:0.0001\n",
      "Epoch 34, Step: 99, Loss: 1.0932667255401611, Lr:0.0001\n",
      "Epoch 34, Step: 100, Loss: 0.6924418210983276, Lr:0.0001\n",
      "Epoch 34, Step: 101, Loss: 1.0413031578063965, Lr:0.0001\n",
      "Epoch 34, Step: 102, Loss: 0.7426138520240784, Lr:0.0001\n",
      "Epoch 34, Step: 103, Loss: 0.04961668699979782, Lr:0.0001\n",
      "Epoch 34, Step: 104, Loss: 0.7143471837043762, Lr:0.0001\n",
      "Epoch 34, Step: 105, Loss: 0.10259775072336197, Lr:0.0001\n",
      "Epoch 34, Step: 106, Loss: 0.5626052021980286, Lr:0.0001\n",
      "Epoch 34, Step: 107, Loss: 0.26980212330818176, Lr:0.0001\n",
      "Epoch 34, Step: 108, Loss: 0.7889547348022461, Lr:0.0001\n",
      "Epoch 34, Step: 109, Loss: 0.8504830002784729, Lr:0.0001\n",
      "Epoch 34, Step: 110, Loss: 0.23660525679588318, Lr:0.0001\n",
      "Epoch 34, Step: 111, Loss: 0.019148990511894226, Lr:0.0001\n",
      "Epoch 34, Step: 112, Loss: 0.7739205360412598, Lr:0.0001\n",
      "Epoch 34, Step: 113, Loss: 0.36552679538726807, Lr:0.0001\n",
      "Epoch 34, Step: 114, Loss: 0.36931583285331726, Lr:0.0001\n",
      "Epoch 34, Step: 115, Loss: 0.46434563398361206, Lr:0.0001\n",
      "Epoch 34, Step: 116, Loss: 1.6223640441894531, Lr:0.0001\n",
      "Epoch 34, Step: 117, Loss: 0.6528794765472412, Lr:0.0001\n",
      "Epoch 34, Step: 118, Loss: 1.110798954963684, Lr:0.0001\n",
      "Epoch 34, Step: 119, Loss: 0.3843456208705902, Lr:0.0001\n",
      "Epoch 34, Step: 120, Loss: 0.8957394957542419, Lr:0.0001\n",
      "Epoch 34, Step: 121, Loss: 1.0923596620559692, Lr:0.0001\n",
      "Epoch 34, Step: 122, Loss: 0.4788789749145508, Lr:0.0001\n",
      "Epoch 34, Step: 123, Loss: 1.2820545434951782, Lr:0.0001\n",
      "Epoch 34, Step: 124, Loss: 1.1075897216796875, Lr:0.0001\n",
      "Epoch 34, Step: 125, Loss: 0.114933080971241, Lr:0.0001\n",
      "Epoch 34, Step: 126, Loss: 0.3420209586620331, Lr:0.0001\n",
      "Epoch 34, Step: 127, Loss: 0.4084579050540924, Lr:0.0001\n",
      "Epoch 34, Step: 128, Loss: 0.9515707492828369, Lr:0.0001\n",
      "Epoch 34, Step: 129, Loss: 0.36521121859550476, Lr:0.0001\n",
      "Epoch 34, Step: 130, Loss: 1.8412742614746094, Lr:0.0001\n",
      "Epoch 34, Step: 131, Loss: 0.08037669211626053, Lr:0.0001\n",
      "Epoch 34, Step: 132, Loss: 0.9617201089859009, Lr:0.0001\n",
      "Epoch 34, Step: 133, Loss: 0.5337569117546082, Lr:0.0001\n",
      "Epoch 34, Step: 134, Loss: 1.0508867502212524, Lr:0.0001\n",
      "Epoch 34, Step: 135, Loss: 0.8788466453552246, Lr:0.0001\n",
      "Epoch 34, Step: 136, Loss: 0.6739414930343628, Lr:0.0001\n",
      "Epoch 34, Step: 137, Loss: 1.5304677486419678, Lr:0.0001\n",
      "Epoch 34, Step: 138, Loss: 0.798987090587616, Lr:0.0001\n",
      "Epoch 34, Step: 139, Loss: 0.6155969500541687, Lr:0.0001\n",
      "Epoch 34, Step: 140, Loss: 0.6733821034431458, Lr:0.0001\n",
      "Epoch 34, Step: 141, Loss: 0.6924178004264832, Lr:0.0001\n",
      "Epoch 34, Step: 142, Loss: 0.8889015913009644, Lr:0.0001\n",
      "Epoch 34, Step: 143, Loss: 0.3511132299900055, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 34\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.9321 (0.9321) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0109 data: 0.0049 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.9590 (1.1344) acc1: 75.0000 (69.6970) acc5: 100.0000 (100.0000) time: 0.0075 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0075 s / it)\n",
      "* Acc@1 69.697 Acc@5 100.000 loss 1.134\n",
      "Accuracy of the network on the 33 test image: 69.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 35, Step: 0, Loss: 0.28513097763061523, Lr:0.0001\n",
      "Epoch 35, Step: 1, Loss: 0.8259886503219604, Lr:0.0001\n",
      "Epoch 35, Step: 2, Loss: 0.7641476392745972, Lr:0.0001\n",
      "Epoch 35, Step: 3, Loss: 0.2610751688480377, Lr:0.0001\n",
      "Epoch 35, Step: 4, Loss: 0.09970518946647644, Lr:0.0001\n",
      "Epoch 35, Step: 5, Loss: 0.8681849837303162, Lr:0.0001\n",
      "Epoch 35, Step: 6, Loss: 0.4263346195220947, Lr:0.0001\n",
      "Epoch 35, Step: 7, Loss: 0.593570351600647, Lr:0.0001\n",
      "Epoch 35, Step: 8, Loss: 0.3997414708137512, Lr:0.0001\n",
      "Epoch 35, Step: 9, Loss: 0.16672776639461517, Lr:0.0001\n",
      "Epoch 35, Step: 10, Loss: 0.2081964612007141, Lr:0.0001\n",
      "Epoch 35, Step: 11, Loss: 0.3824786841869354, Lr:0.0001\n",
      "Epoch 35, Step: 12, Loss: 0.6340053677558899, Lr:0.0001\n",
      "Epoch 35, Step: 13, Loss: 0.30311354994773865, Lr:0.0001\n",
      "Epoch 35, Step: 14, Loss: 0.4499055743217468, Lr:0.0001\n",
      "Epoch 35, Step: 15, Loss: 1.1311771869659424, Lr:0.0001\n",
      "Epoch 35, Step: 16, Loss: 0.7928252816200256, Lr:0.0001\n",
      "Epoch 35, Step: 17, Loss: 0.3596488833427429, Lr:0.0001\n",
      "Epoch 35, Step: 18, Loss: 0.22925640642642975, Lr:0.0001\n",
      "Epoch 35, Step: 19, Loss: 0.6981948614120483, Lr:0.0001\n",
      "Epoch 35, Step: 20, Loss: 2.6042678356170654, Lr:0.0001\n",
      "Epoch 35, Step: 21, Loss: 0.5778385996818542, Lr:0.0001\n",
      "Epoch 35, Step: 22, Loss: 0.08305361121892929, Lr:0.0001\n",
      "Epoch 35, Step: 23, Loss: 0.67684406042099, Lr:0.0001\n",
      "Epoch 35, Step: 24, Loss: 0.8685873746871948, Lr:0.0001\n",
      "Epoch 35, Step: 25, Loss: 1.5019495487213135, Lr:0.0001\n",
      "Epoch 35, Step: 26, Loss: 0.3428245484828949, Lr:0.0001\n",
      "Epoch 35, Step: 27, Loss: 0.32761532068252563, Lr:0.0001\n",
      "Epoch 35, Step: 28, Loss: 0.38730087876319885, Lr:0.0001\n",
      "Epoch 35, Step: 29, Loss: 0.47523948550224304, Lr:0.0001\n",
      "Epoch 35, Step: 30, Loss: 0.4585742950439453, Lr:0.0001\n",
      "Epoch 35, Step: 31, Loss: 1.2997000217437744, Lr:0.0001\n",
      "Epoch 35, Step: 32, Loss: 0.5032959580421448, Lr:0.0001\n",
      "Epoch 35, Step: 33, Loss: 1.1242154836654663, Lr:0.0001\n",
      "Epoch 35, Step: 34, Loss: 0.396263986825943, Lr:0.0001\n",
      "Epoch 35, Step: 35, Loss: 0.25979942083358765, Lr:0.0001\n",
      "Epoch 35, Step: 36, Loss: 0.9670466780662537, Lr:0.0001\n",
      "Epoch 35, Step: 37, Loss: 1.2875580787658691, Lr:0.0001\n",
      "Epoch 35, Step: 38, Loss: 0.3102647662162781, Lr:0.0001\n",
      "Epoch 35, Step: 39, Loss: 1.0915926694869995, Lr:0.0001\n",
      "Epoch 35, Step: 40, Loss: 0.33619174361228943, Lr:0.0001\n",
      "Epoch 35, Step: 41, Loss: 0.7280584573745728, Lr:0.0001\n",
      "Epoch 35, Step: 42, Loss: 0.7822536826133728, Lr:0.0001\n",
      "Epoch 35, Step: 43, Loss: 0.023106247186660767, Lr:0.0001\n",
      "Epoch 35, Step: 44, Loss: 0.2682448625564575, Lr:0.0001\n",
      "Epoch 35, Step: 45, Loss: 0.30909979343414307, Lr:0.0001\n",
      "Epoch 35, Step: 46, Loss: 1.1268181800842285, Lr:0.0001\n",
      "Epoch 35, Step: 47, Loss: 0.29853400588035583, Lr:0.0001\n",
      "Epoch 35, Step: 48, Loss: 0.567477285861969, Lr:0.0001\n",
      "Epoch 35, Step: 49, Loss: 0.16759933531284332, Lr:0.0001\n",
      "Epoch 35, Step: 50, Loss: 0.3997483551502228, Lr:0.0001\n",
      "Epoch 35, Step: 51, Loss: 1.937880039215088, Lr:0.0001\n",
      "Epoch 35, Step: 52, Loss: 0.28613778948783875, Lr:0.0001\n",
      "Epoch 35, Step: 53, Loss: 0.6408774852752686, Lr:0.0001\n",
      "Epoch 35, Step: 54, Loss: 0.7798298001289368, Lr:0.0001\n",
      "Epoch 35, Step: 55, Loss: 1.0677374601364136, Lr:0.0001\n",
      "Epoch 35, Step: 56, Loss: 0.6403066515922546, Lr:0.0001\n",
      "Epoch 35, Step: 57, Loss: 0.19440872967243195, Lr:0.0001\n",
      "Epoch 35, Step: 58, Loss: 0.09545550495386124, Lr:0.0001\n",
      "Epoch 35, Step: 59, Loss: 0.19938381016254425, Lr:0.0001\n",
      "Epoch 35, Step: 60, Loss: 1.652295708656311, Lr:0.0001\n",
      "Epoch 35, Step: 61, Loss: 0.9617459774017334, Lr:0.0001\n",
      "Epoch 35, Step: 62, Loss: 0.9272297620773315, Lr:0.0001\n",
      "Epoch 35, Step: 63, Loss: 0.2798212170600891, Lr:0.0001\n",
      "Epoch 35, Step: 64, Loss: 0.5360640287399292, Lr:0.0001\n",
      "Epoch 35, Step: 65, Loss: 0.8419354557991028, Lr:0.0001\n",
      "Epoch 35, Step: 66, Loss: 0.44336193799972534, Lr:0.0001\n",
      "Epoch 35, Step: 67, Loss: 0.3826022148132324, Lr:0.0001\n",
      "Epoch 35, Step: 68, Loss: 0.2912367880344391, Lr:0.0001\n",
      "Epoch 35, Step: 69, Loss: 0.12081998586654663, Lr:0.0001\n",
      "Epoch 35, Step: 70, Loss: 0.4818463623523712, Lr:0.0001\n",
      "Epoch 35, Step: 71, Loss: 1.5442042350769043, Lr:0.0001\n",
      "Epoch 35, Step: 72, Loss: 0.6195060014724731, Lr:0.0001\n",
      "Epoch 35, Step: 73, Loss: 1.6624767780303955, Lr:0.0001\n",
      "Epoch 35, Step: 74, Loss: 0.3470388352870941, Lr:0.0001\n",
      "Epoch 35, Step: 75, Loss: 0.6040042042732239, Lr:0.0001\n",
      "Epoch 35, Step: 76, Loss: 2.9519259929656982, Lr:0.0001\n",
      "Epoch 35, Step: 77, Loss: 0.474666565656662, Lr:0.0001\n",
      "Epoch 35, Step: 78, Loss: 0.354883074760437, Lr:0.0001\n",
      "Epoch 35, Step: 79, Loss: 0.8477851152420044, Lr:0.0001\n",
      "Epoch 35, Step: 80, Loss: 0.40817973017692566, Lr:0.0001\n",
      "Epoch 35, Step: 81, Loss: 0.1228552907705307, Lr:0.0001\n",
      "Epoch 35, Step: 82, Loss: 0.1054876372218132, Lr:0.0001\n",
      "Epoch 35, Step: 83, Loss: 0.4372568130493164, Lr:0.0001\n",
      "Epoch 35, Step: 84, Loss: 1.057519793510437, Lr:0.0001\n",
      "Epoch 35, Step: 85, Loss: 0.771963357925415, Lr:0.0001\n",
      "Epoch 35, Step: 86, Loss: 1.6467033624649048, Lr:0.0001\n",
      "Epoch 35, Step: 87, Loss: 0.6450898051261902, Lr:0.0001\n",
      "Epoch 35, Step: 88, Loss: 0.587681770324707, Lr:0.0001\n",
      "Epoch 35, Step: 89, Loss: 1.6354750394821167, Lr:0.0001\n",
      "Epoch 35, Step: 90, Loss: 1.4105128049850464, Lr:0.0001\n",
      "Epoch 35, Step: 91, Loss: 1.2680860757827759, Lr:0.0001\n",
      "Epoch 35, Step: 92, Loss: 0.8578537702560425, Lr:0.0001\n",
      "Epoch 35, Step: 93, Loss: 0.5105413794517517, Lr:0.0001\n",
      "Epoch 35, Step: 94, Loss: 1.084657073020935, Lr:0.0001\n",
      "Epoch 35, Step: 95, Loss: 0.8038946390151978, Lr:0.0001\n",
      "Epoch 35, Step: 96, Loss: 1.439954161643982, Lr:0.0001\n",
      "Epoch 35, Step: 97, Loss: 0.9042371511459351, Lr:0.0001\n",
      "Epoch 35, Step: 98, Loss: 0.06633370369672775, Lr:0.0001\n",
      "Epoch 35, Step: 99, Loss: 0.48669227957725525, Lr:0.0001\n",
      "Epoch 35, Step: 100, Loss: 0.402956485748291, Lr:0.0001\n",
      "Epoch 35, Step: 101, Loss: 0.4890648126602173, Lr:0.0001\n",
      "Epoch 35, Step: 102, Loss: 0.292805016040802, Lr:0.0001\n",
      "Epoch 35, Step: 103, Loss: 0.9440467953681946, Lr:0.0001\n",
      "Epoch 35, Step: 104, Loss: 0.10273031890392303, Lr:0.0001\n",
      "Epoch 35, Step: 105, Loss: 1.1566271781921387, Lr:0.0001\n",
      "Epoch 35, Step: 106, Loss: 0.7920436263084412, Lr:0.0001\n",
      "Epoch 35, Step: 107, Loss: 0.8349373936653137, Lr:0.0001\n",
      "Epoch 35, Step: 108, Loss: 0.998650848865509, Lr:0.0001\n",
      "Epoch 35, Step: 109, Loss: 0.4318515956401825, Lr:0.0001\n",
      "Epoch 35, Step: 110, Loss: 1.5995802879333496, Lr:0.0001\n",
      "Epoch 35, Step: 111, Loss: 0.7389178276062012, Lr:0.0001\n",
      "Epoch 35, Step: 112, Loss: 0.5296706557273865, Lr:0.0001\n",
      "Epoch 35, Step: 113, Loss: 0.743492603302002, Lr:0.0001\n",
      "Epoch 35, Step: 114, Loss: 0.3616393804550171, Lr:0.0001\n",
      "Epoch 35, Step: 115, Loss: 0.25921642780303955, Lr:0.0001\n",
      "Epoch 35, Step: 116, Loss: 0.7770376801490784, Lr:0.0001\n",
      "Epoch 35, Step: 117, Loss: 0.1618746519088745, Lr:0.0001\n",
      "Epoch 35, Step: 118, Loss: 1.7406017780303955, Lr:0.0001\n",
      "Epoch 35, Step: 119, Loss: 0.2622984051704407, Lr:0.0001\n",
      "Epoch 35, Step: 120, Loss: 0.029418030753731728, Lr:0.0001\n",
      "Epoch 35, Step: 121, Loss: 0.11687500774860382, Lr:0.0001\n",
      "Epoch 35, Step: 122, Loss: 0.5641372203826904, Lr:0.0001\n",
      "Epoch 35, Step: 123, Loss: 1.0253794193267822, Lr:0.0001\n",
      "Epoch 35, Step: 124, Loss: 0.17523127794265747, Lr:0.0001\n",
      "Epoch 35, Step: 125, Loss: 0.2770923972129822, Lr:0.0001\n",
      "Epoch 35, Step: 126, Loss: 1.3754746913909912, Lr:0.0001\n",
      "Epoch 35, Step: 127, Loss: 1.1137304306030273, Lr:0.0001\n",
      "Epoch 35, Step: 128, Loss: 0.12101160734891891, Lr:0.0001\n",
      "Epoch 35, Step: 129, Loss: 0.4951716363430023, Lr:0.0001\n",
      "Epoch 35, Step: 130, Loss: 0.6224181056022644, Lr:0.0001\n",
      "Epoch 35, Step: 131, Loss: 0.5375083684921265, Lr:0.0001\n",
      "Epoch 35, Step: 132, Loss: 0.48011162877082825, Lr:0.0001\n",
      "Epoch 35, Step: 133, Loss: 0.47719478607177734, Lr:0.0001\n",
      "Epoch 35, Step: 134, Loss: 0.5893031358718872, Lr:0.0001\n",
      "Epoch 35, Step: 135, Loss: 1.13651442527771, Lr:0.0001\n",
      "Epoch 35, Step: 136, Loss: 1.3077729940414429, Lr:0.0001\n",
      "Epoch 35, Step: 137, Loss: 0.771873950958252, Lr:0.0001\n",
      "Epoch 35, Step: 138, Loss: 0.9704814553260803, Lr:0.0001\n",
      "Epoch 35, Step: 139, Loss: 1.2065738439559937, Lr:0.0001\n",
      "Epoch 35, Step: 140, Loss: 0.6524299383163452, Lr:0.0001\n",
      "Epoch 35, Step: 141, Loss: 1.060642123222351, Lr:0.0001\n",
      "Epoch 35, Step: 142, Loss: 0.38654863834381104, Lr:0.0001\n",
      "Epoch 35, Step: 143, Loss: 0.922955334186554, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 35\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0334 (0.0334) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0119 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.2424 (0.7468) acc1: 100.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0093 data: 0.0039 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0093 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.747\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 36, Step: 0, Loss: 0.4668530821800232, Lr:0.0001\n",
      "Epoch 36, Step: 1, Loss: 0.952033281326294, Lr:0.0001\n",
      "Epoch 36, Step: 2, Loss: 0.31466811895370483, Lr:0.0001\n",
      "Epoch 36, Step: 3, Loss: 0.558472752571106, Lr:0.0001\n",
      "Epoch 36, Step: 4, Loss: 1.5807522535324097, Lr:0.0001\n",
      "Epoch 36, Step: 5, Loss: 0.5525768399238586, Lr:0.0001\n",
      "Epoch 36, Step: 6, Loss: 0.3525654971599579, Lr:0.0001\n",
      "Epoch 36, Step: 7, Loss: 0.8406534790992737, Lr:0.0001\n",
      "Epoch 36, Step: 8, Loss: 0.05857294797897339, Lr:0.0001\n",
      "Epoch 36, Step: 9, Loss: 0.38374966382980347, Lr:0.0001\n",
      "Epoch 36, Step: 10, Loss: 0.5969197154045105, Lr:0.0001\n",
      "Epoch 36, Step: 11, Loss: 0.13502946496009827, Lr:0.0001\n",
      "Epoch 36, Step: 12, Loss: 1.3343781232833862, Lr:0.0001\n",
      "Epoch 36, Step: 13, Loss: 0.441606342792511, Lr:0.0001\n",
      "Epoch 36, Step: 14, Loss: 0.03510148823261261, Lr:0.0001\n",
      "Epoch 36, Step: 15, Loss: 0.24100708961486816, Lr:0.0001\n",
      "Epoch 36, Step: 16, Loss: 0.38215431571006775, Lr:0.0001\n",
      "Epoch 36, Step: 17, Loss: 0.4281936287879944, Lr:0.0001\n",
      "Epoch 36, Step: 18, Loss: 0.291762113571167, Lr:0.0001\n",
      "Epoch 36, Step: 19, Loss: 0.34624025225639343, Lr:0.0001\n",
      "Epoch 36, Step: 20, Loss: 0.9394065737724304, Lr:0.0001\n",
      "Epoch 36, Step: 21, Loss: 0.6167258620262146, Lr:0.0001\n",
      "Epoch 36, Step: 22, Loss: 1.4061579704284668, Lr:0.0001\n",
      "Epoch 36, Step: 23, Loss: 0.705146312713623, Lr:0.0001\n",
      "Epoch 36, Step: 24, Loss: 0.6553194522857666, Lr:0.0001\n",
      "Epoch 36, Step: 25, Loss: 0.2224225401878357, Lr:0.0001\n",
      "Epoch 36, Step: 26, Loss: 0.03269859403371811, Lr:0.0001\n",
      "Epoch 36, Step: 27, Loss: 0.784336268901825, Lr:0.0001\n",
      "Epoch 36, Step: 28, Loss: 0.43741828203201294, Lr:0.0001\n",
      "Epoch 36, Step: 29, Loss: 0.5924792289733887, Lr:0.0001\n",
      "Epoch 36, Step: 30, Loss: 0.4462774693965912, Lr:0.0001\n",
      "Epoch 36, Step: 31, Loss: 0.27736982703208923, Lr:0.0001\n",
      "Epoch 36, Step: 32, Loss: 0.4220098555088043, Lr:0.0001\n",
      "Epoch 36, Step: 33, Loss: 0.946990430355072, Lr:0.0001\n",
      "Epoch 36, Step: 34, Loss: 1.3473563194274902, Lr:0.0001\n",
      "Epoch 36, Step: 35, Loss: 1.1413263082504272, Lr:0.0001\n",
      "Epoch 36, Step: 36, Loss: 0.4151260256767273, Lr:0.0001\n",
      "Epoch 36, Step: 37, Loss: 0.7291164398193359, Lr:0.0001\n",
      "Epoch 36, Step: 38, Loss: 0.7762240767478943, Lr:0.0001\n",
      "Epoch 36, Step: 39, Loss: 0.6304501295089722, Lr:0.0001\n",
      "Epoch 36, Step: 40, Loss: 0.7846978306770325, Lr:0.0001\n",
      "Epoch 36, Step: 41, Loss: 0.5456844568252563, Lr:0.0001\n",
      "Epoch 36, Step: 42, Loss: 0.8539063334465027, Lr:0.0001\n",
      "Epoch 36, Step: 43, Loss: 0.35630926489830017, Lr:0.0001\n",
      "Epoch 36, Step: 44, Loss: 1.215517520904541, Lr:0.0001\n",
      "Epoch 36, Step: 45, Loss: 0.5868927240371704, Lr:0.0001\n",
      "Epoch 36, Step: 46, Loss: 1.0514459609985352, Lr:0.0001\n",
      "Epoch 36, Step: 47, Loss: 0.6190674304962158, Lr:0.0001\n",
      "Epoch 36, Step: 48, Loss: 0.15125131607055664, Lr:0.0001\n",
      "Epoch 36, Step: 49, Loss: 0.38490283489227295, Lr:0.0001\n",
      "Epoch 36, Step: 50, Loss: 0.4979025721549988, Lr:0.0001\n",
      "Epoch 36, Step: 51, Loss: 0.5712416768074036, Lr:0.0001\n",
      "Epoch 36, Step: 52, Loss: 0.1239042580127716, Lr:0.0001\n",
      "Epoch 36, Step: 53, Loss: 0.5352325439453125, Lr:0.0001\n",
      "Epoch 36, Step: 54, Loss: 0.412969172000885, Lr:0.0001\n",
      "Epoch 36, Step: 55, Loss: 0.46252211928367615, Lr:0.0001\n",
      "Epoch 36, Step: 56, Loss: 0.5818893909454346, Lr:0.0001\n",
      "Epoch 36, Step: 57, Loss: 0.3206307888031006, Lr:0.0001\n",
      "Epoch 36, Step: 58, Loss: 0.8561652898788452, Lr:0.0001\n",
      "Epoch 36, Step: 59, Loss: 0.41293865442276, Lr:0.0001\n",
      "Epoch 36, Step: 60, Loss: 0.6509941816329956, Lr:0.0001\n",
      "Epoch 36, Step: 61, Loss: 0.02884131856262684, Lr:0.0001\n",
      "Epoch 36, Step: 62, Loss: 0.6111120581626892, Lr:0.0001\n",
      "Epoch 36, Step: 63, Loss: 0.22541017830371857, Lr:0.0001\n",
      "Epoch 36, Step: 64, Loss: 1.3885818719863892, Lr:0.0001\n",
      "Epoch 36, Step: 65, Loss: 0.19723689556121826, Lr:0.0001\n",
      "Epoch 36, Step: 66, Loss: 0.17637887597084045, Lr:0.0001\n",
      "Epoch 36, Step: 67, Loss: 0.7107129096984863, Lr:0.0001\n",
      "Epoch 36, Step: 68, Loss: 1.422408938407898, Lr:0.0001\n",
      "Epoch 36, Step: 69, Loss: 0.6481171250343323, Lr:0.0001\n",
      "Epoch 36, Step: 70, Loss: 0.18116572499275208, Lr:0.0001\n",
      "Epoch 36, Step: 71, Loss: 0.6074153780937195, Lr:0.0001\n",
      "Epoch 36, Step: 72, Loss: 0.3470887839794159, Lr:0.0001\n",
      "Epoch 36, Step: 73, Loss: 0.3873923122882843, Lr:0.0001\n",
      "Epoch 36, Step: 74, Loss: 0.18984079360961914, Lr:0.0001\n",
      "Epoch 36, Step: 75, Loss: 0.5252048373222351, Lr:0.0001\n",
      "Epoch 36, Step: 76, Loss: 0.7844653129577637, Lr:0.0001\n",
      "Epoch 36, Step: 77, Loss: 0.1712537258863449, Lr:0.0001\n",
      "Epoch 36, Step: 78, Loss: 2.1787118911743164, Lr:0.0001\n",
      "Epoch 36, Step: 79, Loss: 0.4201371967792511, Lr:0.0001\n",
      "Epoch 36, Step: 80, Loss: 0.7951011061668396, Lr:0.0001\n",
      "Epoch 36, Step: 81, Loss: 0.5633111000061035, Lr:0.0001\n",
      "Epoch 36, Step: 82, Loss: 1.0254065990447998, Lr:0.0001\n",
      "Epoch 36, Step: 83, Loss: 0.5123003721237183, Lr:0.0001\n",
      "Epoch 36, Step: 84, Loss: 1.433593511581421, Lr:0.0001\n",
      "Epoch 36, Step: 85, Loss: 1.567266821861267, Lr:0.0001\n",
      "Epoch 36, Step: 86, Loss: 0.9202463030815125, Lr:0.0001\n",
      "Epoch 36, Step: 87, Loss: 0.6161669492721558, Lr:0.0001\n",
      "Epoch 36, Step: 88, Loss: 0.30513373017311096, Lr:0.0001\n",
      "Epoch 36, Step: 89, Loss: 0.7500171661376953, Lr:0.0001\n",
      "Epoch 36, Step: 90, Loss: 0.24419841170310974, Lr:0.0001\n",
      "Epoch 36, Step: 91, Loss: 0.20286986231803894, Lr:0.0001\n",
      "Epoch 36, Step: 92, Loss: 0.6403757333755493, Lr:0.0001\n",
      "Epoch 36, Step: 93, Loss: 0.24746553599834442, Lr:0.0001\n",
      "Epoch 36, Step: 94, Loss: 0.38249486684799194, Lr:0.0001\n",
      "Epoch 36, Step: 95, Loss: 0.7296982407569885, Lr:0.0001\n",
      "Epoch 36, Step: 96, Loss: 0.6400198936462402, Lr:0.0001\n",
      "Epoch 36, Step: 97, Loss: 0.20680654048919678, Lr:0.0001\n",
      "Epoch 36, Step: 98, Loss: 1.527445673942566, Lr:0.0001\n",
      "Epoch 36, Step: 99, Loss: 0.06870962679386139, Lr:0.0001\n",
      "Epoch 36, Step: 100, Loss: 0.61679607629776, Lr:0.0001\n",
      "Epoch 36, Step: 101, Loss: 0.3143981099128723, Lr:0.0001\n",
      "Epoch 36, Step: 102, Loss: 1.1260840892791748, Lr:0.0001\n",
      "Epoch 36, Step: 103, Loss: 0.9337206482887268, Lr:0.0001\n",
      "Epoch 36, Step: 104, Loss: 0.4603751301765442, Lr:0.0001\n",
      "Epoch 36, Step: 105, Loss: 0.3616168200969696, Lr:0.0001\n",
      "Epoch 36, Step: 106, Loss: 0.4189840257167816, Lr:0.0001\n",
      "Epoch 36, Step: 107, Loss: 0.5104601979255676, Lr:0.0001\n",
      "Epoch 36, Step: 108, Loss: 0.28989696502685547, Lr:0.0001\n",
      "Epoch 36, Step: 109, Loss: 0.4537266790866852, Lr:0.0001\n",
      "Epoch 36, Step: 110, Loss: 0.55215984582901, Lr:0.0001\n",
      "Epoch 36, Step: 111, Loss: 0.37189507484436035, Lr:0.0001\n",
      "Epoch 36, Step: 112, Loss: 1.0765643119812012, Lr:0.0001\n",
      "Epoch 36, Step: 113, Loss: 0.5023543834686279, Lr:0.0001\n",
      "Epoch 36, Step: 114, Loss: 0.7059561014175415, Lr:0.0001\n",
      "Epoch 36, Step: 115, Loss: 0.12584872543811798, Lr:0.0001\n",
      "Epoch 36, Step: 116, Loss: 0.2886239290237427, Lr:0.0001\n",
      "Epoch 36, Step: 117, Loss: 0.47138845920562744, Lr:0.0001\n",
      "Epoch 36, Step: 118, Loss: 0.4597298800945282, Lr:0.0001\n",
      "Epoch 36, Step: 119, Loss: 0.5880368947982788, Lr:0.0001\n",
      "Epoch 36, Step: 120, Loss: 0.5178966522216797, Lr:0.0001\n",
      "Epoch 36, Step: 121, Loss: 2.256273031234741, Lr:0.0001\n",
      "Epoch 36, Step: 122, Loss: 0.37060850858688354, Lr:0.0001\n",
      "Epoch 36, Step: 123, Loss: 0.40636876225471497, Lr:0.0001\n",
      "Epoch 36, Step: 124, Loss: 1.2156230211257935, Lr:0.0001\n",
      "Epoch 36, Step: 125, Loss: 0.8412672281265259, Lr:0.0001\n",
      "Epoch 36, Step: 126, Loss: 0.8798399567604065, Lr:0.0001\n",
      "Epoch 36, Step: 127, Loss: 0.6111320853233337, Lr:0.0001\n",
      "Epoch 36, Step: 128, Loss: 1.7131215333938599, Lr:0.0001\n",
      "Epoch 36, Step: 129, Loss: 1.0033694505691528, Lr:0.0001\n",
      "Epoch 36, Step: 130, Loss: 0.5476462841033936, Lr:0.0001\n",
      "Epoch 36, Step: 131, Loss: 0.37812384963035583, Lr:0.0001\n",
      "Epoch 36, Step: 132, Loss: 0.4691773056983948, Lr:0.0001\n",
      "Epoch 36, Step: 133, Loss: 0.341494619846344, Lr:0.0001\n",
      "Epoch 36, Step: 134, Loss: 0.6778571009635925, Lr:0.0001\n",
      "Epoch 36, Step: 135, Loss: 0.2823699414730072, Lr:0.0001\n",
      "Epoch 36, Step: 136, Loss: 0.41697102785110474, Lr:0.0001\n",
      "Epoch 36, Step: 137, Loss: 0.4224313497543335, Lr:0.0001\n",
      "Epoch 36, Step: 138, Loss: 0.29927998781204224, Lr:0.0001\n",
      "Epoch 36, Step: 139, Loss: 0.42846646904945374, Lr:0.0001\n",
      "Epoch 36, Step: 140, Loss: 0.13783018290996552, Lr:0.0001\n",
      "Epoch 36, Step: 141, Loss: 0.13723382353782654, Lr:0.0001\n",
      "Epoch 36, Step: 142, Loss: 1.2520607709884644, Lr:0.0001\n",
      "Epoch 36, Step: 143, Loss: 0.808780312538147, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 36\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.2857 (0.2857) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0049 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4104 (0.6987) acc1: 75.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0069 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0070 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.699\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 37, Step: 0, Loss: 0.7054291367530823, Lr:0.0001\n",
      "Epoch 37, Step: 1, Loss: 0.5500108003616333, Lr:0.0001\n",
      "Epoch 37, Step: 2, Loss: 0.8584755659103394, Lr:0.0001\n",
      "Epoch 37, Step: 3, Loss: 0.3534069359302521, Lr:0.0001\n",
      "Epoch 37, Step: 4, Loss: 0.8470698595046997, Lr:0.0001\n",
      "Epoch 37, Step: 5, Loss: 0.40243858098983765, Lr:0.0001\n",
      "Epoch 37, Step: 6, Loss: 0.9181700348854065, Lr:0.0001\n",
      "Epoch 37, Step: 7, Loss: 0.2795858085155487, Lr:0.0001\n",
      "Epoch 37, Step: 8, Loss: 0.5797888040542603, Lr:0.0001\n",
      "Epoch 37, Step: 9, Loss: 0.7834151983261108, Lr:0.0001\n",
      "Epoch 37, Step: 10, Loss: 0.27940285205841064, Lr:0.0001\n",
      "Epoch 37, Step: 11, Loss: 1.1034594774246216, Lr:0.0001\n",
      "Epoch 37, Step: 12, Loss: 0.318022221326828, Lr:0.0001\n",
      "Epoch 37, Step: 13, Loss: 0.5023494362831116, Lr:0.0001\n",
      "Epoch 37, Step: 14, Loss: 0.768520176410675, Lr:0.0001\n",
      "Epoch 37, Step: 15, Loss: 0.25745415687561035, Lr:0.0001\n",
      "Epoch 37, Step: 16, Loss: 0.7201718091964722, Lr:0.0001\n",
      "Epoch 37, Step: 17, Loss: 0.14071689546108246, Lr:0.0001\n",
      "Epoch 37, Step: 18, Loss: 0.04239580035209656, Lr:0.0001\n",
      "Epoch 37, Step: 19, Loss: 0.6194838285446167, Lr:0.0001\n",
      "Epoch 37, Step: 20, Loss: 0.44033586978912354, Lr:0.0001\n",
      "Epoch 37, Step: 21, Loss: 0.4003176689147949, Lr:0.0001\n",
      "Epoch 37, Step: 22, Loss: 1.021214485168457, Lr:0.0001\n",
      "Epoch 37, Step: 23, Loss: 0.7507004737854004, Lr:0.0001\n",
      "Epoch 37, Step: 24, Loss: 0.6335306167602539, Lr:0.0001\n",
      "Epoch 37, Step: 25, Loss: 0.1924118846654892, Lr:0.0001\n",
      "Epoch 37, Step: 26, Loss: 0.5026134252548218, Lr:0.0001\n",
      "Epoch 37, Step: 27, Loss: 0.046561431139707565, Lr:0.0001\n",
      "Epoch 37, Step: 28, Loss: 0.3790585398674011, Lr:0.0001\n",
      "Epoch 37, Step: 29, Loss: 0.7325468063354492, Lr:0.0001\n",
      "Epoch 37, Step: 30, Loss: 0.6950050592422485, Lr:0.0001\n",
      "Epoch 37, Step: 31, Loss: 0.5009999871253967, Lr:0.0001\n",
      "Epoch 37, Step: 32, Loss: 0.4865988492965698, Lr:0.0001\n",
      "Epoch 37, Step: 33, Loss: 0.333905965089798, Lr:0.0001\n",
      "Epoch 37, Step: 34, Loss: 0.8139134049415588, Lr:0.0001\n",
      "Epoch 37, Step: 35, Loss: 0.12935002148151398, Lr:0.0001\n",
      "Epoch 37, Step: 36, Loss: 0.9215953946113586, Lr:0.0001\n",
      "Epoch 37, Step: 37, Loss: 0.4062332212924957, Lr:0.0001\n",
      "Epoch 37, Step: 38, Loss: 0.5812721252441406, Lr:0.0001\n",
      "Epoch 37, Step: 39, Loss: 0.19503623247146606, Lr:0.0001\n",
      "Epoch 37, Step: 40, Loss: 0.39535245299339294, Lr:0.0001\n",
      "Epoch 37, Step: 41, Loss: 0.12505671381950378, Lr:0.0001\n",
      "Epoch 37, Step: 42, Loss: 0.4034442603588104, Lr:0.0001\n",
      "Epoch 37, Step: 43, Loss: 0.28904199600219727, Lr:0.0001\n",
      "Epoch 37, Step: 44, Loss: 0.4735373854637146, Lr:0.0001\n",
      "Epoch 37, Step: 45, Loss: 0.22719621658325195, Lr:0.0001\n",
      "Epoch 37, Step: 46, Loss: 0.33744189143180847, Lr:0.0001\n",
      "Epoch 37, Step: 47, Loss: 0.5327340960502625, Lr:0.0001\n",
      "Epoch 37, Step: 48, Loss: 0.6716650724411011, Lr:0.0001\n",
      "Epoch 37, Step: 49, Loss: 0.26879170536994934, Lr:0.0001\n",
      "Epoch 37, Step: 50, Loss: 0.4653817415237427, Lr:0.0001\n",
      "Epoch 37, Step: 51, Loss: 0.4030173420906067, Lr:0.0001\n",
      "Epoch 37, Step: 52, Loss: 0.36038053035736084, Lr:0.0001\n",
      "Epoch 37, Step: 53, Loss: 0.9663466811180115, Lr:0.0001\n",
      "Epoch 37, Step: 54, Loss: 0.6794316172599792, Lr:0.0001\n",
      "Epoch 37, Step: 55, Loss: 0.07585755735635757, Lr:0.0001\n",
      "Epoch 37, Step: 56, Loss: 0.2017040252685547, Lr:0.0001\n",
      "Epoch 37, Step: 57, Loss: 1.0844755172729492, Lr:0.0001\n",
      "Epoch 37, Step: 58, Loss: 0.5881624817848206, Lr:0.0001\n",
      "Epoch 37, Step: 59, Loss: 0.5066823363304138, Lr:0.0001\n",
      "Epoch 37, Step: 60, Loss: 0.23233827948570251, Lr:0.0001\n",
      "Epoch 37, Step: 61, Loss: 0.8901048898696899, Lr:0.0001\n",
      "Epoch 37, Step: 62, Loss: 0.35168927907943726, Lr:0.0001\n",
      "Epoch 37, Step: 63, Loss: 0.1796693205833435, Lr:0.0001\n",
      "Epoch 37, Step: 64, Loss: 0.7827150225639343, Lr:0.0001\n",
      "Epoch 37, Step: 65, Loss: 1.0564143657684326, Lr:0.0001\n",
      "Epoch 37, Step: 66, Loss: 0.5693528652191162, Lr:0.0001\n",
      "Epoch 37, Step: 67, Loss: 0.09468046575784683, Lr:0.0001\n",
      "Epoch 37, Step: 68, Loss: 0.6929385662078857, Lr:0.0001\n",
      "Epoch 37, Step: 69, Loss: 0.5708901882171631, Lr:0.0001\n",
      "Epoch 37, Step: 70, Loss: 2.1348257064819336, Lr:0.0001\n",
      "Epoch 37, Step: 71, Loss: 1.0351446866989136, Lr:0.0001\n",
      "Epoch 37, Step: 72, Loss: 0.31745994091033936, Lr:0.0001\n",
      "Epoch 37, Step: 73, Loss: 1.0666307210922241, Lr:0.0001\n",
      "Epoch 37, Step: 74, Loss: 0.1119893491268158, Lr:0.0001\n",
      "Epoch 37, Step: 75, Loss: 0.6810952425003052, Lr:0.0001\n",
      "Epoch 37, Step: 76, Loss: 0.704150378704071, Lr:0.0001\n",
      "Epoch 37, Step: 77, Loss: 0.4908382296562195, Lr:0.0001\n",
      "Epoch 37, Step: 78, Loss: 0.3723021149635315, Lr:0.0001\n",
      "Epoch 37, Step: 79, Loss: 0.33237117528915405, Lr:0.0001\n",
      "Epoch 37, Step: 80, Loss: 1.0626213550567627, Lr:0.0001\n",
      "Epoch 37, Step: 81, Loss: 0.9912481307983398, Lr:0.0001\n",
      "Epoch 37, Step: 82, Loss: 0.07719302922487259, Lr:0.0001\n",
      "Epoch 37, Step: 83, Loss: 0.8694334626197815, Lr:0.0001\n",
      "Epoch 37, Step: 84, Loss: 0.46979814767837524, Lr:0.0001\n",
      "Epoch 37, Step: 85, Loss: 0.912949800491333, Lr:0.0001\n",
      "Epoch 37, Step: 86, Loss: 0.07590121775865555, Lr:0.0001\n",
      "Epoch 37, Step: 87, Loss: 0.28883862495422363, Lr:0.0001\n",
      "Epoch 37, Step: 88, Loss: 1.3025426864624023, Lr:0.0001\n",
      "Epoch 37, Step: 89, Loss: 0.7526456117630005, Lr:0.0001\n",
      "Epoch 37, Step: 90, Loss: 0.08896849304437637, Lr:0.0001\n",
      "Epoch 37, Step: 91, Loss: 0.4756549000740051, Lr:0.0001\n",
      "Epoch 37, Step: 92, Loss: 0.8912657499313354, Lr:0.0001\n",
      "Epoch 37, Step: 93, Loss: 0.12593941390514374, Lr:0.0001\n",
      "Epoch 37, Step: 94, Loss: 0.49819985032081604, Lr:0.0001\n",
      "Epoch 37, Step: 95, Loss: 0.08224131911993027, Lr:0.0001\n",
      "Epoch 37, Step: 96, Loss: 0.6170276999473572, Lr:0.0001\n",
      "Epoch 37, Step: 97, Loss: 0.2279784232378006, Lr:0.0001\n",
      "Epoch 37, Step: 98, Loss: 0.8029838800430298, Lr:0.0001\n",
      "Epoch 37, Step: 99, Loss: 0.47249335050582886, Lr:0.0001\n",
      "Epoch 37, Step: 100, Loss: 0.5913761258125305, Lr:0.0001\n",
      "Epoch 37, Step: 101, Loss: 1.0115307569503784, Lr:0.0001\n",
      "Epoch 37, Step: 102, Loss: 0.22296355664730072, Lr:0.0001\n",
      "Epoch 37, Step: 103, Loss: 0.5876795053482056, Lr:0.0001\n",
      "Epoch 37, Step: 104, Loss: 0.3559321165084839, Lr:0.0001\n",
      "Epoch 37, Step: 105, Loss: 0.16323624551296234, Lr:0.0001\n",
      "Epoch 37, Step: 106, Loss: 0.1409798413515091, Lr:0.0001\n",
      "Epoch 37, Step: 107, Loss: 0.9215787649154663, Lr:0.0001\n",
      "Epoch 37, Step: 108, Loss: 1.056069016456604, Lr:0.0001\n",
      "Epoch 37, Step: 109, Loss: 1.9592152833938599, Lr:0.0001\n",
      "Epoch 37, Step: 110, Loss: 0.633957028388977, Lr:0.0001\n",
      "Epoch 37, Step: 111, Loss: 2.191601037979126, Lr:0.0001\n",
      "Epoch 37, Step: 112, Loss: 0.32914191484451294, Lr:0.0001\n",
      "Epoch 37, Step: 113, Loss: 0.6195898056030273, Lr:0.0001\n",
      "Epoch 37, Step: 114, Loss: 0.7176297903060913, Lr:0.0001\n",
      "Epoch 37, Step: 115, Loss: 0.6629612445831299, Lr:0.0001\n",
      "Epoch 37, Step: 116, Loss: 0.2897666096687317, Lr:0.0001\n",
      "Epoch 37, Step: 117, Loss: 0.337375283241272, Lr:0.0001\n",
      "Epoch 37, Step: 118, Loss: 0.4476023316383362, Lr:0.0001\n",
      "Epoch 37, Step: 119, Loss: 0.14034685492515564, Lr:0.0001\n",
      "Epoch 37, Step: 120, Loss: 0.16163387894630432, Lr:0.0001\n",
      "Epoch 37, Step: 121, Loss: 0.48047706484794617, Lr:0.0001\n",
      "Epoch 37, Step: 122, Loss: 0.3232709467411041, Lr:0.0001\n",
      "Epoch 37, Step: 123, Loss: 0.9477339386940002, Lr:0.0001\n",
      "Epoch 37, Step: 124, Loss: 1.183658242225647, Lr:0.0001\n",
      "Epoch 37, Step: 125, Loss: 1.7305697202682495, Lr:0.0001\n",
      "Epoch 37, Step: 126, Loss: 1.0820612907409668, Lr:0.0001\n",
      "Epoch 37, Step: 127, Loss: 0.6982242465019226, Lr:0.0001\n",
      "Epoch 37, Step: 128, Loss: 0.9838945269584656, Lr:0.0001\n",
      "Epoch 37, Step: 129, Loss: 1.6215928792953491, Lr:0.0001\n",
      "Epoch 37, Step: 130, Loss: 0.37579646706581116, Lr:0.0001\n",
      "Epoch 37, Step: 131, Loss: 0.48066824674606323, Lr:0.0001\n",
      "Epoch 37, Step: 132, Loss: 0.5519630908966064, Lr:0.0001\n",
      "Epoch 37, Step: 133, Loss: 0.249881312251091, Lr:0.0001\n",
      "Epoch 37, Step: 134, Loss: 0.6797378063201904, Lr:0.0001\n",
      "Epoch 37, Step: 135, Loss: 0.21293282508850098, Lr:0.0001\n",
      "Epoch 37, Step: 136, Loss: 0.7402657270431519, Lr:0.0001\n",
      "Epoch 37, Step: 137, Loss: 0.16349349915981293, Lr:0.0001\n",
      "Epoch 37, Step: 138, Loss: 0.33577895164489746, Lr:0.0001\n",
      "Epoch 37, Step: 139, Loss: 1.158276915550232, Lr:0.0001\n",
      "Epoch 37, Step: 140, Loss: 0.3170124590396881, Lr:0.0001\n",
      "Epoch 37, Step: 141, Loss: 0.967689037322998, Lr:0.0001\n",
      "Epoch 37, Step: 142, Loss: 1.376707911491394, Lr:0.0001\n",
      "Epoch 37, Step: 143, Loss: 0.09214767813682556, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 37\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0088 (0.0088) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.2057 (0.8914) acc1: 75.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0074 data: 0.0038 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0074 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.891\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 38, Step: 0, Loss: 0.5059452056884766, Lr:0.0001\n",
      "Epoch 38, Step: 1, Loss: 0.6987496018409729, Lr:0.0001\n",
      "Epoch 38, Step: 2, Loss: 0.016795208677649498, Lr:0.0001\n",
      "Epoch 38, Step: 3, Loss: 0.17573267221450806, Lr:0.0001\n",
      "Epoch 38, Step: 4, Loss: 0.6699944734573364, Lr:0.0001\n",
      "Epoch 38, Step: 5, Loss: 1.0040335655212402, Lr:0.0001\n",
      "Epoch 38, Step: 6, Loss: 0.67520672082901, Lr:0.0001\n",
      "Epoch 38, Step: 7, Loss: 0.4982292056083679, Lr:0.0001\n",
      "Epoch 38, Step: 8, Loss: 0.6426336169242859, Lr:0.0001\n",
      "Epoch 38, Step: 9, Loss: 0.642256498336792, Lr:0.0001\n",
      "Epoch 38, Step: 10, Loss: 0.17305496335029602, Lr:0.0001\n",
      "Epoch 38, Step: 11, Loss: 0.12854185700416565, Lr:0.0001\n",
      "Epoch 38, Step: 12, Loss: 1.5600390434265137, Lr:0.0001\n",
      "Epoch 38, Step: 13, Loss: 1.5121281147003174, Lr:0.0001\n",
      "Epoch 38, Step: 14, Loss: 0.4849779009819031, Lr:0.0001\n",
      "Epoch 38, Step: 15, Loss: 0.46349987387657166, Lr:0.0001\n",
      "Epoch 38, Step: 16, Loss: 1.190579891204834, Lr:0.0001\n",
      "Epoch 38, Step: 17, Loss: 0.7007579207420349, Lr:0.0001\n",
      "Epoch 38, Step: 18, Loss: 0.9646652340888977, Lr:0.0001\n",
      "Epoch 38, Step: 19, Loss: 0.3546290993690491, Lr:0.0001\n",
      "Epoch 38, Step: 20, Loss: 0.6641038060188293, Lr:0.0001\n",
      "Epoch 38, Step: 21, Loss: 0.34279465675354004, Lr:0.0001\n",
      "Epoch 38, Step: 22, Loss: 0.8794481158256531, Lr:0.0001\n",
      "Epoch 38, Step: 23, Loss: 0.17707523703575134, Lr:0.0001\n",
      "Epoch 38, Step: 24, Loss: 1.8671972751617432, Lr:0.0001\n",
      "Epoch 38, Step: 25, Loss: 1.9117567539215088, Lr:0.0001\n",
      "Epoch 38, Step: 26, Loss: 0.687950849533081, Lr:0.0001\n",
      "Epoch 38, Step: 27, Loss: 1.0843920707702637, Lr:0.0001\n",
      "Epoch 38, Step: 28, Loss: 1.1565614938735962, Lr:0.0001\n",
      "Epoch 38, Step: 29, Loss: 0.04122484475374222, Lr:0.0001\n",
      "Epoch 38, Step: 30, Loss: 1.3900420665740967, Lr:0.0001\n",
      "Epoch 38, Step: 31, Loss: 0.6757179498672485, Lr:0.0001\n",
      "Epoch 38, Step: 32, Loss: 0.40415048599243164, Lr:0.0001\n",
      "Epoch 38, Step: 33, Loss: 0.6151548027992249, Lr:0.0001\n",
      "Epoch 38, Step: 34, Loss: 0.31863200664520264, Lr:0.0001\n",
      "Epoch 38, Step: 35, Loss: 0.6150482892990112, Lr:0.0001\n",
      "Epoch 38, Step: 36, Loss: 0.06429210305213928, Lr:0.0001\n",
      "Epoch 38, Step: 37, Loss: 1.8171550035476685, Lr:0.0001\n",
      "Epoch 38, Step: 38, Loss: 0.9246821999549866, Lr:0.0001\n",
      "Epoch 38, Step: 39, Loss: 0.9816625118255615, Lr:0.0001\n",
      "Epoch 38, Step: 40, Loss: 0.6392194628715515, Lr:0.0001\n",
      "Epoch 38, Step: 41, Loss: 0.0384024940431118, Lr:0.0001\n",
      "Epoch 38, Step: 42, Loss: 0.9536494016647339, Lr:0.0001\n",
      "Epoch 38, Step: 43, Loss: 0.5280228853225708, Lr:0.0001\n",
      "Epoch 38, Step: 44, Loss: 1.4256739616394043, Lr:0.0001\n",
      "Epoch 38, Step: 45, Loss: 0.6524867415428162, Lr:0.0001\n",
      "Epoch 38, Step: 46, Loss: 1.2176331281661987, Lr:0.0001\n",
      "Epoch 38, Step: 47, Loss: 1.1032075881958008, Lr:0.0001\n",
      "Epoch 38, Step: 48, Loss: 0.03576652705669403, Lr:0.0001\n",
      "Epoch 38, Step: 49, Loss: 0.6485271453857422, Lr:0.0001\n",
      "Epoch 38, Step: 50, Loss: 0.28795337677001953, Lr:0.0001\n",
      "Epoch 38, Step: 51, Loss: 0.7262665033340454, Lr:0.0001\n",
      "Epoch 38, Step: 52, Loss: 0.182210773229599, Lr:0.0001\n",
      "Epoch 38, Step: 53, Loss: 0.6140936017036438, Lr:0.0001\n",
      "Epoch 38, Step: 54, Loss: 2.214057683944702, Lr:0.0001\n",
      "Epoch 38, Step: 55, Loss: 2.0152206420898438, Lr:0.0001\n",
      "Epoch 38, Step: 56, Loss: 0.6320433616638184, Lr:0.0001\n",
      "Epoch 38, Step: 57, Loss: 0.595242977142334, Lr:0.0001\n",
      "Epoch 38, Step: 58, Loss: 0.5069042444229126, Lr:0.0001\n",
      "Epoch 38, Step: 59, Loss: 1.053187370300293, Lr:0.0001\n",
      "Epoch 38, Step: 60, Loss: 0.3773043751716614, Lr:0.0001\n",
      "Epoch 38, Step: 61, Loss: 0.1807541698217392, Lr:0.0001\n",
      "Epoch 38, Step: 62, Loss: 0.3790017366409302, Lr:0.0001\n",
      "Epoch 38, Step: 63, Loss: 0.5158373117446899, Lr:0.0001\n",
      "Epoch 38, Step: 64, Loss: 0.9152132868766785, Lr:0.0001\n",
      "Epoch 38, Step: 65, Loss: 0.4201389253139496, Lr:0.0001\n",
      "Epoch 38, Step: 66, Loss: 0.5958194732666016, Lr:0.0001\n",
      "Epoch 38, Step: 67, Loss: 0.6204565763473511, Lr:0.0001\n",
      "Epoch 38, Step: 68, Loss: 1.0393710136413574, Lr:0.0001\n",
      "Epoch 38, Step: 69, Loss: 0.1372707635164261, Lr:0.0001\n",
      "Epoch 38, Step: 70, Loss: 0.3214355707168579, Lr:0.0001\n",
      "Epoch 38, Step: 71, Loss: 1.0648648738861084, Lr:0.0001\n",
      "Epoch 38, Step: 72, Loss: 0.28031980991363525, Lr:0.0001\n",
      "Epoch 38, Step: 73, Loss: 0.6684094667434692, Lr:0.0001\n",
      "Epoch 38, Step: 74, Loss: 0.5329214334487915, Lr:0.0001\n",
      "Epoch 38, Step: 75, Loss: 0.2690293490886688, Lr:0.0001\n",
      "Epoch 38, Step: 76, Loss: 0.34544891119003296, Lr:0.0001\n",
      "Epoch 38, Step: 77, Loss: 0.6524046659469604, Lr:0.0001\n",
      "Epoch 38, Step: 78, Loss: 0.738846480846405, Lr:0.0001\n",
      "Epoch 38, Step: 79, Loss: 0.29637452960014343, Lr:0.0001\n",
      "Epoch 38, Step: 80, Loss: 0.5682283639907837, Lr:0.0001\n",
      "Epoch 38, Step: 81, Loss: 0.20863750576972961, Lr:0.0001\n",
      "Epoch 38, Step: 82, Loss: 0.6128413677215576, Lr:0.0001\n",
      "Epoch 38, Step: 83, Loss: 0.4312760829925537, Lr:0.0001\n",
      "Epoch 38, Step: 84, Loss: 0.5226582884788513, Lr:0.0001\n",
      "Epoch 38, Step: 85, Loss: 0.36818233132362366, Lr:0.0001\n",
      "Epoch 38, Step: 86, Loss: 0.39324790239334106, Lr:0.0001\n",
      "Epoch 38, Step: 87, Loss: 1.316401720046997, Lr:0.0001\n",
      "Epoch 38, Step: 88, Loss: 0.6354991793632507, Lr:0.0001\n",
      "Epoch 38, Step: 89, Loss: 0.4273693561553955, Lr:0.0001\n",
      "Epoch 38, Step: 90, Loss: 1.137978434562683, Lr:0.0001\n",
      "Epoch 38, Step: 91, Loss: 1.067234754562378, Lr:0.0001\n",
      "Epoch 38, Step: 92, Loss: 0.5287072062492371, Lr:0.0001\n",
      "Epoch 38, Step: 93, Loss: 0.5045928955078125, Lr:0.0001\n",
      "Epoch 38, Step: 94, Loss: 0.09008421003818512, Lr:0.0001\n",
      "Epoch 38, Step: 95, Loss: 2.3637170791625977, Lr:0.0001\n",
      "Epoch 38, Step: 96, Loss: 1.4067201614379883, Lr:0.0001\n",
      "Epoch 38, Step: 97, Loss: 0.11860373616218567, Lr:0.0001\n",
      "Epoch 38, Step: 98, Loss: 1.1914970874786377, Lr:0.0001\n",
      "Epoch 38, Step: 99, Loss: 0.45082080364227295, Lr:0.0001\n",
      "Epoch 38, Step: 100, Loss: 1.114973545074463, Lr:0.0001\n",
      "Epoch 38, Step: 101, Loss: 0.13197815418243408, Lr:0.0001\n",
      "Epoch 38, Step: 102, Loss: 1.306294560432434, Lr:0.0001\n",
      "Epoch 38, Step: 103, Loss: 0.6660453677177429, Lr:0.0001\n",
      "Epoch 38, Step: 104, Loss: 0.22685615718364716, Lr:0.0001\n",
      "Epoch 38, Step: 105, Loss: 1.522871971130371, Lr:0.0001\n",
      "Epoch 38, Step: 106, Loss: 0.5420387387275696, Lr:0.0001\n",
      "Epoch 38, Step: 107, Loss: 0.8607311844825745, Lr:0.0001\n",
      "Epoch 38, Step: 108, Loss: 0.5837301015853882, Lr:0.0001\n",
      "Epoch 38, Step: 109, Loss: 0.7619665265083313, Lr:0.0001\n",
      "Epoch 38, Step: 110, Loss: 0.15083874762058258, Lr:0.0001\n",
      "Epoch 38, Step: 111, Loss: 0.032515935599803925, Lr:0.0001\n",
      "Epoch 38, Step: 112, Loss: 0.8942265510559082, Lr:0.0001\n",
      "Epoch 38, Step: 113, Loss: 0.7093754410743713, Lr:0.0001\n",
      "Epoch 38, Step: 114, Loss: 0.3222522735595703, Lr:0.0001\n",
      "Epoch 38, Step: 115, Loss: 0.31580787897109985, Lr:0.0001\n",
      "Epoch 38, Step: 116, Loss: 0.6675072908401489, Lr:0.0001\n",
      "Epoch 38, Step: 117, Loss: 0.5545702576637268, Lr:0.0001\n",
      "Epoch 38, Step: 118, Loss: 2.929226875305176, Lr:0.0001\n",
      "Epoch 38, Step: 119, Loss: 0.6723406314849854, Lr:0.0001\n",
      "Epoch 38, Step: 120, Loss: 0.3424447774887085, Lr:0.0001\n",
      "Epoch 38, Step: 121, Loss: 0.7526243925094604, Lr:0.0001\n",
      "Epoch 38, Step: 122, Loss: 0.9024924039840698, Lr:0.0001\n",
      "Epoch 38, Step: 123, Loss: 0.5333713293075562, Lr:0.0001\n",
      "Epoch 38, Step: 124, Loss: 0.291485458612442, Lr:0.0001\n",
      "Epoch 38, Step: 125, Loss: 0.17436639964580536, Lr:0.0001\n",
      "Epoch 38, Step: 126, Loss: 0.2633502781391144, Lr:0.0001\n",
      "Epoch 38, Step: 127, Loss: 0.8412895798683167, Lr:0.0001\n",
      "Epoch 38, Step: 128, Loss: 0.03417211025953293, Lr:0.0001\n",
      "Epoch 38, Step: 129, Loss: 0.1683535873889923, Lr:0.0001\n",
      "Epoch 38, Step: 130, Loss: 0.0724877268075943, Lr:0.0001\n",
      "Epoch 38, Step: 131, Loss: 0.24919381737709045, Lr:0.0001\n",
      "Epoch 38, Step: 132, Loss: 0.5433874726295471, Lr:0.0001\n",
      "Epoch 38, Step: 133, Loss: 0.3261191248893738, Lr:0.0001\n",
      "Epoch 38, Step: 134, Loss: 1.0073097944259644, Lr:0.0001\n",
      "Epoch 38, Step: 135, Loss: 0.03404657170176506, Lr:0.0001\n",
      "Epoch 38, Step: 136, Loss: 0.9647938013076782, Lr:0.0001\n",
      "Epoch 38, Step: 137, Loss: 1.107064127922058, Lr:0.0001\n",
      "Epoch 38, Step: 138, Loss: 0.5362449884414673, Lr:0.0001\n",
      "Epoch 38, Step: 139, Loss: 0.15078113973140717, Lr:0.0001\n",
      "Epoch 38, Step: 140, Loss: 0.4166604280471802, Lr:0.0001\n",
      "Epoch 38, Step: 141, Loss: 0.6502300500869751, Lr:0.0001\n",
      "Epoch 38, Step: 142, Loss: 0.4136372208595276, Lr:0.0001\n",
      "Epoch 38, Step: 143, Loss: 0.4603016972541809, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 38\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0307 (0.0307) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0111 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.8686 (0.6852) acc1: 75.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0087 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0088 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.685\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 39, Step: 0, Loss: 0.6952107548713684, Lr:0.0001\n",
      "Epoch 39, Step: 1, Loss: 0.8760215044021606, Lr:0.0001\n",
      "Epoch 39, Step: 2, Loss: 0.18652024865150452, Lr:0.0001\n",
      "Epoch 39, Step: 3, Loss: 0.6912280917167664, Lr:0.0001\n",
      "Epoch 39, Step: 4, Loss: 0.06403211504220963, Lr:0.0001\n",
      "Epoch 39, Step: 5, Loss: 0.7237685918807983, Lr:0.0001\n",
      "Epoch 39, Step: 6, Loss: 0.41718846559524536, Lr:0.0001\n",
      "Epoch 39, Step: 7, Loss: 0.2856563925743103, Lr:0.0001\n",
      "Epoch 39, Step: 8, Loss: 0.5333825349807739, Lr:0.0001\n",
      "Epoch 39, Step: 9, Loss: 0.2522621750831604, Lr:0.0001\n",
      "Epoch 39, Step: 10, Loss: 0.8302584886550903, Lr:0.0001\n",
      "Epoch 39, Step: 11, Loss: 1.0485999584197998, Lr:0.0001\n",
      "Epoch 39, Step: 12, Loss: 0.21717020869255066, Lr:0.0001\n",
      "Epoch 39, Step: 13, Loss: 0.49578380584716797, Lr:0.0001\n",
      "Epoch 39, Step: 14, Loss: 0.744702935218811, Lr:0.0001\n",
      "Epoch 39, Step: 15, Loss: 1.6934045553207397, Lr:0.0001\n",
      "Epoch 39, Step: 16, Loss: 0.5516713261604309, Lr:0.0001\n",
      "Epoch 39, Step: 17, Loss: 0.9917445182800293, Lr:0.0001\n",
      "Epoch 39, Step: 18, Loss: 0.6383002400398254, Lr:0.0001\n",
      "Epoch 39, Step: 19, Loss: 0.4017549753189087, Lr:0.0001\n",
      "Epoch 39, Step: 20, Loss: 0.3431060314178467, Lr:0.0001\n",
      "Epoch 39, Step: 21, Loss: 0.6685695052146912, Lr:0.0001\n",
      "Epoch 39, Step: 22, Loss: 0.3579027056694031, Lr:0.0001\n",
      "Epoch 39, Step: 23, Loss: 0.45461422204971313, Lr:0.0001\n",
      "Epoch 39, Step: 24, Loss: 0.5840007066726685, Lr:0.0001\n",
      "Epoch 39, Step: 25, Loss: 0.8042300939559937, Lr:0.0001\n",
      "Epoch 39, Step: 26, Loss: 1.5845167636871338, Lr:0.0001\n",
      "Epoch 39, Step: 27, Loss: 0.5908668041229248, Lr:0.0001\n",
      "Epoch 39, Step: 28, Loss: 0.3772392272949219, Lr:0.0001\n",
      "Epoch 39, Step: 29, Loss: 0.024159466847777367, Lr:0.0001\n",
      "Epoch 39, Step: 30, Loss: 0.08099806308746338, Lr:0.0001\n",
      "Epoch 39, Step: 31, Loss: 0.013485307805240154, Lr:0.0001\n",
      "Epoch 39, Step: 32, Loss: 0.43373632431030273, Lr:0.0001\n",
      "Epoch 39, Step: 33, Loss: 0.1569834202528, Lr:0.0001\n",
      "Epoch 39, Step: 34, Loss: 0.1400292068719864, Lr:0.0001\n",
      "Epoch 39, Step: 35, Loss: 0.22471599280834198, Lr:0.0001\n",
      "Epoch 39, Step: 36, Loss: 0.1002885103225708, Lr:0.0001\n",
      "Epoch 39, Step: 37, Loss: 0.05423197150230408, Lr:0.0001\n",
      "Epoch 39, Step: 38, Loss: 0.8611540198326111, Lr:0.0001\n",
      "Epoch 39, Step: 39, Loss: 0.7534784078598022, Lr:0.0001\n",
      "Epoch 39, Step: 40, Loss: 0.39046037197113037, Lr:0.0001\n",
      "Epoch 39, Step: 41, Loss: 0.6070463061332703, Lr:0.0001\n",
      "Epoch 39, Step: 42, Loss: 0.845612645149231, Lr:0.0001\n",
      "Epoch 39, Step: 43, Loss: 0.5843389630317688, Lr:0.0001\n",
      "Epoch 39, Step: 44, Loss: 1.355518102645874, Lr:0.0001\n",
      "Epoch 39, Step: 45, Loss: 0.5355834364891052, Lr:0.0001\n",
      "Epoch 39, Step: 46, Loss: 1.3892203569412231, Lr:0.0001\n",
      "Epoch 39, Step: 47, Loss: 0.6886926889419556, Lr:0.0001\n",
      "Epoch 39, Step: 48, Loss: 0.35563743114471436, Lr:0.0001\n",
      "Epoch 39, Step: 49, Loss: 0.624759316444397, Lr:0.0001\n",
      "Epoch 39, Step: 50, Loss: 1.1521704196929932, Lr:0.0001\n",
      "Epoch 39, Step: 51, Loss: 0.26089543104171753, Lr:0.0001\n",
      "Epoch 39, Step: 52, Loss: 0.5807931423187256, Lr:0.0001\n",
      "Epoch 39, Step: 53, Loss: 0.9108119010925293, Lr:0.0001\n",
      "Epoch 39, Step: 54, Loss: 0.9171833992004395, Lr:0.0001\n",
      "Epoch 39, Step: 55, Loss: 0.5757119059562683, Lr:0.0001\n",
      "Epoch 39, Step: 56, Loss: 0.662295937538147, Lr:0.0001\n",
      "Epoch 39, Step: 57, Loss: 0.18372716009616852, Lr:0.0001\n",
      "Epoch 39, Step: 58, Loss: 0.11454665660858154, Lr:0.0001\n",
      "Epoch 39, Step: 59, Loss: 0.37233689427375793, Lr:0.0001\n",
      "Epoch 39, Step: 60, Loss: 0.029902763664722443, Lr:0.0001\n",
      "Epoch 39, Step: 61, Loss: 1.4536718130111694, Lr:0.0001\n",
      "Epoch 39, Step: 62, Loss: 0.6548147797584534, Lr:0.0001\n",
      "Epoch 39, Step: 63, Loss: 0.9207324385643005, Lr:0.0001\n",
      "Epoch 39, Step: 64, Loss: 0.5652731657028198, Lr:0.0001\n",
      "Epoch 39, Step: 65, Loss: 0.854427695274353, Lr:0.0001\n",
      "Epoch 39, Step: 66, Loss: 0.2768933176994324, Lr:0.0001\n",
      "Epoch 39, Step: 67, Loss: 0.611865222454071, Lr:0.0001\n",
      "Epoch 39, Step: 68, Loss: 0.021598409861326218, Lr:0.0001\n",
      "Epoch 39, Step: 69, Loss: 0.48911282420158386, Lr:0.0001\n",
      "Epoch 39, Step: 70, Loss: 0.5124111175537109, Lr:0.0001\n",
      "Epoch 39, Step: 71, Loss: 1.7736722230911255, Lr:0.0001\n",
      "Epoch 39, Step: 72, Loss: 0.8976231813430786, Lr:0.0001\n",
      "Epoch 39, Step: 73, Loss: 0.7203921675682068, Lr:0.0001\n",
      "Epoch 39, Step: 74, Loss: 0.8802832365036011, Lr:0.0001\n",
      "Epoch 39, Step: 75, Loss: 1.1233270168304443, Lr:0.0001\n",
      "Epoch 39, Step: 76, Loss: 1.0990110635757446, Lr:0.0001\n",
      "Epoch 39, Step: 77, Loss: 0.339916467666626, Lr:0.0001\n",
      "Epoch 39, Step: 78, Loss: 0.7828672528266907, Lr:0.0001\n",
      "Epoch 39, Step: 79, Loss: 0.3037477135658264, Lr:0.0001\n",
      "Epoch 39, Step: 80, Loss: 0.27606630325317383, Lr:0.0001\n",
      "Epoch 39, Step: 81, Loss: 2.1841912269592285, Lr:0.0001\n",
      "Epoch 39, Step: 82, Loss: 1.488898754119873, Lr:0.0001\n",
      "Epoch 39, Step: 83, Loss: 0.3168538212776184, Lr:0.0001\n",
      "Epoch 39, Step: 84, Loss: 0.499007910490036, Lr:0.0001\n",
      "Epoch 39, Step: 85, Loss: 0.5616921186447144, Lr:0.0001\n",
      "Epoch 39, Step: 86, Loss: 1.0350817441940308, Lr:0.0001\n",
      "Epoch 39, Step: 87, Loss: 0.7795702219009399, Lr:0.0001\n",
      "Epoch 39, Step: 88, Loss: 0.6079605221748352, Lr:0.0001\n",
      "Epoch 39, Step: 89, Loss: 0.5479511618614197, Lr:0.0001\n",
      "Epoch 39, Step: 90, Loss: 0.17582984268665314, Lr:0.0001\n",
      "Epoch 39, Step: 91, Loss: 0.21486592292785645, Lr:0.0001\n",
      "Epoch 39, Step: 92, Loss: 0.7791276574134827, Lr:0.0001\n",
      "Epoch 39, Step: 93, Loss: 0.4211481213569641, Lr:0.0001\n",
      "Epoch 39, Step: 94, Loss: 1.4337588548660278, Lr:0.0001\n",
      "Epoch 39, Step: 95, Loss: 0.21692845225334167, Lr:0.0001\n",
      "Epoch 39, Step: 96, Loss: 0.38057950139045715, Lr:0.0001\n",
      "Epoch 39, Step: 97, Loss: 4.716647148132324, Lr:0.0001\n",
      "Epoch 39, Step: 98, Loss: 0.4671984314918518, Lr:0.0001\n",
      "Epoch 39, Step: 99, Loss: 0.6404332518577576, Lr:0.0001\n",
      "Epoch 39, Step: 100, Loss: 1.4169347286224365, Lr:0.0001\n",
      "Epoch 39, Step: 101, Loss: 0.9877512454986572, Lr:0.0001\n",
      "Epoch 39, Step: 102, Loss: 0.5102505683898926, Lr:0.0001\n",
      "Epoch 39, Step: 103, Loss: 1.1249606609344482, Lr:0.0001\n",
      "Epoch 39, Step: 104, Loss: 0.719591498374939, Lr:0.0001\n",
      "Epoch 39, Step: 105, Loss: 0.0573357529938221, Lr:0.0001\n",
      "Epoch 39, Step: 106, Loss: 0.5272257328033447, Lr:0.0001\n",
      "Epoch 39, Step: 107, Loss: 0.3243084251880646, Lr:0.0001\n",
      "Epoch 39, Step: 108, Loss: 0.912246823310852, Lr:0.0001\n",
      "Epoch 39, Step: 109, Loss: 0.5527775287628174, Lr:0.0001\n",
      "Epoch 39, Step: 110, Loss: 0.8216997981071472, Lr:0.0001\n",
      "Epoch 39, Step: 111, Loss: 0.20161429047584534, Lr:0.0001\n",
      "Epoch 39, Step: 112, Loss: 0.2889493703842163, Lr:0.0001\n",
      "Epoch 39, Step: 113, Loss: 0.4845050871372223, Lr:0.0001\n",
      "Epoch 39, Step: 114, Loss: 1.0191298723220825, Lr:0.0001\n",
      "Epoch 39, Step: 115, Loss: 0.6419408917427063, Lr:0.0001\n",
      "Epoch 39, Step: 116, Loss: 0.1368052363395691, Lr:0.0001\n",
      "Epoch 39, Step: 117, Loss: 0.6062166690826416, Lr:0.0001\n",
      "Epoch 39, Step: 118, Loss: 0.7736433744430542, Lr:0.0001\n",
      "Epoch 39, Step: 119, Loss: 0.3958517909049988, Lr:0.0001\n",
      "Epoch 39, Step: 120, Loss: 0.3616504669189453, Lr:0.0001\n",
      "Epoch 39, Step: 121, Loss: 0.6563918590545654, Lr:0.0001\n",
      "Epoch 39, Step: 122, Loss: 0.6142545938491821, Lr:0.0001\n",
      "Epoch 39, Step: 123, Loss: 0.6701682209968567, Lr:0.0001\n",
      "Epoch 39, Step: 124, Loss: 0.2588057816028595, Lr:0.0001\n",
      "Epoch 39, Step: 125, Loss: 0.046065155416727066, Lr:0.0001\n",
      "Epoch 39, Step: 126, Loss: 0.7987298965454102, Lr:0.0001\n",
      "Epoch 39, Step: 127, Loss: 0.5723079442977905, Lr:0.0001\n",
      "Epoch 39, Step: 128, Loss: 1.4635969400405884, Lr:0.0001\n",
      "Epoch 39, Step: 129, Loss: 0.777342677116394, Lr:0.0001\n",
      "Epoch 39, Step: 130, Loss: 0.8610122203826904, Lr:0.0001\n",
      "Epoch 39, Step: 131, Loss: 0.46767133474349976, Lr:0.0001\n",
      "Epoch 39, Step: 132, Loss: 0.3737243711948395, Lr:0.0001\n",
      "Epoch 39, Step: 133, Loss: 0.4849022328853607, Lr:0.0001\n",
      "Epoch 39, Step: 134, Loss: 0.8148441314697266, Lr:0.0001\n",
      "Epoch 39, Step: 135, Loss: 0.2702580690383911, Lr:0.0001\n",
      "Epoch 39, Step: 136, Loss: 0.3443893492221832, Lr:0.0001\n",
      "Epoch 39, Step: 137, Loss: 0.774104654788971, Lr:0.0001\n",
      "Epoch 39, Step: 138, Loss: 0.3081046938896179, Lr:0.0001\n",
      "Epoch 39, Step: 139, Loss: 0.4046279788017273, Lr:0.0001\n",
      "Epoch 39, Step: 140, Loss: 0.4746299088001251, Lr:0.0001\n",
      "Epoch 39, Step: 141, Loss: 0.3766288459300995, Lr:0.0001\n",
      "Epoch 39, Step: 142, Loss: 1.1739994287490845, Lr:0.0001\n",
      "Epoch 39, Step: 143, Loss: 0.9986438155174255, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 39\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0047 (0.0047) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5258 (0.6583) acc1: 75.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0068 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0068 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.658\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 40, Step: 0, Loss: 0.6764253973960876, Lr:0.0001\n",
      "Epoch 40, Step: 1, Loss: 0.30207422375679016, Lr:0.0001\n",
      "Epoch 40, Step: 2, Loss: 0.9615496397018433, Lr:0.0001\n",
      "Epoch 40, Step: 3, Loss: 0.8283305168151855, Lr:0.0001\n",
      "Epoch 40, Step: 4, Loss: 0.7994265556335449, Lr:0.0001\n",
      "Epoch 40, Step: 5, Loss: 0.8932123184204102, Lr:0.0001\n",
      "Epoch 40, Step: 6, Loss: 1.367032766342163, Lr:0.0001\n",
      "Epoch 40, Step: 7, Loss: 0.5161226987838745, Lr:0.0001\n",
      "Epoch 40, Step: 8, Loss: 1.3702504634857178, Lr:0.0001\n",
      "Epoch 40, Step: 9, Loss: 0.012404827401041985, Lr:0.0001\n",
      "Epoch 40, Step: 10, Loss: 0.12938889861106873, Lr:0.0001\n",
      "Epoch 40, Step: 11, Loss: 0.3714889883995056, Lr:0.0001\n",
      "Epoch 40, Step: 12, Loss: 0.8678393363952637, Lr:0.0001\n",
      "Epoch 40, Step: 13, Loss: 0.814145565032959, Lr:0.0001\n",
      "Epoch 40, Step: 14, Loss: 0.36483991146087646, Lr:0.0001\n",
      "Epoch 40, Step: 15, Loss: 0.7673816680908203, Lr:0.0001\n",
      "Epoch 40, Step: 16, Loss: 0.5473840236663818, Lr:0.0001\n",
      "Epoch 40, Step: 17, Loss: 0.7023465633392334, Lr:0.0001\n",
      "Epoch 40, Step: 18, Loss: 1.1519381999969482, Lr:0.0001\n",
      "Epoch 40, Step: 19, Loss: 0.29433318972587585, Lr:0.0001\n",
      "Epoch 40, Step: 20, Loss: 0.8595021963119507, Lr:0.0001\n",
      "Epoch 40, Step: 21, Loss: 0.48707008361816406, Lr:0.0001\n",
      "Epoch 40, Step: 22, Loss: 0.4182012677192688, Lr:0.0001\n",
      "Epoch 40, Step: 23, Loss: 0.46359148621559143, Lr:0.0001\n",
      "Epoch 40, Step: 24, Loss: 0.360870897769928, Lr:0.0001\n",
      "Epoch 40, Step: 25, Loss: 0.4667964279651642, Lr:0.0001\n",
      "Epoch 40, Step: 26, Loss: 0.9259160161018372, Lr:0.0001\n",
      "Epoch 40, Step: 27, Loss: 0.3609793782234192, Lr:0.0001\n",
      "Epoch 40, Step: 28, Loss: 1.465342402458191, Lr:0.0001\n",
      "Epoch 40, Step: 29, Loss: 0.557468593120575, Lr:0.0001\n",
      "Epoch 40, Step: 30, Loss: 0.5698801875114441, Lr:0.0001\n",
      "Epoch 40, Step: 31, Loss: 0.969660758972168, Lr:0.0001\n",
      "Epoch 40, Step: 32, Loss: 1.0798163414001465, Lr:0.0001\n",
      "Epoch 40, Step: 33, Loss: 0.6010996103286743, Lr:0.0001\n",
      "Epoch 40, Step: 34, Loss: 0.6460115909576416, Lr:0.0001\n",
      "Epoch 40, Step: 35, Loss: 0.6570838689804077, Lr:0.0001\n",
      "Epoch 40, Step: 36, Loss: 0.508424699306488, Lr:0.0001\n",
      "Epoch 40, Step: 37, Loss: 1.3290404081344604, Lr:0.0001\n",
      "Epoch 40, Step: 38, Loss: 0.41922664642333984, Lr:0.0001\n",
      "Epoch 40, Step: 39, Loss: 1.0241084098815918, Lr:0.0001\n",
      "Epoch 40, Step: 40, Loss: 0.8428393602371216, Lr:0.0001\n",
      "Epoch 40, Step: 41, Loss: 0.20106102526187897, Lr:0.0001\n",
      "Epoch 40, Step: 42, Loss: 1.2968213558197021, Lr:0.0001\n",
      "Epoch 40, Step: 43, Loss: 0.7984603047370911, Lr:0.0001\n",
      "Epoch 40, Step: 44, Loss: 0.35371050238609314, Lr:0.0001\n",
      "Epoch 40, Step: 45, Loss: 0.23785455524921417, Lr:0.0001\n",
      "Epoch 40, Step: 46, Loss: 0.08017568290233612, Lr:0.0001\n",
      "Epoch 40, Step: 47, Loss: 0.41280385851860046, Lr:0.0001\n",
      "Epoch 40, Step: 48, Loss: 0.2406848669052124, Lr:0.0001\n",
      "Epoch 40, Step: 49, Loss: 1.6452100276947021, Lr:0.0001\n",
      "Epoch 40, Step: 50, Loss: 0.3457324504852295, Lr:0.0001\n",
      "Epoch 40, Step: 51, Loss: 0.7071191072463989, Lr:0.0001\n",
      "Epoch 40, Step: 52, Loss: 0.6269030570983887, Lr:0.0001\n",
      "Epoch 40, Step: 53, Loss: 1.0376509428024292, Lr:0.0001\n",
      "Epoch 40, Step: 54, Loss: 0.5556333661079407, Lr:0.0001\n",
      "Epoch 40, Step: 55, Loss: 0.16581837832927704, Lr:0.0001\n",
      "Epoch 40, Step: 56, Loss: 1.1583918333053589, Lr:0.0001\n",
      "Epoch 40, Step: 57, Loss: 1.1866044998168945, Lr:0.0001\n",
      "Epoch 40, Step: 58, Loss: 0.7872523665428162, Lr:0.0001\n",
      "Epoch 40, Step: 59, Loss: 1.8461130857467651, Lr:0.0001\n",
      "Epoch 40, Step: 60, Loss: 0.40900856256484985, Lr:0.0001\n",
      "Epoch 40, Step: 61, Loss: 0.49066948890686035, Lr:0.0001\n",
      "Epoch 40, Step: 62, Loss: 0.9997039437294006, Lr:0.0001\n",
      "Epoch 40, Step: 63, Loss: 0.7077925205230713, Lr:0.0001\n",
      "Epoch 40, Step: 64, Loss: 0.7078565955162048, Lr:0.0001\n",
      "Epoch 40, Step: 65, Loss: 0.9813253283500671, Lr:0.0001\n",
      "Epoch 40, Step: 66, Loss: 0.6541680097579956, Lr:0.0001\n",
      "Epoch 40, Step: 67, Loss: 0.2828947901725769, Lr:0.0001\n",
      "Epoch 40, Step: 68, Loss: 0.6566148996353149, Lr:0.0001\n",
      "Epoch 40, Step: 69, Loss: 0.4596558213233948, Lr:0.0001\n",
      "Epoch 40, Step: 70, Loss: 0.2926740348339081, Lr:0.0001\n",
      "Epoch 40, Step: 71, Loss: 0.4422168731689453, Lr:0.0001\n",
      "Epoch 40, Step: 72, Loss: 0.9259892702102661, Lr:0.0001\n",
      "Epoch 40, Step: 73, Loss: 1.2291107177734375, Lr:0.0001\n",
      "Epoch 40, Step: 74, Loss: 0.2747741937637329, Lr:0.0001\n",
      "Epoch 40, Step: 75, Loss: 0.06722869724035263, Lr:0.0001\n",
      "Epoch 40, Step: 76, Loss: 0.683537483215332, Lr:0.0001\n",
      "Epoch 40, Step: 77, Loss: 0.5483026504516602, Lr:0.0001\n",
      "Epoch 40, Step: 78, Loss: 0.8915692567825317, Lr:0.0001\n",
      "Epoch 40, Step: 79, Loss: 0.38718268275260925, Lr:0.0001\n",
      "Epoch 40, Step: 80, Loss: 0.4860926866531372, Lr:0.0001\n",
      "Epoch 40, Step: 81, Loss: 0.4623570740222931, Lr:0.0001\n",
      "Epoch 40, Step: 82, Loss: 0.5958590507507324, Lr:0.0001\n",
      "Epoch 40, Step: 83, Loss: 1.1236224174499512, Lr:0.0001\n",
      "Epoch 40, Step: 84, Loss: 0.3447618782520294, Lr:0.0001\n",
      "Epoch 40, Step: 85, Loss: 0.3009660840034485, Lr:0.0001\n",
      "Epoch 40, Step: 86, Loss: 1.0956083536148071, Lr:0.0001\n",
      "Epoch 40, Step: 87, Loss: 0.8607710003852844, Lr:0.0001\n",
      "Epoch 40, Step: 88, Loss: 0.59974205493927, Lr:0.0001\n",
      "Epoch 40, Step: 89, Loss: 0.44923290610313416, Lr:0.0001\n",
      "Epoch 40, Step: 90, Loss: 0.31291353702545166, Lr:0.0001\n",
      "Epoch 40, Step: 91, Loss: 0.5764238834381104, Lr:0.0001\n",
      "Epoch 40, Step: 92, Loss: 0.07017982006072998, Lr:0.0001\n",
      "Epoch 40, Step: 93, Loss: 0.6256835460662842, Lr:0.0001\n",
      "Epoch 40, Step: 94, Loss: 0.028238609433174133, Lr:0.0001\n",
      "Epoch 40, Step: 95, Loss: 0.9402181506156921, Lr:0.0001\n",
      "Epoch 40, Step: 96, Loss: 1.2348462343215942, Lr:0.0001\n",
      "Epoch 40, Step: 97, Loss: 0.36295512318611145, Lr:0.0001\n",
      "Epoch 40, Step: 98, Loss: 0.7386013269424438, Lr:0.0001\n",
      "Epoch 40, Step: 99, Loss: 0.09871388971805573, Lr:0.0001\n",
      "Epoch 40, Step: 100, Loss: 0.241983100771904, Lr:0.0001\n",
      "Epoch 40, Step: 101, Loss: 1.1422537565231323, Lr:0.0001\n",
      "Epoch 40, Step: 102, Loss: 0.43364423513412476, Lr:0.0001\n",
      "Epoch 40, Step: 103, Loss: 0.2527512013912201, Lr:0.0001\n",
      "Epoch 40, Step: 104, Loss: 0.6163088083267212, Lr:0.0001\n",
      "Epoch 40, Step: 105, Loss: 1.1625092029571533, Lr:0.0001\n",
      "Epoch 40, Step: 106, Loss: 2.007296562194824, Lr:0.0001\n",
      "Epoch 40, Step: 107, Loss: 0.4904681444168091, Lr:0.0001\n",
      "Epoch 40, Step: 108, Loss: 0.41941988468170166, Lr:0.0001\n",
      "Epoch 40, Step: 109, Loss: 0.9570308923721313, Lr:0.0001\n",
      "Epoch 40, Step: 110, Loss: 0.558937668800354, Lr:0.0001\n",
      "Epoch 40, Step: 111, Loss: 0.30098503828048706, Lr:0.0001\n",
      "Epoch 40, Step: 112, Loss: 1.1848299503326416, Lr:0.0001\n",
      "Epoch 40, Step: 113, Loss: 0.2622358500957489, Lr:0.0001\n",
      "Epoch 40, Step: 114, Loss: 0.9394968748092651, Lr:0.0001\n",
      "Epoch 40, Step: 115, Loss: 0.4059493839740753, Lr:0.0001\n",
      "Epoch 40, Step: 116, Loss: 0.5468012690544128, Lr:0.0001\n",
      "Epoch 40, Step: 117, Loss: 0.39018216729164124, Lr:0.0001\n",
      "Epoch 40, Step: 118, Loss: 0.3978528678417206, Lr:0.0001\n",
      "Epoch 40, Step: 119, Loss: 1.4480516910552979, Lr:0.0001\n",
      "Epoch 40, Step: 120, Loss: 1.080496072769165, Lr:0.0001\n",
      "Epoch 40, Step: 121, Loss: 0.3089449405670166, Lr:0.0001\n",
      "Epoch 40, Step: 122, Loss: 0.3905842900276184, Lr:0.0001\n",
      "Epoch 40, Step: 123, Loss: 0.19950991868972778, Lr:0.0001\n",
      "Epoch 40, Step: 124, Loss: 0.12748193740844727, Lr:0.0001\n",
      "Epoch 40, Step: 125, Loss: 0.7673303484916687, Lr:0.0001\n",
      "Epoch 40, Step: 126, Loss: 1.3163784742355347, Lr:0.0001\n",
      "Epoch 40, Step: 127, Loss: 1.6377818584442139, Lr:0.0001\n",
      "Epoch 40, Step: 128, Loss: 0.4790031313896179, Lr:0.0001\n",
      "Epoch 40, Step: 129, Loss: 0.21148230135440826, Lr:0.0001\n",
      "Epoch 40, Step: 130, Loss: 0.22894415259361267, Lr:0.0001\n",
      "Epoch 40, Step: 131, Loss: 0.6263631582260132, Lr:0.0001\n",
      "Epoch 40, Step: 132, Loss: 0.9180015921592712, Lr:0.0001\n",
      "Epoch 40, Step: 133, Loss: 0.3496430218219757, Lr:0.0001\n",
      "Epoch 40, Step: 134, Loss: 0.20178034901618958, Lr:0.0001\n",
      "Epoch 40, Step: 135, Loss: 0.46415460109710693, Lr:0.0001\n",
      "Epoch 40, Step: 136, Loss: 0.270662784576416, Lr:0.0001\n",
      "Epoch 40, Step: 137, Loss: 1.1718099117279053, Lr:0.0001\n",
      "Epoch 40, Step: 138, Loss: 0.41430041193962097, Lr:0.0001\n",
      "Epoch 40, Step: 139, Loss: 0.23483474552631378, Lr:0.0001\n",
      "Epoch 40, Step: 140, Loss: 0.9559330940246582, Lr:0.0001\n",
      "Epoch 40, Step: 141, Loss: 0.29438838362693787, Lr:0.0001\n",
      "Epoch 40, Step: 142, Loss: 1.1144760847091675, Lr:0.0001\n",
      "Epoch 40, Step: 143, Loss: 0.2595042586326599, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 40\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0611 (0.0611) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.6844 (0.5738) acc1: 100.0000 (75.7576) acc5: 100.0000 (100.0000) time: 0.0067 data: 0.0033 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0069 s / it)\n",
      "* Acc@1 75.758 Acc@5 100.000 loss 0.574\n",
      "Accuracy of the network on the 33 test image: 75.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 41, Step: 0, Loss: 0.594047486782074, Lr:0.0001\n",
      "Epoch 41, Step: 1, Loss: 1.2923595905303955, Lr:0.0001\n",
      "Epoch 41, Step: 2, Loss: 0.31699836254119873, Lr:0.0001\n",
      "Epoch 41, Step: 3, Loss: 1.0213547945022583, Lr:0.0001\n",
      "Epoch 41, Step: 4, Loss: 0.48241564631462097, Lr:0.0001\n",
      "Epoch 41, Step: 5, Loss: 0.6156708002090454, Lr:0.0001\n",
      "Epoch 41, Step: 6, Loss: 0.12840743362903595, Lr:0.0001\n",
      "Epoch 41, Step: 7, Loss: 0.10018658638000488, Lr:0.0001\n",
      "Epoch 41, Step: 8, Loss: 0.8209575414657593, Lr:0.0001\n",
      "Epoch 41, Step: 9, Loss: 0.860197126865387, Lr:0.0001\n",
      "Epoch 41, Step: 10, Loss: 0.2746228277683258, Lr:0.0001\n",
      "Epoch 41, Step: 11, Loss: 0.6643044948577881, Lr:0.0001\n",
      "Epoch 41, Step: 12, Loss: 0.20964601635932922, Lr:0.0001\n",
      "Epoch 41, Step: 13, Loss: 0.7358505129814148, Lr:0.0001\n",
      "Epoch 41, Step: 14, Loss: 1.1474119424819946, Lr:0.0001\n",
      "Epoch 41, Step: 15, Loss: 1.0561665296554565, Lr:0.0001\n",
      "Epoch 41, Step: 16, Loss: 0.7570772767066956, Lr:0.0001\n",
      "Epoch 41, Step: 17, Loss: 0.8551338315010071, Lr:0.0001\n",
      "Epoch 41, Step: 18, Loss: 0.05453299731016159, Lr:0.0001\n",
      "Epoch 41, Step: 19, Loss: 0.11770036071538925, Lr:0.0001\n",
      "Epoch 41, Step: 20, Loss: 1.7713465690612793, Lr:0.0001\n",
      "Epoch 41, Step: 21, Loss: 0.6759939789772034, Lr:0.0001\n",
      "Epoch 41, Step: 22, Loss: 0.5588904023170471, Lr:0.0001\n",
      "Epoch 41, Step: 23, Loss: 0.5161657929420471, Lr:0.0001\n",
      "Epoch 41, Step: 24, Loss: 0.11439836025238037, Lr:0.0001\n",
      "Epoch 41, Step: 25, Loss: 1.0295096635818481, Lr:0.0001\n",
      "Epoch 41, Step: 26, Loss: 0.4263118803501129, Lr:0.0001\n",
      "Epoch 41, Step: 27, Loss: 0.4648470878601074, Lr:0.0001\n",
      "Epoch 41, Step: 28, Loss: 0.9515361189842224, Lr:0.0001\n",
      "Epoch 41, Step: 29, Loss: 0.4429495334625244, Lr:0.0001\n",
      "Epoch 41, Step: 30, Loss: 1.0729365348815918, Lr:0.0001\n",
      "Epoch 41, Step: 31, Loss: 0.6677973866462708, Lr:0.0001\n",
      "Epoch 41, Step: 32, Loss: 0.5107498168945312, Lr:0.0001\n",
      "Epoch 41, Step: 33, Loss: 0.48553186655044556, Lr:0.0001\n",
      "Epoch 41, Step: 34, Loss: 0.5727702379226685, Lr:0.0001\n",
      "Epoch 41, Step: 35, Loss: 0.5112948417663574, Lr:0.0001\n",
      "Epoch 41, Step: 36, Loss: 0.8651744723320007, Lr:0.0001\n",
      "Epoch 41, Step: 37, Loss: 1.1054444313049316, Lr:0.0001\n",
      "Epoch 41, Step: 38, Loss: 0.5368713140487671, Lr:0.0001\n",
      "Epoch 41, Step: 39, Loss: 0.24566437304019928, Lr:0.0001\n",
      "Epoch 41, Step: 40, Loss: 0.2987738251686096, Lr:0.0001\n",
      "Epoch 41, Step: 41, Loss: 0.370498925447464, Lr:0.0001\n",
      "Epoch 41, Step: 42, Loss: 0.3420080244541168, Lr:0.0001\n",
      "Epoch 41, Step: 43, Loss: 0.7405184507369995, Lr:0.0001\n",
      "Epoch 41, Step: 44, Loss: 0.7693241834640503, Lr:0.0001\n",
      "Epoch 41, Step: 45, Loss: 0.39685481786727905, Lr:0.0001\n",
      "Epoch 41, Step: 46, Loss: 0.4894248843193054, Lr:0.0001\n",
      "Epoch 41, Step: 47, Loss: 0.4535030424594879, Lr:0.0001\n",
      "Epoch 41, Step: 48, Loss: 0.2521195411682129, Lr:0.0001\n",
      "Epoch 41, Step: 49, Loss: 0.625635027885437, Lr:0.0001\n",
      "Epoch 41, Step: 50, Loss: 0.40450194478034973, Lr:0.0001\n",
      "Epoch 41, Step: 51, Loss: 0.2619387209415436, Lr:0.0001\n",
      "Epoch 41, Step: 52, Loss: 1.875953197479248, Lr:0.0001\n",
      "Epoch 41, Step: 53, Loss: 1.6634321212768555, Lr:0.0001\n",
      "Epoch 41, Step: 54, Loss: 0.544277548789978, Lr:0.0001\n",
      "Epoch 41, Step: 55, Loss: 0.5200439095497131, Lr:0.0001\n",
      "Epoch 41, Step: 56, Loss: 0.4701838493347168, Lr:0.0001\n",
      "Epoch 41, Step: 57, Loss: 0.12687817215919495, Lr:0.0001\n",
      "Epoch 41, Step: 58, Loss: 0.3845665752887726, Lr:0.0001\n",
      "Epoch 41, Step: 59, Loss: 1.231634497642517, Lr:0.0001\n",
      "Epoch 41, Step: 60, Loss: 0.7150611877441406, Lr:0.0001\n",
      "Epoch 41, Step: 61, Loss: 0.6881880760192871, Lr:0.0001\n",
      "Epoch 41, Step: 62, Loss: 0.43140876293182373, Lr:0.0001\n",
      "Epoch 41, Step: 63, Loss: 1.0687673091888428, Lr:0.0001\n",
      "Epoch 41, Step: 64, Loss: 0.9777519106864929, Lr:0.0001\n",
      "Epoch 41, Step: 65, Loss: 0.7067338228225708, Lr:0.0001\n",
      "Epoch 41, Step: 66, Loss: 0.6829763650894165, Lr:0.0001\n",
      "Epoch 41, Step: 67, Loss: 1.6336801052093506, Lr:0.0001\n",
      "Epoch 41, Step: 68, Loss: 0.5195686221122742, Lr:0.0001\n",
      "Epoch 41, Step: 69, Loss: 0.37870973348617554, Lr:0.0001\n",
      "Epoch 41, Step: 70, Loss: 0.24354656040668488, Lr:0.0001\n",
      "Epoch 41, Step: 71, Loss: 1.0184749364852905, Lr:0.0001\n",
      "Epoch 41, Step: 72, Loss: 0.5662575960159302, Lr:0.0001\n",
      "Epoch 41, Step: 73, Loss: 0.6192312240600586, Lr:0.0001\n",
      "Epoch 41, Step: 74, Loss: 0.9435628056526184, Lr:0.0001\n",
      "Epoch 41, Step: 75, Loss: 0.3836425840854645, Lr:0.0001\n",
      "Epoch 41, Step: 76, Loss: 0.30592080950737, Lr:0.0001\n",
      "Epoch 41, Step: 77, Loss: 0.8469750285148621, Lr:0.0001\n",
      "Epoch 41, Step: 78, Loss: 1.1419819593429565, Lr:0.0001\n",
      "Epoch 41, Step: 79, Loss: 0.9638460874557495, Lr:0.0001\n",
      "Epoch 41, Step: 80, Loss: 0.639885425567627, Lr:0.0001\n",
      "Epoch 41, Step: 81, Loss: 0.9921820163726807, Lr:0.0001\n",
      "Epoch 41, Step: 82, Loss: 0.8709416389465332, Lr:0.0001\n",
      "Epoch 41, Step: 83, Loss: 1.359384298324585, Lr:0.0001\n",
      "Epoch 41, Step: 84, Loss: 0.3564956784248352, Lr:0.0001\n",
      "Epoch 41, Step: 85, Loss: 0.4075848460197449, Lr:0.0001\n",
      "Epoch 41, Step: 86, Loss: 0.3317594826221466, Lr:0.0001\n",
      "Epoch 41, Step: 87, Loss: 0.34988293051719666, Lr:0.0001\n",
      "Epoch 41, Step: 88, Loss: 0.23930998146533966, Lr:0.0001\n",
      "Epoch 41, Step: 89, Loss: 0.6268714666366577, Lr:0.0001\n",
      "Epoch 41, Step: 90, Loss: 0.9199774265289307, Lr:0.0001\n",
      "Epoch 41, Step: 91, Loss: 0.1410793513059616, Lr:0.0001\n",
      "Epoch 41, Step: 92, Loss: 0.8855952620506287, Lr:0.0001\n",
      "Epoch 41, Step: 93, Loss: 0.05171578377485275, Lr:0.0001\n",
      "Epoch 41, Step: 94, Loss: 0.38443422317504883, Lr:0.0001\n",
      "Epoch 41, Step: 95, Loss: 1.6224656105041504, Lr:0.0001\n",
      "Epoch 41, Step: 96, Loss: 0.3751804530620575, Lr:0.0001\n",
      "Epoch 41, Step: 97, Loss: 0.2456103414297104, Lr:0.0001\n",
      "Epoch 41, Step: 98, Loss: 0.21275055408477783, Lr:0.0001\n",
      "Epoch 41, Step: 99, Loss: 0.7733084559440613, Lr:0.0001\n",
      "Epoch 41, Step: 100, Loss: 0.26748231053352356, Lr:0.0001\n",
      "Epoch 41, Step: 101, Loss: 0.5108199715614319, Lr:0.0001\n",
      "Epoch 41, Step: 102, Loss: 0.5023975968360901, Lr:0.0001\n",
      "Epoch 41, Step: 103, Loss: 0.9903736114501953, Lr:0.0001\n",
      "Epoch 41, Step: 104, Loss: 1.5630481243133545, Lr:0.0001\n",
      "Epoch 41, Step: 105, Loss: 1.2511670589447021, Lr:0.0001\n",
      "Epoch 41, Step: 106, Loss: 0.3339695930480957, Lr:0.0001\n",
      "Epoch 41, Step: 107, Loss: 1.3757268190383911, Lr:0.0001\n",
      "Epoch 41, Step: 108, Loss: 0.22123904526233673, Lr:0.0001\n",
      "Epoch 41, Step: 109, Loss: 1.532202124595642, Lr:0.0001\n",
      "Epoch 41, Step: 110, Loss: 0.8304561376571655, Lr:0.0001\n",
      "Epoch 41, Step: 111, Loss: 0.2769937813282013, Lr:0.0001\n",
      "Epoch 41, Step: 112, Loss: 1.965454339981079, Lr:0.0001\n",
      "Epoch 41, Step: 113, Loss: 0.34654754400253296, Lr:0.0001\n",
      "Epoch 41, Step: 114, Loss: 0.6342334151268005, Lr:0.0001\n",
      "Epoch 41, Step: 115, Loss: 0.9774037003517151, Lr:0.0001\n",
      "Epoch 41, Step: 116, Loss: 0.5634004473686218, Lr:0.0001\n",
      "Epoch 41, Step: 117, Loss: 0.6441560387611389, Lr:0.0001\n",
      "Epoch 41, Step: 118, Loss: 0.37029626965522766, Lr:0.0001\n",
      "Epoch 41, Step: 119, Loss: 1.2233524322509766, Lr:0.0001\n",
      "Epoch 41, Step: 120, Loss: 0.6230709552764893, Lr:0.0001\n",
      "Epoch 41, Step: 121, Loss: 0.1480097621679306, Lr:0.0001\n",
      "Epoch 41, Step: 122, Loss: 1.143628478050232, Lr:0.0001\n",
      "Epoch 41, Step: 123, Loss: 0.4021393656730652, Lr:0.0001\n",
      "Epoch 41, Step: 124, Loss: 0.21764960885047913, Lr:0.0001\n",
      "Epoch 41, Step: 125, Loss: 0.10594485700130463, Lr:0.0001\n",
      "Epoch 41, Step: 126, Loss: 0.8789615035057068, Lr:0.0001\n",
      "Epoch 41, Step: 127, Loss: 1.7419942617416382, Lr:0.0001\n",
      "Epoch 41, Step: 128, Loss: 1.7580516338348389, Lr:0.0001\n",
      "Epoch 41, Step: 129, Loss: 0.21556617319583893, Lr:0.0001\n",
      "Epoch 41, Step: 130, Loss: 0.5824606418609619, Lr:0.0001\n",
      "Epoch 41, Step: 131, Loss: 0.9688901305198669, Lr:0.0001\n",
      "Epoch 41, Step: 132, Loss: 0.47582995891571045, Lr:0.0001\n",
      "Epoch 41, Step: 133, Loss: 0.026237064972519875, Lr:0.0001\n",
      "Epoch 41, Step: 134, Loss: 0.6484199166297913, Lr:0.0001\n",
      "Epoch 41, Step: 135, Loss: 0.4119529128074646, Lr:0.0001\n",
      "Epoch 41, Step: 136, Loss: 0.4072685241699219, Lr:0.0001\n",
      "Epoch 41, Step: 137, Loss: 0.5535388588905334, Lr:0.0001\n",
      "Epoch 41, Step: 138, Loss: 0.6162503957748413, Lr:0.0001\n",
      "Epoch 41, Step: 139, Loss: 0.5705663561820984, Lr:0.0001\n",
      "Epoch 41, Step: 140, Loss: 0.9019395709037781, Lr:0.0001\n",
      "Epoch 41, Step: 141, Loss: 0.25232410430908203, Lr:0.0001\n",
      "Epoch 41, Step: 142, Loss: 0.7770563364028931, Lr:0.0001\n",
      "Epoch 41, Step: 143, Loss: 1.2379803657531738, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 41\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0525 (0.0525) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0109 data: 0.0049 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.2743 (0.9359) acc1: 100.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0079 data: 0.0034 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0080 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.936\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 42, Step: 0, Loss: 0.6756275296211243, Lr:0.0001\n",
      "Epoch 42, Step: 1, Loss: 0.3788704574108124, Lr:0.0001\n",
      "Epoch 42, Step: 2, Loss: 0.4873287081718445, Lr:0.0001\n",
      "Epoch 42, Step: 3, Loss: 0.9482782483100891, Lr:0.0001\n",
      "Epoch 42, Step: 4, Loss: 0.6542344093322754, Lr:0.0001\n",
      "Epoch 42, Step: 5, Loss: 0.6250652074813843, Lr:0.0001\n",
      "Epoch 42, Step: 6, Loss: 0.039040710777044296, Lr:0.0001\n",
      "Epoch 42, Step: 7, Loss: 0.4900571405887604, Lr:0.0001\n",
      "Epoch 42, Step: 8, Loss: 0.8096114993095398, Lr:0.0001\n",
      "Epoch 42, Step: 9, Loss: 0.5443055033683777, Lr:0.0001\n",
      "Epoch 42, Step: 10, Loss: 1.6021382808685303, Lr:0.0001\n",
      "Epoch 42, Step: 11, Loss: 0.7859411835670471, Lr:0.0001\n",
      "Epoch 42, Step: 12, Loss: 1.4267024993896484, Lr:0.0001\n",
      "Epoch 42, Step: 13, Loss: 0.5213451981544495, Lr:0.0001\n",
      "Epoch 42, Step: 14, Loss: 0.3616145849227905, Lr:0.0001\n",
      "Epoch 42, Step: 15, Loss: 0.17793554067611694, Lr:0.0001\n",
      "Epoch 42, Step: 16, Loss: 0.47915875911712646, Lr:0.0001\n",
      "Epoch 42, Step: 17, Loss: 0.06850551813840866, Lr:0.0001\n",
      "Epoch 42, Step: 18, Loss: 0.4357728958129883, Lr:0.0001\n",
      "Epoch 42, Step: 19, Loss: 0.42653292417526245, Lr:0.0001\n",
      "Epoch 42, Step: 20, Loss: 0.7977007627487183, Lr:0.0001\n",
      "Epoch 42, Step: 21, Loss: 0.4597281813621521, Lr:0.0001\n",
      "Epoch 42, Step: 22, Loss: 0.7922799587249756, Lr:0.0001\n",
      "Epoch 42, Step: 23, Loss: 0.4795394241809845, Lr:0.0001\n",
      "Epoch 42, Step: 24, Loss: 0.2280152142047882, Lr:0.0001\n",
      "Epoch 42, Step: 25, Loss: 0.40437865257263184, Lr:0.0001\n",
      "Epoch 42, Step: 26, Loss: 0.1799628734588623, Lr:0.0001\n",
      "Epoch 42, Step: 27, Loss: 0.8901565670967102, Lr:0.0001\n",
      "Epoch 42, Step: 28, Loss: 0.43653878569602966, Lr:0.0001\n",
      "Epoch 42, Step: 29, Loss: 1.1769347190856934, Lr:0.0001\n",
      "Epoch 42, Step: 30, Loss: 0.2763002812862396, Lr:0.0001\n",
      "Epoch 42, Step: 31, Loss: 0.600187361240387, Lr:0.0001\n",
      "Epoch 42, Step: 32, Loss: 0.5286449193954468, Lr:0.0001\n",
      "Epoch 42, Step: 33, Loss: 1.196224570274353, Lr:0.0001\n",
      "Epoch 42, Step: 34, Loss: 1.2609282732009888, Lr:0.0001\n",
      "Epoch 42, Step: 35, Loss: 0.886012852191925, Lr:0.0001\n",
      "Epoch 42, Step: 36, Loss: 0.5662914514541626, Lr:0.0001\n",
      "Epoch 42, Step: 37, Loss: 0.27136340737342834, Lr:0.0001\n",
      "Epoch 42, Step: 38, Loss: 0.6606806516647339, Lr:0.0001\n",
      "Epoch 42, Step: 39, Loss: 0.29011374711990356, Lr:0.0001\n",
      "Epoch 42, Step: 40, Loss: 0.3362668752670288, Lr:0.0001\n",
      "Epoch 42, Step: 41, Loss: 0.8701150417327881, Lr:0.0001\n",
      "Epoch 42, Step: 42, Loss: 1.229088544845581, Lr:0.0001\n",
      "Epoch 42, Step: 43, Loss: 0.8270846605300903, Lr:0.0001\n",
      "Epoch 42, Step: 44, Loss: 0.44547349214553833, Lr:0.0001\n",
      "Epoch 42, Step: 45, Loss: 0.31046465039253235, Lr:0.0001\n",
      "Epoch 42, Step: 46, Loss: 0.9750791192054749, Lr:0.0001\n",
      "Epoch 42, Step: 47, Loss: 0.9769713878631592, Lr:0.0001\n",
      "Epoch 42, Step: 48, Loss: 0.31487420201301575, Lr:0.0001\n",
      "Epoch 42, Step: 49, Loss: 0.2563003599643707, Lr:0.0001\n",
      "Epoch 42, Step: 50, Loss: 1.2667607069015503, Lr:0.0001\n",
      "Epoch 42, Step: 51, Loss: 0.6415069103240967, Lr:0.0001\n",
      "Epoch 42, Step: 52, Loss: 0.7047849297523499, Lr:0.0001\n",
      "Epoch 42, Step: 53, Loss: 0.21911053359508514, Lr:0.0001\n",
      "Epoch 42, Step: 54, Loss: 0.22425134479999542, Lr:0.0001\n",
      "Epoch 42, Step: 55, Loss: 0.6612356901168823, Lr:0.0001\n",
      "Epoch 42, Step: 56, Loss: 0.21137088537216187, Lr:0.0001\n",
      "Epoch 42, Step: 57, Loss: 0.39250707626342773, Lr:0.0001\n",
      "Epoch 42, Step: 58, Loss: 2.8629324436187744, Lr:0.0001\n",
      "Epoch 42, Step: 59, Loss: 0.9716958403587341, Lr:0.0001\n",
      "Epoch 42, Step: 60, Loss: 0.9071915745735168, Lr:0.0001\n",
      "Epoch 42, Step: 61, Loss: 0.3597746193408966, Lr:0.0001\n",
      "Epoch 42, Step: 62, Loss: 0.3127657473087311, Lr:0.0001\n",
      "Epoch 42, Step: 63, Loss: 0.3748325705528259, Lr:0.0001\n",
      "Epoch 42, Step: 64, Loss: 0.9434686899185181, Lr:0.0001\n",
      "Epoch 42, Step: 65, Loss: 0.6464614272117615, Lr:0.0001\n",
      "Epoch 42, Step: 66, Loss: 0.5550503730773926, Lr:0.0001\n",
      "Epoch 42, Step: 67, Loss: 0.5141559839248657, Lr:0.0001\n",
      "Epoch 42, Step: 68, Loss: 0.5198262333869934, Lr:0.0001\n",
      "Epoch 42, Step: 69, Loss: 0.4085105061531067, Lr:0.0001\n",
      "Epoch 42, Step: 70, Loss: 0.29864275455474854, Lr:0.0001\n",
      "Epoch 42, Step: 71, Loss: 1.0643967390060425, Lr:0.0001\n",
      "Epoch 42, Step: 72, Loss: 0.47464707493782043, Lr:0.0001\n",
      "Epoch 42, Step: 73, Loss: 0.7996846437454224, Lr:0.0001\n",
      "Epoch 42, Step: 74, Loss: 0.5770673751831055, Lr:0.0001\n",
      "Epoch 42, Step: 75, Loss: 0.20989349484443665, Lr:0.0001\n",
      "Epoch 42, Step: 76, Loss: 0.2549905478954315, Lr:0.0001\n",
      "Epoch 42, Step: 77, Loss: 0.839628279209137, Lr:0.0001\n",
      "Epoch 42, Step: 78, Loss: 0.45952266454696655, Lr:0.0001\n",
      "Epoch 42, Step: 79, Loss: 0.8977165818214417, Lr:0.0001\n",
      "Epoch 42, Step: 80, Loss: 0.626885175704956, Lr:0.0001\n",
      "Epoch 42, Step: 81, Loss: 0.2698851227760315, Lr:0.0001\n",
      "Epoch 42, Step: 82, Loss: 0.14538542926311493, Lr:0.0001\n",
      "Epoch 42, Step: 83, Loss: 0.5864383578300476, Lr:0.0001\n",
      "Epoch 42, Step: 84, Loss: 0.39859142899513245, Lr:0.0001\n",
      "Epoch 42, Step: 85, Loss: 0.016263199970126152, Lr:0.0001\n",
      "Epoch 42, Step: 86, Loss: 0.11617053300142288, Lr:0.0001\n",
      "Epoch 42, Step: 87, Loss: 0.07554648071527481, Lr:0.0001\n",
      "Epoch 42, Step: 88, Loss: 0.264814555644989, Lr:0.0001\n",
      "Epoch 42, Step: 89, Loss: 0.3712667226791382, Lr:0.0001\n",
      "Epoch 42, Step: 90, Loss: 0.7104566693305969, Lr:0.0001\n",
      "Epoch 42, Step: 91, Loss: 0.011732995510101318, Lr:0.0001\n",
      "Epoch 42, Step: 92, Loss: 0.09116562455892563, Lr:0.0001\n",
      "Epoch 42, Step: 93, Loss: 1.0900940895080566, Lr:0.0001\n",
      "Epoch 42, Step: 94, Loss: 0.5892956256866455, Lr:0.0001\n",
      "Epoch 42, Step: 95, Loss: 0.35974547266960144, Lr:0.0001\n",
      "Epoch 42, Step: 96, Loss: 0.18655073642730713, Lr:0.0001\n",
      "Epoch 42, Step: 97, Loss: 0.1473357379436493, Lr:0.0001\n",
      "Epoch 42, Step: 98, Loss: 0.031335826963186264, Lr:0.0001\n",
      "Epoch 42, Step: 99, Loss: 0.06072842329740524, Lr:0.0001\n",
      "Epoch 42, Step: 100, Loss: 0.8700196146965027, Lr:0.0001\n",
      "Epoch 42, Step: 101, Loss: 0.05921586602926254, Lr:0.0001\n",
      "Epoch 42, Step: 102, Loss: 0.7498511672019958, Lr:0.0001\n",
      "Epoch 42, Step: 103, Loss: 1.2071537971496582, Lr:0.0001\n",
      "Epoch 42, Step: 104, Loss: 1.968416690826416, Lr:0.0001\n",
      "Epoch 42, Step: 105, Loss: 1.08871591091156, Lr:0.0001\n",
      "Epoch 42, Step: 106, Loss: 0.3764547109603882, Lr:0.0001\n",
      "Epoch 42, Step: 107, Loss: 0.6800119876861572, Lr:0.0001\n",
      "Epoch 42, Step: 108, Loss: 0.22818365693092346, Lr:0.0001\n",
      "Epoch 42, Step: 109, Loss: 0.469850093126297, Lr:0.0001\n",
      "Epoch 42, Step: 110, Loss: 1.2542202472686768, Lr:0.0001\n",
      "Epoch 42, Step: 111, Loss: 0.7106513381004333, Lr:0.0001\n",
      "Epoch 42, Step: 112, Loss: 0.3379680812358856, Lr:0.0001\n",
      "Epoch 42, Step: 113, Loss: 0.1274362951517105, Lr:0.0001\n",
      "Epoch 42, Step: 114, Loss: 0.4713398218154907, Lr:0.0001\n",
      "Epoch 42, Step: 115, Loss: 0.4874827265739441, Lr:0.0001\n",
      "Epoch 42, Step: 116, Loss: 0.5883938670158386, Lr:0.0001\n",
      "Epoch 42, Step: 117, Loss: 0.5955542922019958, Lr:0.0001\n",
      "Epoch 42, Step: 118, Loss: 0.4258892834186554, Lr:0.0001\n",
      "Epoch 42, Step: 119, Loss: 1.3735078573226929, Lr:0.0001\n",
      "Epoch 42, Step: 120, Loss: 1.0583624839782715, Lr:0.0001\n",
      "Epoch 42, Step: 121, Loss: 0.20540350675582886, Lr:0.0001\n",
      "Epoch 42, Step: 122, Loss: 0.07023108750581741, Lr:0.0001\n",
      "Epoch 42, Step: 123, Loss: 0.12427792698144913, Lr:0.0001\n",
      "Epoch 42, Step: 124, Loss: 1.0019185543060303, Lr:0.0001\n",
      "Epoch 42, Step: 125, Loss: 0.8960763812065125, Lr:0.0001\n",
      "Epoch 42, Step: 126, Loss: 0.28497472405433655, Lr:0.0001\n",
      "Epoch 42, Step: 127, Loss: 0.1920996904373169, Lr:0.0001\n",
      "Epoch 42, Step: 128, Loss: 0.7365819811820984, Lr:0.0001\n",
      "Epoch 42, Step: 129, Loss: 0.37712740898132324, Lr:0.0001\n",
      "Epoch 42, Step: 130, Loss: 0.0405353382229805, Lr:0.0001\n",
      "Epoch 42, Step: 131, Loss: 0.8624675273895264, Lr:0.0001\n",
      "Epoch 42, Step: 132, Loss: 0.2651858627796173, Lr:0.0001\n",
      "Epoch 42, Step: 133, Loss: 0.71043461561203, Lr:0.0001\n",
      "Epoch 42, Step: 134, Loss: 0.9834868907928467, Lr:0.0001\n",
      "Epoch 42, Step: 135, Loss: 1.3015778064727783, Lr:0.0001\n",
      "Epoch 42, Step: 136, Loss: 0.9048338532447815, Lr:0.0001\n",
      "Epoch 42, Step: 137, Loss: 0.4625818729400635, Lr:0.0001\n",
      "Epoch 42, Step: 138, Loss: 1.1818007230758667, Lr:0.0001\n",
      "Epoch 42, Step: 139, Loss: 0.6135421991348267, Lr:0.0001\n",
      "Epoch 42, Step: 140, Loss: 0.5443390607833862, Lr:0.0001\n",
      "Epoch 42, Step: 141, Loss: 0.47984078526496887, Lr:0.0001\n",
      "Epoch 42, Step: 142, Loss: 0.09065413475036621, Lr:0.0001\n",
      "Epoch 42, Step: 143, Loss: 0.7659292221069336, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 42\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.2109 (0.2109) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.5254 (0.5882) acc1: 100.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0092 data: 0.0039 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0092 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.588\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 43, Step: 0, Loss: 0.2902737557888031, Lr:0.0001\n",
      "Epoch 43, Step: 1, Loss: 0.35990044474601746, Lr:0.0001\n",
      "Epoch 43, Step: 2, Loss: 0.6089664697647095, Lr:0.0001\n",
      "Epoch 43, Step: 3, Loss: 0.9658275246620178, Lr:0.0001\n",
      "Epoch 43, Step: 4, Loss: 0.5030984878540039, Lr:0.0001\n",
      "Epoch 43, Step: 5, Loss: 0.11179184913635254, Lr:0.0001\n",
      "Epoch 43, Step: 6, Loss: 0.2921105623245239, Lr:0.0001\n",
      "Epoch 43, Step: 7, Loss: 0.5192371010780334, Lr:0.0001\n",
      "Epoch 43, Step: 8, Loss: 0.3598490059375763, Lr:0.0001\n",
      "Epoch 43, Step: 9, Loss: 0.538026750087738, Lr:0.0001\n",
      "Epoch 43, Step: 10, Loss: 1.402459979057312, Lr:0.0001\n",
      "Epoch 43, Step: 11, Loss: 0.9750453233718872, Lr:0.0001\n",
      "Epoch 43, Step: 12, Loss: 0.7815728187561035, Lr:0.0001\n",
      "Epoch 43, Step: 13, Loss: 1.4571698904037476, Lr:0.0001\n",
      "Epoch 43, Step: 14, Loss: 1.0454270839691162, Lr:0.0001\n",
      "Epoch 43, Step: 15, Loss: 0.4254506230354309, Lr:0.0001\n",
      "Epoch 43, Step: 16, Loss: 0.7033737897872925, Lr:0.0001\n",
      "Epoch 43, Step: 17, Loss: 0.43023180961608887, Lr:0.0001\n",
      "Epoch 43, Step: 18, Loss: 0.16210448741912842, Lr:0.0001\n",
      "Epoch 43, Step: 19, Loss: 1.1438064575195312, Lr:0.0001\n",
      "Epoch 43, Step: 20, Loss: 0.305355966091156, Lr:0.0001\n",
      "Epoch 43, Step: 21, Loss: 0.2001371681690216, Lr:0.0001\n",
      "Epoch 43, Step: 22, Loss: 0.5386501550674438, Lr:0.0001\n",
      "Epoch 43, Step: 23, Loss: 0.6015194654464722, Lr:0.0001\n",
      "Epoch 43, Step: 24, Loss: 0.5333332419395447, Lr:0.0001\n",
      "Epoch 43, Step: 25, Loss: 0.964564323425293, Lr:0.0001\n",
      "Epoch 43, Step: 26, Loss: 0.8976929783821106, Lr:0.0001\n",
      "Epoch 43, Step: 27, Loss: 0.05582179129123688, Lr:0.0001\n",
      "Epoch 43, Step: 28, Loss: 1.1184301376342773, Lr:0.0001\n",
      "Epoch 43, Step: 29, Loss: 0.6793107986450195, Lr:0.0001\n",
      "Epoch 43, Step: 30, Loss: 0.6281182765960693, Lr:0.0001\n",
      "Epoch 43, Step: 31, Loss: 0.25039374828338623, Lr:0.0001\n",
      "Epoch 43, Step: 32, Loss: 2.1976587772369385, Lr:0.0001\n",
      "Epoch 43, Step: 33, Loss: 0.16393524408340454, Lr:0.0001\n",
      "Epoch 43, Step: 34, Loss: 0.5049892663955688, Lr:0.0001\n",
      "Epoch 43, Step: 35, Loss: 0.9584254026412964, Lr:0.0001\n",
      "Epoch 43, Step: 36, Loss: 0.314484566450119, Lr:0.0001\n",
      "Epoch 43, Step: 37, Loss: 0.3774279057979584, Lr:0.0001\n",
      "Epoch 43, Step: 38, Loss: 0.25033214688301086, Lr:0.0001\n",
      "Epoch 43, Step: 39, Loss: 0.5029606223106384, Lr:0.0001\n",
      "Epoch 43, Step: 40, Loss: 0.47103720903396606, Lr:0.0001\n",
      "Epoch 43, Step: 41, Loss: 0.16103431582450867, Lr:0.0001\n",
      "Epoch 43, Step: 42, Loss: 0.3637453317642212, Lr:0.0001\n",
      "Epoch 43, Step: 43, Loss: 0.7736681699752808, Lr:0.0001\n",
      "Epoch 43, Step: 44, Loss: 0.37676098942756653, Lr:0.0001\n",
      "Epoch 43, Step: 45, Loss: 0.8817870616912842, Lr:0.0001\n",
      "Epoch 43, Step: 46, Loss: 0.7958581447601318, Lr:0.0001\n",
      "Epoch 43, Step: 47, Loss: 0.7824206948280334, Lr:0.0001\n",
      "Epoch 43, Step: 48, Loss: 0.5180432796478271, Lr:0.0001\n",
      "Epoch 43, Step: 49, Loss: 0.719440221786499, Lr:0.0001\n",
      "Epoch 43, Step: 50, Loss: 0.7274091243743896, Lr:0.0001\n",
      "Epoch 43, Step: 51, Loss: 0.17135003209114075, Lr:0.0001\n",
      "Epoch 43, Step: 52, Loss: 0.5902999639511108, Lr:0.0001\n",
      "Epoch 43, Step: 53, Loss: 1.7476238012313843, Lr:0.0001\n",
      "Epoch 43, Step: 54, Loss: 0.1503826081752777, Lr:0.0001\n",
      "Epoch 43, Step: 55, Loss: 1.6481739282608032, Lr:0.0001\n",
      "Epoch 43, Step: 56, Loss: 0.12429288029670715, Lr:0.0001\n",
      "Epoch 43, Step: 57, Loss: 0.670944094657898, Lr:0.0001\n",
      "Epoch 43, Step: 58, Loss: 0.13611677289009094, Lr:0.0001\n",
      "Epoch 43, Step: 59, Loss: 0.4380573034286499, Lr:0.0001\n",
      "Epoch 43, Step: 60, Loss: 1.2036958932876587, Lr:0.0001\n",
      "Epoch 43, Step: 61, Loss: 0.8736525177955627, Lr:0.0001\n",
      "Epoch 43, Step: 62, Loss: 0.8697952032089233, Lr:0.0001\n",
      "Epoch 43, Step: 63, Loss: 0.38458648324012756, Lr:0.0001\n",
      "Epoch 43, Step: 64, Loss: 1.7416726350784302, Lr:0.0001\n",
      "Epoch 43, Step: 65, Loss: 0.8770749568939209, Lr:0.0001\n",
      "Epoch 43, Step: 66, Loss: 0.7883167266845703, Lr:0.0001\n",
      "Epoch 43, Step: 67, Loss: 0.16506913304328918, Lr:0.0001\n",
      "Epoch 43, Step: 68, Loss: 0.6144015192985535, Lr:0.0001\n",
      "Epoch 43, Step: 69, Loss: 1.2319016456604004, Lr:0.0001\n",
      "Epoch 43, Step: 70, Loss: 0.9237624406814575, Lr:0.0001\n",
      "Epoch 43, Step: 71, Loss: 0.8355007171630859, Lr:0.0001\n",
      "Epoch 43, Step: 72, Loss: 0.5974915027618408, Lr:0.0001\n",
      "Epoch 43, Step: 73, Loss: 0.3890179395675659, Lr:0.0001\n",
      "Epoch 43, Step: 74, Loss: 0.07997284829616547, Lr:0.0001\n",
      "Epoch 43, Step: 75, Loss: 0.9071085453033447, Lr:0.0001\n",
      "Epoch 43, Step: 76, Loss: 0.7506042718887329, Lr:0.0001\n",
      "Epoch 43, Step: 77, Loss: 0.42481285333633423, Lr:0.0001\n",
      "Epoch 43, Step: 78, Loss: 0.5085114240646362, Lr:0.0001\n",
      "Epoch 43, Step: 79, Loss: 0.17812210321426392, Lr:0.0001\n",
      "Epoch 43, Step: 80, Loss: 0.10043220967054367, Lr:0.0001\n",
      "Epoch 43, Step: 81, Loss: 0.07875277101993561, Lr:0.0001\n",
      "Epoch 43, Step: 82, Loss: 0.6925458312034607, Lr:0.0001\n",
      "Epoch 43, Step: 83, Loss: 2.4322469234466553, Lr:0.0001\n",
      "Epoch 43, Step: 84, Loss: 0.5993015766143799, Lr:0.0001\n",
      "Epoch 43, Step: 85, Loss: 0.5155971646308899, Lr:0.0001\n",
      "Epoch 43, Step: 86, Loss: 0.6228151917457581, Lr:0.0001\n",
      "Epoch 43, Step: 87, Loss: 1.2646186351776123, Lr:0.0001\n",
      "Epoch 43, Step: 88, Loss: 0.6020516157150269, Lr:0.0001\n",
      "Epoch 43, Step: 89, Loss: 1.045849323272705, Lr:0.0001\n",
      "Epoch 43, Step: 90, Loss: 0.36364662647247314, Lr:0.0001\n",
      "Epoch 43, Step: 91, Loss: 1.5969115495681763, Lr:0.0001\n",
      "Epoch 43, Step: 92, Loss: 0.2727324664592743, Lr:0.0001\n",
      "Epoch 43, Step: 93, Loss: 0.8834927678108215, Lr:0.0001\n",
      "Epoch 43, Step: 94, Loss: 0.651438295841217, Lr:0.0001\n",
      "Epoch 43, Step: 95, Loss: 0.47894716262817383, Lr:0.0001\n",
      "Epoch 43, Step: 96, Loss: 0.02790912427008152, Lr:0.0001\n",
      "Epoch 43, Step: 97, Loss: 0.9604251980781555, Lr:0.0001\n",
      "Epoch 43, Step: 98, Loss: 0.7554774284362793, Lr:0.0001\n",
      "Epoch 43, Step: 99, Loss: 0.47062456607818604, Lr:0.0001\n",
      "Epoch 43, Step: 100, Loss: 1.1376750469207764, Lr:0.0001\n",
      "Epoch 43, Step: 101, Loss: 0.409770131111145, Lr:0.0001\n",
      "Epoch 43, Step: 102, Loss: 0.4095115661621094, Lr:0.0001\n",
      "Epoch 43, Step: 103, Loss: 0.9533882141113281, Lr:0.0001\n",
      "Epoch 43, Step: 104, Loss: 0.015113553032279015, Lr:0.0001\n",
      "Epoch 43, Step: 105, Loss: 0.6801692247390747, Lr:0.0001\n",
      "Epoch 43, Step: 106, Loss: 0.09147156029939651, Lr:0.0001\n",
      "Epoch 43, Step: 107, Loss: 0.7899081707000732, Lr:0.0001\n",
      "Epoch 43, Step: 108, Loss: 0.3637266457080841, Lr:0.0001\n",
      "Epoch 43, Step: 109, Loss: 1.4721647500991821, Lr:0.0001\n",
      "Epoch 43, Step: 110, Loss: 0.9272699952125549, Lr:0.0001\n",
      "Epoch 43, Step: 111, Loss: 0.3047884702682495, Lr:0.0001\n",
      "Epoch 43, Step: 112, Loss: 0.18747131526470184, Lr:0.0001\n",
      "Epoch 43, Step: 113, Loss: 0.375761479139328, Lr:0.0001\n",
      "Epoch 43, Step: 114, Loss: 0.4859514832496643, Lr:0.0001\n",
      "Epoch 43, Step: 115, Loss: 0.39396244287490845, Lr:0.0001\n",
      "Epoch 43, Step: 116, Loss: 0.34044909477233887, Lr:0.0001\n",
      "Epoch 43, Step: 117, Loss: 0.20689865946769714, Lr:0.0001\n",
      "Epoch 43, Step: 118, Loss: 0.43696829676628113, Lr:0.0001\n",
      "Epoch 43, Step: 119, Loss: 0.5472723841667175, Lr:0.0001\n",
      "Epoch 43, Step: 120, Loss: 0.3132205605506897, Lr:0.0001\n",
      "Epoch 43, Step: 121, Loss: 0.5167733430862427, Lr:0.0001\n",
      "Epoch 43, Step: 122, Loss: 0.846601128578186, Lr:0.0001\n",
      "Epoch 43, Step: 123, Loss: 0.6655171513557434, Lr:0.0001\n",
      "Epoch 43, Step: 124, Loss: 0.9539766311645508, Lr:0.0001\n",
      "Epoch 43, Step: 125, Loss: 0.545904278755188, Lr:0.0001\n",
      "Epoch 43, Step: 126, Loss: 0.6073738932609558, Lr:0.0001\n",
      "Epoch 43, Step: 127, Loss: 0.3198848068714142, Lr:0.0001\n",
      "Epoch 43, Step: 128, Loss: 0.28154271841049194, Lr:0.0001\n",
      "Epoch 43, Step: 129, Loss: 0.5219821929931641, Lr:0.0001\n",
      "Epoch 43, Step: 130, Loss: 0.38136669993400574, Lr:0.0001\n",
      "Epoch 43, Step: 131, Loss: 0.7221139073371887, Lr:0.0001\n",
      "Epoch 43, Step: 132, Loss: 0.15273745357990265, Lr:0.0001\n",
      "Epoch 43, Step: 133, Loss: 0.18894138932228088, Lr:0.0001\n",
      "Epoch 43, Step: 134, Loss: 0.38797950744628906, Lr:0.0001\n",
      "Epoch 43, Step: 135, Loss: 0.34372347593307495, Lr:0.0001\n",
      "Epoch 43, Step: 136, Loss: 0.6856004595756531, Lr:0.0001\n",
      "Epoch 43, Step: 137, Loss: 0.3150622844696045, Lr:0.0001\n",
      "Epoch 43, Step: 138, Loss: 0.9343066215515137, Lr:0.0001\n",
      "Epoch 43, Step: 139, Loss: 0.4847526550292969, Lr:0.0001\n",
      "Epoch 43, Step: 140, Loss: 0.44528281688690186, Lr:0.0001\n",
      "Epoch 43, Step: 141, Loss: 0.3223102390766144, Lr:0.0001\n",
      "Epoch 43, Step: 142, Loss: 0.6930822134017944, Lr:0.0001\n",
      "Epoch 43, Step: 143, Loss: 0.8858911991119385, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 43\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0207 (0.0207) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0069 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4724 (0.8526) acc1: 100.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0068 data: 0.0035 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0069 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.853\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 44, Step: 0, Loss: 0.41650640964508057, Lr:0.0001\n",
      "Epoch 44, Step: 1, Loss: 1.3645809888839722, Lr:0.0001\n",
      "Epoch 44, Step: 2, Loss: 0.07471974194049835, Lr:0.0001\n",
      "Epoch 44, Step: 3, Loss: 0.43234366178512573, Lr:0.0001\n",
      "Epoch 44, Step: 4, Loss: 0.6762624979019165, Lr:0.0001\n",
      "Epoch 44, Step: 5, Loss: 0.03257725387811661, Lr:0.0001\n",
      "Epoch 44, Step: 6, Loss: 1.2935082912445068, Lr:0.0001\n",
      "Epoch 44, Step: 7, Loss: 0.4924587905406952, Lr:0.0001\n",
      "Epoch 44, Step: 8, Loss: 0.46866950392723083, Lr:0.0001\n",
      "Epoch 44, Step: 9, Loss: 0.2829608619213104, Lr:0.0001\n",
      "Epoch 44, Step: 10, Loss: 1.2510011196136475, Lr:0.0001\n",
      "Epoch 44, Step: 11, Loss: 0.24465236067771912, Lr:0.0001\n",
      "Epoch 44, Step: 12, Loss: 0.8977608680725098, Lr:0.0001\n",
      "Epoch 44, Step: 13, Loss: 0.2599591314792633, Lr:0.0001\n",
      "Epoch 44, Step: 14, Loss: 0.3579363226890564, Lr:0.0001\n",
      "Epoch 44, Step: 15, Loss: 0.5854682922363281, Lr:0.0001\n",
      "Epoch 44, Step: 16, Loss: 0.21992622315883636, Lr:0.0001\n",
      "Epoch 44, Step: 17, Loss: 0.31470271944999695, Lr:0.0001\n",
      "Epoch 44, Step: 18, Loss: 0.5937594175338745, Lr:0.0001\n",
      "Epoch 44, Step: 19, Loss: 0.6329140067100525, Lr:0.0001\n",
      "Epoch 44, Step: 20, Loss: 0.18062278628349304, Lr:0.0001\n",
      "Epoch 44, Step: 21, Loss: 0.6987326145172119, Lr:0.0001\n",
      "Epoch 44, Step: 22, Loss: 0.720171332359314, Lr:0.0001\n",
      "Epoch 44, Step: 23, Loss: 0.7640209197998047, Lr:0.0001\n",
      "Epoch 44, Step: 24, Loss: 0.886603057384491, Lr:0.0001\n",
      "Epoch 44, Step: 25, Loss: 0.38523486256599426, Lr:0.0001\n",
      "Epoch 44, Step: 26, Loss: 0.5628994107246399, Lr:0.0001\n",
      "Epoch 44, Step: 27, Loss: 0.19836929440498352, Lr:0.0001\n",
      "Epoch 44, Step: 28, Loss: 0.49084028601646423, Lr:0.0001\n",
      "Epoch 44, Step: 29, Loss: 0.41124868392944336, Lr:0.0001\n",
      "Epoch 44, Step: 30, Loss: 0.8874948024749756, Lr:0.0001\n",
      "Epoch 44, Step: 31, Loss: 1.040937066078186, Lr:0.0001\n",
      "Epoch 44, Step: 32, Loss: 0.7315724492073059, Lr:0.0001\n",
      "Epoch 44, Step: 33, Loss: 1.073734998703003, Lr:0.0001\n",
      "Epoch 44, Step: 34, Loss: 0.2572411000728607, Lr:0.0001\n",
      "Epoch 44, Step: 35, Loss: 0.5000261068344116, Lr:0.0001\n",
      "Epoch 44, Step: 36, Loss: 0.754222571849823, Lr:0.0001\n",
      "Epoch 44, Step: 37, Loss: 0.40111243724823, Lr:0.0001\n",
      "Epoch 44, Step: 38, Loss: 0.476796418428421, Lr:0.0001\n",
      "Epoch 44, Step: 39, Loss: 0.2913113236427307, Lr:0.0001\n",
      "Epoch 44, Step: 40, Loss: 0.5124726295471191, Lr:0.0001\n",
      "Epoch 44, Step: 41, Loss: 0.7434621453285217, Lr:0.0001\n",
      "Epoch 44, Step: 42, Loss: 0.1788545846939087, Lr:0.0001\n",
      "Epoch 44, Step: 43, Loss: 0.867378830909729, Lr:0.0001\n",
      "Epoch 44, Step: 44, Loss: 0.9978947639465332, Lr:0.0001\n",
      "Epoch 44, Step: 45, Loss: 1.2283552885055542, Lr:0.0001\n",
      "Epoch 44, Step: 46, Loss: 0.9552035331726074, Lr:0.0001\n",
      "Epoch 44, Step: 47, Loss: 0.6615986227989197, Lr:0.0001\n",
      "Epoch 44, Step: 48, Loss: 0.8885635137557983, Lr:0.0001\n",
      "Epoch 44, Step: 49, Loss: 0.9140726327896118, Lr:0.0001\n",
      "Epoch 44, Step: 50, Loss: 0.1140613928437233, Lr:0.0001\n",
      "Epoch 44, Step: 51, Loss: 0.00723285460844636, Lr:0.0001\n",
      "Epoch 44, Step: 52, Loss: 2.815882921218872, Lr:0.0001\n",
      "Epoch 44, Step: 53, Loss: 0.9028666019439697, Lr:0.0001\n",
      "Epoch 44, Step: 54, Loss: 1.4322319030761719, Lr:0.0001\n",
      "Epoch 44, Step: 55, Loss: 0.013142724521458149, Lr:0.0001\n",
      "Epoch 44, Step: 56, Loss: 0.5396502017974854, Lr:0.0001\n",
      "Epoch 44, Step: 57, Loss: 0.23350302875041962, Lr:0.0001\n",
      "Epoch 44, Step: 58, Loss: 0.346435546875, Lr:0.0001\n",
      "Epoch 44, Step: 59, Loss: 1.4272716045379639, Lr:0.0001\n",
      "Epoch 44, Step: 60, Loss: 0.4017580449581146, Lr:0.0001\n",
      "Epoch 44, Step: 61, Loss: 0.885770320892334, Lr:0.0001\n",
      "Epoch 44, Step: 62, Loss: 0.01799660362303257, Lr:0.0001\n",
      "Epoch 44, Step: 63, Loss: 0.6097916960716248, Lr:0.0001\n",
      "Epoch 44, Step: 64, Loss: 0.30681586265563965, Lr:0.0001\n",
      "Epoch 44, Step: 65, Loss: 1.3948205709457397, Lr:0.0001\n",
      "Epoch 44, Step: 66, Loss: 1.4127174615859985, Lr:0.0001\n",
      "Epoch 44, Step: 67, Loss: 0.24074339866638184, Lr:0.0001\n",
      "Epoch 44, Step: 68, Loss: 1.2899986505508423, Lr:0.0001\n",
      "Epoch 44, Step: 69, Loss: 0.5198412537574768, Lr:0.0001\n",
      "Epoch 44, Step: 70, Loss: 0.4412197172641754, Lr:0.0001\n",
      "Epoch 44, Step: 71, Loss: 0.3172534704208374, Lr:0.0001\n",
      "Epoch 44, Step: 72, Loss: 0.532983124256134, Lr:0.0001\n",
      "Epoch 44, Step: 73, Loss: 0.34406131505966187, Lr:0.0001\n",
      "Epoch 44, Step: 74, Loss: 1.0318347215652466, Lr:0.0001\n",
      "Epoch 44, Step: 75, Loss: 0.5276291966438293, Lr:0.0001\n",
      "Epoch 44, Step: 76, Loss: 0.3308253586292267, Lr:0.0001\n",
      "Epoch 44, Step: 77, Loss: 0.6738239526748657, Lr:0.0001\n",
      "Epoch 44, Step: 78, Loss: 1.6845511198043823, Lr:0.0001\n",
      "Epoch 44, Step: 79, Loss: 0.10848105698823929, Lr:0.0001\n",
      "Epoch 44, Step: 80, Loss: 0.688622772693634, Lr:0.0001\n",
      "Epoch 44, Step: 81, Loss: 0.41899949312210083, Lr:0.0001\n",
      "Epoch 44, Step: 82, Loss: 0.11809653043746948, Lr:0.0001\n",
      "Epoch 44, Step: 83, Loss: 0.5841152667999268, Lr:0.0001\n",
      "Epoch 44, Step: 84, Loss: 0.37145793437957764, Lr:0.0001\n",
      "Epoch 44, Step: 85, Loss: 0.4115292429924011, Lr:0.0001\n",
      "Epoch 44, Step: 86, Loss: 0.5704358816146851, Lr:0.0001\n",
      "Epoch 44, Step: 87, Loss: 0.37663671374320984, Lr:0.0001\n",
      "Epoch 44, Step: 88, Loss: 1.8849961757659912, Lr:0.0001\n",
      "Epoch 44, Step: 89, Loss: 0.6680383682250977, Lr:0.0001\n",
      "Epoch 44, Step: 90, Loss: 0.23257103562355042, Lr:0.0001\n",
      "Epoch 44, Step: 91, Loss: 0.09518004208803177, Lr:0.0001\n",
      "Epoch 44, Step: 92, Loss: 0.7632066011428833, Lr:0.0001\n",
      "Epoch 44, Step: 93, Loss: 0.3233964741230011, Lr:0.0001\n",
      "Epoch 44, Step: 94, Loss: 0.2829152047634125, Lr:0.0001\n",
      "Epoch 44, Step: 95, Loss: 0.08241286873817444, Lr:0.0001\n",
      "Epoch 44, Step: 96, Loss: 0.3102836608886719, Lr:0.0001\n",
      "Epoch 44, Step: 97, Loss: 1.389630913734436, Lr:0.0001\n",
      "Epoch 44, Step: 98, Loss: 0.4743691682815552, Lr:0.0001\n",
      "Epoch 44, Step: 99, Loss: 0.10889241099357605, Lr:0.0001\n",
      "Epoch 44, Step: 100, Loss: 0.8798515796661377, Lr:0.0001\n",
      "Epoch 44, Step: 101, Loss: 0.5519520044326782, Lr:0.0001\n",
      "Epoch 44, Step: 102, Loss: 0.4532172977924347, Lr:0.0001\n",
      "Epoch 44, Step: 103, Loss: 0.49254855513572693, Lr:0.0001\n",
      "Epoch 44, Step: 104, Loss: 0.8525810241699219, Lr:0.0001\n",
      "Epoch 44, Step: 105, Loss: 0.6221944093704224, Lr:0.0001\n",
      "Epoch 44, Step: 106, Loss: 0.2961609363555908, Lr:0.0001\n",
      "Epoch 44, Step: 107, Loss: 0.3635824918746948, Lr:0.0001\n",
      "Epoch 44, Step: 108, Loss: 1.537131428718567, Lr:0.0001\n",
      "Epoch 44, Step: 109, Loss: 0.20750685036182404, Lr:0.0001\n",
      "Epoch 44, Step: 110, Loss: 0.9310404062271118, Lr:0.0001\n",
      "Epoch 44, Step: 111, Loss: 0.7792103290557861, Lr:0.0001\n",
      "Epoch 44, Step: 112, Loss: 1.254662036895752, Lr:0.0001\n",
      "Epoch 44, Step: 113, Loss: 0.10794658213853836, Lr:0.0001\n",
      "Epoch 44, Step: 114, Loss: 0.3210303485393524, Lr:0.0001\n",
      "Epoch 44, Step: 115, Loss: 1.3300931453704834, Lr:0.0001\n",
      "Epoch 44, Step: 116, Loss: 0.42961597442626953, Lr:0.0001\n",
      "Epoch 44, Step: 117, Loss: 1.3999007940292358, Lr:0.0001\n",
      "Epoch 44, Step: 118, Loss: 0.6671192049980164, Lr:0.0001\n",
      "Epoch 44, Step: 119, Loss: 0.4998053312301636, Lr:0.0001\n",
      "Epoch 44, Step: 120, Loss: 0.3792686462402344, Lr:0.0001\n",
      "Epoch 44, Step: 121, Loss: 0.09756729006767273, Lr:0.0001\n",
      "Epoch 44, Step: 122, Loss: 0.1905338168144226, Lr:0.0001\n",
      "Epoch 44, Step: 123, Loss: 1.2606033086776733, Lr:0.0001\n",
      "Epoch 44, Step: 124, Loss: 0.5090903043746948, Lr:0.0001\n",
      "Epoch 44, Step: 125, Loss: 0.021219739690423012, Lr:0.0001\n",
      "Epoch 44, Step: 126, Loss: 0.357791543006897, Lr:0.0001\n",
      "Epoch 44, Step: 127, Loss: 1.0919153690338135, Lr:0.0001\n",
      "Epoch 44, Step: 128, Loss: 0.6612231731414795, Lr:0.0001\n",
      "Epoch 44, Step: 129, Loss: 0.12883450090885162, Lr:0.0001\n",
      "Epoch 44, Step: 130, Loss: 0.3882750868797302, Lr:0.0001\n",
      "Epoch 44, Step: 131, Loss: 0.6981123685836792, Lr:0.0001\n",
      "Epoch 44, Step: 132, Loss: 0.29991209506988525, Lr:0.0001\n",
      "Epoch 44, Step: 133, Loss: 0.45517897605895996, Lr:0.0001\n",
      "Epoch 44, Step: 134, Loss: 0.7105839252471924, Lr:0.0001\n",
      "Epoch 44, Step: 135, Loss: 0.24972684681415558, Lr:0.0001\n",
      "Epoch 44, Step: 136, Loss: 0.07943312078714371, Lr:0.0001\n",
      "Epoch 44, Step: 137, Loss: 0.18977048993110657, Lr:0.0001\n",
      "Epoch 44, Step: 138, Loss: 0.06183795630931854, Lr:0.0001\n",
      "Epoch 44, Step: 139, Loss: 0.18956169486045837, Lr:0.0001\n",
      "Epoch 44, Step: 140, Loss: 0.3850046992301941, Lr:0.0001\n",
      "Epoch 44, Step: 141, Loss: 1.126922607421875, Lr:0.0001\n",
      "Epoch 44, Step: 142, Loss: 1.242917776107788, Lr:0.0001\n",
      "Epoch 44, Step: 143, Loss: 0.29773321747779846, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 44\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.8710 (0.8710) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0119 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.8710 (1.0100) acc1: 75.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0085 data: 0.0040 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0085 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 1.010\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 45, Step: 0, Loss: 0.2289634644985199, Lr:0.0001\n",
      "Epoch 45, Step: 1, Loss: 0.770620584487915, Lr:0.0001\n",
      "Epoch 45, Step: 2, Loss: 0.4498704671859741, Lr:0.0001\n",
      "Epoch 45, Step: 3, Loss: 0.1808864325284958, Lr:0.0001\n",
      "Epoch 45, Step: 4, Loss: 0.1401587873697281, Lr:0.0001\n",
      "Epoch 45, Step: 5, Loss: 0.8330262303352356, Lr:0.0001\n",
      "Epoch 45, Step: 6, Loss: 0.7760834097862244, Lr:0.0001\n",
      "Epoch 45, Step: 7, Loss: 0.2852252721786499, Lr:0.0001\n",
      "Epoch 45, Step: 8, Loss: 0.8476581573486328, Lr:0.0001\n",
      "Epoch 45, Step: 9, Loss: 0.22635026276111603, Lr:0.0001\n",
      "Epoch 45, Step: 10, Loss: 0.5646019577980042, Lr:0.0001\n",
      "Epoch 45, Step: 11, Loss: 0.2003766894340515, Lr:0.0001\n",
      "Epoch 45, Step: 12, Loss: 1.03468656539917, Lr:0.0001\n",
      "Epoch 45, Step: 13, Loss: 0.607118546962738, Lr:0.0001\n",
      "Epoch 45, Step: 14, Loss: 0.5624210834503174, Lr:0.0001\n",
      "Epoch 45, Step: 15, Loss: 0.4782411754131317, Lr:0.0001\n",
      "Epoch 45, Step: 16, Loss: 0.15745601058006287, Lr:0.0001\n",
      "Epoch 45, Step: 17, Loss: 1.8371515274047852, Lr:0.0001\n",
      "Epoch 45, Step: 18, Loss: 0.43712836503982544, Lr:0.0001\n",
      "Epoch 45, Step: 19, Loss: 0.21676021814346313, Lr:0.0001\n",
      "Epoch 45, Step: 20, Loss: 0.6203640699386597, Lr:0.0001\n",
      "Epoch 45, Step: 21, Loss: 0.31603890657424927, Lr:0.0001\n",
      "Epoch 45, Step: 22, Loss: 0.5486399531364441, Lr:0.0001\n",
      "Epoch 45, Step: 23, Loss: 0.6182316541671753, Lr:0.0001\n",
      "Epoch 45, Step: 24, Loss: 0.31789126992225647, Lr:0.0001\n",
      "Epoch 45, Step: 25, Loss: 0.7350636124610901, Lr:0.0001\n",
      "Epoch 45, Step: 26, Loss: 1.4310599565505981, Lr:0.0001\n",
      "Epoch 45, Step: 27, Loss: 0.39548617601394653, Lr:0.0001\n",
      "Epoch 45, Step: 28, Loss: 0.038653261959552765, Lr:0.0001\n",
      "Epoch 45, Step: 29, Loss: 0.44988781213760376, Lr:0.0001\n",
      "Epoch 45, Step: 30, Loss: 0.5912903547286987, Lr:0.0001\n",
      "Epoch 45, Step: 31, Loss: 1.1448439359664917, Lr:0.0001\n",
      "Epoch 45, Step: 32, Loss: 0.10648009181022644, Lr:0.0001\n",
      "Epoch 45, Step: 33, Loss: 0.4627939462661743, Lr:0.0001\n",
      "Epoch 45, Step: 34, Loss: 1.3859319686889648, Lr:0.0001\n",
      "Epoch 45, Step: 35, Loss: 0.008871426805853844, Lr:0.0001\n",
      "Epoch 45, Step: 36, Loss: 0.0657448023557663, Lr:0.0001\n",
      "Epoch 45, Step: 37, Loss: 0.5201377868652344, Lr:0.0001\n",
      "Epoch 45, Step: 38, Loss: 0.4328296482563019, Lr:0.0001\n",
      "Epoch 45, Step: 39, Loss: 0.30342403054237366, Lr:0.0001\n",
      "Epoch 45, Step: 40, Loss: 0.49135327339172363, Lr:0.0001\n",
      "Epoch 45, Step: 41, Loss: 1.188280701637268, Lr:0.0001\n",
      "Epoch 45, Step: 42, Loss: 0.5198697447776794, Lr:0.0001\n",
      "Epoch 45, Step: 43, Loss: 0.4758877754211426, Lr:0.0001\n",
      "Epoch 45, Step: 44, Loss: 0.399992972612381, Lr:0.0001\n",
      "Epoch 45, Step: 45, Loss: 0.4072444438934326, Lr:0.0001\n",
      "Epoch 45, Step: 46, Loss: 0.20803356170654297, Lr:0.0001\n",
      "Epoch 45, Step: 47, Loss: 0.5411370992660522, Lr:0.0001\n",
      "Epoch 45, Step: 48, Loss: 1.4387142658233643, Lr:0.0001\n",
      "Epoch 45, Step: 49, Loss: 0.6580840349197388, Lr:0.0001\n",
      "Epoch 45, Step: 50, Loss: 0.03119085356593132, Lr:0.0001\n",
      "Epoch 45, Step: 51, Loss: 0.7363972067832947, Lr:0.0001\n",
      "Epoch 45, Step: 52, Loss: 0.09909473359584808, Lr:0.0001\n",
      "Epoch 45, Step: 53, Loss: 0.4225552976131439, Lr:0.0001\n",
      "Epoch 45, Step: 54, Loss: 0.948334813117981, Lr:0.0001\n",
      "Epoch 45, Step: 55, Loss: 0.34625744819641113, Lr:0.0001\n",
      "Epoch 45, Step: 56, Loss: 0.1186785101890564, Lr:0.0001\n",
      "Epoch 45, Step: 57, Loss: 0.5884540677070618, Lr:0.0001\n",
      "Epoch 45, Step: 58, Loss: 0.34959810972213745, Lr:0.0001\n",
      "Epoch 45, Step: 59, Loss: 0.11571615189313889, Lr:0.0001\n",
      "Epoch 45, Step: 60, Loss: 0.5910289287567139, Lr:0.0001\n",
      "Epoch 45, Step: 61, Loss: 0.27395495772361755, Lr:0.0001\n",
      "Epoch 45, Step: 62, Loss: 0.6374903321266174, Lr:0.0001\n",
      "Epoch 45, Step: 63, Loss: 0.11092665791511536, Lr:0.0001\n",
      "Epoch 45, Step: 64, Loss: 0.25905272364616394, Lr:0.0001\n",
      "Epoch 45, Step: 65, Loss: 0.7828571796417236, Lr:0.0001\n",
      "Epoch 45, Step: 66, Loss: 0.8311268091201782, Lr:0.0001\n",
      "Epoch 45, Step: 67, Loss: 0.8097773194313049, Lr:0.0001\n",
      "Epoch 45, Step: 68, Loss: 0.08465167880058289, Lr:0.0001\n",
      "Epoch 45, Step: 69, Loss: 0.37976089119911194, Lr:0.0001\n",
      "Epoch 45, Step: 70, Loss: 0.09721965342760086, Lr:0.0001\n",
      "Epoch 45, Step: 71, Loss: 1.0227797031402588, Lr:0.0001\n",
      "Epoch 45, Step: 72, Loss: 0.6500961780548096, Lr:0.0001\n",
      "Epoch 45, Step: 73, Loss: 0.33335497975349426, Lr:0.0001\n",
      "Epoch 45, Step: 74, Loss: 0.8987991213798523, Lr:0.0001\n",
      "Epoch 45, Step: 75, Loss: 0.557195782661438, Lr:0.0001\n",
      "Epoch 45, Step: 76, Loss: 0.27732548117637634, Lr:0.0001\n",
      "Epoch 45, Step: 77, Loss: 0.30441826581954956, Lr:0.0001\n",
      "Epoch 45, Step: 78, Loss: 0.19386650621891022, Lr:0.0001\n",
      "Epoch 45, Step: 79, Loss: 0.43273115158081055, Lr:0.0001\n",
      "Epoch 45, Step: 80, Loss: 0.8087831735610962, Lr:0.0001\n",
      "Epoch 45, Step: 81, Loss: 0.12903663516044617, Lr:0.0001\n",
      "Epoch 45, Step: 82, Loss: 0.7493060827255249, Lr:0.0001\n",
      "Epoch 45, Step: 83, Loss: 0.5635135173797607, Lr:0.0001\n",
      "Epoch 45, Step: 84, Loss: 1.3153998851776123, Lr:0.0001\n",
      "Epoch 45, Step: 85, Loss: 1.1764856576919556, Lr:0.0001\n",
      "Epoch 45, Step: 86, Loss: 0.7662897109985352, Lr:0.0001\n",
      "Epoch 45, Step: 87, Loss: 0.8267708420753479, Lr:0.0001\n",
      "Epoch 45, Step: 88, Loss: 0.3587093949317932, Lr:0.0001\n",
      "Epoch 45, Step: 89, Loss: 0.14117451012134552, Lr:0.0001\n",
      "Epoch 45, Step: 90, Loss: 0.2736315131187439, Lr:0.0001\n",
      "Epoch 45, Step: 91, Loss: 0.02908315323293209, Lr:0.0001\n",
      "Epoch 45, Step: 92, Loss: 0.6193642020225525, Lr:0.0001\n",
      "Epoch 45, Step: 93, Loss: 0.9493199586868286, Lr:0.0001\n",
      "Epoch 45, Step: 94, Loss: 0.7833456993103027, Lr:0.0001\n",
      "Epoch 45, Step: 95, Loss: 0.4038386642932892, Lr:0.0001\n",
      "Epoch 45, Step: 96, Loss: 1.2723861932754517, Lr:0.0001\n",
      "Epoch 45, Step: 97, Loss: 0.4387844204902649, Lr:0.0001\n",
      "Epoch 45, Step: 98, Loss: 0.7212234735488892, Lr:0.0001\n",
      "Epoch 45, Step: 99, Loss: 1.1436434984207153, Lr:0.0001\n",
      "Epoch 45, Step: 100, Loss: 0.2276814877986908, Lr:0.0001\n",
      "Epoch 45, Step: 101, Loss: 0.017690593376755714, Lr:0.0001\n",
      "Epoch 45, Step: 102, Loss: 0.01951836608350277, Lr:0.0001\n",
      "Epoch 45, Step: 103, Loss: 0.1935802549123764, Lr:0.0001\n",
      "Epoch 45, Step: 104, Loss: 1.109867811203003, Lr:0.0001\n",
      "Epoch 45, Step: 105, Loss: 0.9445981383323669, Lr:0.0001\n",
      "Epoch 45, Step: 106, Loss: 0.8471506834030151, Lr:0.0001\n",
      "Epoch 45, Step: 107, Loss: 0.6000686287879944, Lr:0.0001\n",
      "Epoch 45, Step: 108, Loss: 1.0710257291793823, Lr:0.0001\n",
      "Epoch 45, Step: 109, Loss: 1.5338600873947144, Lr:0.0001\n",
      "Epoch 45, Step: 110, Loss: 0.018775302916765213, Lr:0.0001\n",
      "Epoch 45, Step: 111, Loss: 0.43132102489471436, Lr:0.0001\n",
      "Epoch 45, Step: 112, Loss: 1.6412795782089233, Lr:0.0001\n",
      "Epoch 45, Step: 113, Loss: 1.2130143642425537, Lr:0.0001\n",
      "Epoch 45, Step: 114, Loss: 0.7614453434944153, Lr:0.0001\n",
      "Epoch 45, Step: 115, Loss: 1.1844825744628906, Lr:0.0001\n",
      "Epoch 45, Step: 116, Loss: 0.9968945384025574, Lr:0.0001\n",
      "Epoch 45, Step: 117, Loss: 0.6922951936721802, Lr:0.0001\n",
      "Epoch 45, Step: 118, Loss: 0.12934979796409607, Lr:0.0001\n",
      "Epoch 45, Step: 119, Loss: 0.3009987473487854, Lr:0.0001\n",
      "Epoch 45, Step: 120, Loss: 0.08520033955574036, Lr:0.0001\n",
      "Epoch 45, Step: 121, Loss: 0.5487310886383057, Lr:0.0001\n",
      "Epoch 45, Step: 122, Loss: 0.9367771744728088, Lr:0.0001\n",
      "Epoch 45, Step: 123, Loss: 0.17291000485420227, Lr:0.0001\n",
      "Epoch 45, Step: 124, Loss: 0.025609619915485382, Lr:0.0001\n",
      "Epoch 45, Step: 125, Loss: 0.5209261775016785, Lr:0.0001\n",
      "Epoch 45, Step: 126, Loss: 1.8889226913452148, Lr:0.0001\n",
      "Epoch 45, Step: 127, Loss: 0.7178297638893127, Lr:0.0001\n",
      "Epoch 45, Step: 128, Loss: 0.5625175833702087, Lr:0.0001\n",
      "Epoch 45, Step: 129, Loss: 0.8530521988868713, Lr:0.0001\n",
      "Epoch 45, Step: 130, Loss: 0.5525798797607422, Lr:0.0001\n",
      "Epoch 45, Step: 131, Loss: 0.5969318747520447, Lr:0.0001\n",
      "Epoch 45, Step: 132, Loss: 0.14462704956531525, Lr:0.0001\n",
      "Epoch 45, Step: 133, Loss: 0.43959131836891174, Lr:0.0001\n",
      "Epoch 45, Step: 134, Loss: 0.4780268669128418, Lr:0.0001\n",
      "Epoch 45, Step: 135, Loss: 0.1994258463382721, Lr:0.0001\n",
      "Epoch 45, Step: 136, Loss: 1.6589515209197998, Lr:0.0001\n",
      "Epoch 45, Step: 137, Loss: 0.5683127641677856, Lr:0.0001\n",
      "Epoch 45, Step: 138, Loss: 0.3701122999191284, Lr:0.0001\n",
      "Epoch 45, Step: 139, Loss: 0.5225774049758911, Lr:0.0001\n",
      "Epoch 45, Step: 140, Loss: 0.2058544158935547, Lr:0.0001\n",
      "Epoch 45, Step: 141, Loss: 0.07197181135416031, Lr:0.0001\n",
      "Epoch 45, Step: 142, Loss: 0.5044780373573303, Lr:0.0001\n",
      "Epoch 45, Step: 143, Loss: 0.22474509477615356, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 45\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0070 (0.0070) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0110 data: 0.0060 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.2574 (0.5436) acc1: 100.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0090 data: 0.0040 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0090 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.544\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 46, Step: 0, Loss: 0.18063095211982727, Lr:0.0001\n",
      "Epoch 46, Step: 1, Loss: 0.09204870462417603, Lr:0.0001\n",
      "Epoch 46, Step: 2, Loss: 0.821151614189148, Lr:0.0001\n",
      "Epoch 46, Step: 3, Loss: 0.19571241736412048, Lr:0.0001\n",
      "Epoch 46, Step: 4, Loss: 0.18109823763370514, Lr:0.0001\n",
      "Epoch 46, Step: 5, Loss: 0.7763718366622925, Lr:0.0001\n",
      "Epoch 46, Step: 6, Loss: 0.549752414226532, Lr:0.0001\n",
      "Epoch 46, Step: 7, Loss: 0.03637222573161125, Lr:0.0001\n",
      "Epoch 46, Step: 8, Loss: 0.6455082893371582, Lr:0.0001\n",
      "Epoch 46, Step: 9, Loss: 0.08749271184206009, Lr:0.0001\n",
      "Epoch 46, Step: 10, Loss: 0.1506526619195938, Lr:0.0001\n",
      "Epoch 46, Step: 11, Loss: 0.5572280883789062, Lr:0.0001\n",
      "Epoch 46, Step: 12, Loss: 0.1928386092185974, Lr:0.0001\n",
      "Epoch 46, Step: 13, Loss: 0.5890796184539795, Lr:0.0001\n",
      "Epoch 46, Step: 14, Loss: 1.344070315361023, Lr:0.0001\n",
      "Epoch 46, Step: 15, Loss: 0.6832364797592163, Lr:0.0001\n",
      "Epoch 46, Step: 16, Loss: 0.6273593902587891, Lr:0.0001\n",
      "Epoch 46, Step: 17, Loss: 0.4320977032184601, Lr:0.0001\n",
      "Epoch 46, Step: 18, Loss: 0.005614984780550003, Lr:0.0001\n",
      "Epoch 46, Step: 19, Loss: 0.1295216977596283, Lr:0.0001\n",
      "Epoch 46, Step: 20, Loss: 0.4736543297767639, Lr:0.0001\n",
      "Epoch 46, Step: 21, Loss: 0.3847275972366333, Lr:0.0001\n",
      "Epoch 46, Step: 22, Loss: 0.23810750246047974, Lr:0.0001\n",
      "Epoch 46, Step: 23, Loss: 0.43130943179130554, Lr:0.0001\n",
      "Epoch 46, Step: 24, Loss: 0.010138997808098793, Lr:0.0001\n",
      "Epoch 46, Step: 25, Loss: 0.0673530176281929, Lr:0.0001\n",
      "Epoch 46, Step: 26, Loss: 0.42588838934898376, Lr:0.0001\n",
      "Epoch 46, Step: 27, Loss: 0.45097967982292175, Lr:0.0001\n",
      "Epoch 46, Step: 28, Loss: 0.6106879711151123, Lr:0.0001\n",
      "Epoch 46, Step: 29, Loss: 0.6047025322914124, Lr:0.0001\n",
      "Epoch 46, Step: 30, Loss: 0.31852108240127563, Lr:0.0001\n",
      "Epoch 46, Step: 31, Loss: 1.2127656936645508, Lr:0.0001\n",
      "Epoch 46, Step: 32, Loss: 0.28905633091926575, Lr:0.0001\n",
      "Epoch 46, Step: 33, Loss: 0.4089822471141815, Lr:0.0001\n",
      "Epoch 46, Step: 34, Loss: 1.0531138181686401, Lr:0.0001\n",
      "Epoch 46, Step: 35, Loss: 0.7439438104629517, Lr:0.0001\n",
      "Epoch 46, Step: 36, Loss: 0.30877113342285156, Lr:0.0001\n",
      "Epoch 46, Step: 37, Loss: 0.8668214678764343, Lr:0.0001\n",
      "Epoch 46, Step: 38, Loss: 0.0590052530169487, Lr:0.0001\n",
      "Epoch 46, Step: 39, Loss: 0.5118340849876404, Lr:0.0001\n",
      "Epoch 46, Step: 40, Loss: 0.399538516998291, Lr:0.0001\n",
      "Epoch 46, Step: 41, Loss: 0.671141505241394, Lr:0.0001\n",
      "Epoch 46, Step: 42, Loss: 0.6195134520530701, Lr:0.0001\n",
      "Epoch 46, Step: 43, Loss: 2.359055280685425, Lr:0.0001\n",
      "Epoch 46, Step: 44, Loss: 1.0345089435577393, Lr:0.0001\n",
      "Epoch 46, Step: 45, Loss: 0.8163062930107117, Lr:0.0001\n",
      "Epoch 46, Step: 46, Loss: 0.8656489849090576, Lr:0.0001\n",
      "Epoch 46, Step: 47, Loss: 1.0871541500091553, Lr:0.0001\n",
      "Epoch 46, Step: 48, Loss: 0.8692008256912231, Lr:0.0001\n",
      "Epoch 46, Step: 49, Loss: 0.5391897559165955, Lr:0.0001\n",
      "Epoch 46, Step: 50, Loss: 0.9791313409805298, Lr:0.0001\n",
      "Epoch 46, Step: 51, Loss: 0.4103028178215027, Lr:0.0001\n",
      "Epoch 46, Step: 52, Loss: 0.2867890000343323, Lr:0.0001\n",
      "Epoch 46, Step: 53, Loss: 0.8347821235656738, Lr:0.0001\n",
      "Epoch 46, Step: 54, Loss: 0.2149440050125122, Lr:0.0001\n",
      "Epoch 46, Step: 55, Loss: 0.38518813252449036, Lr:0.0001\n",
      "Epoch 46, Step: 56, Loss: 1.112474799156189, Lr:0.0001\n",
      "Epoch 46, Step: 57, Loss: 0.7919548749923706, Lr:0.0001\n",
      "Epoch 46, Step: 58, Loss: 0.17571865022182465, Lr:0.0001\n",
      "Epoch 46, Step: 59, Loss: 0.6771311163902283, Lr:0.0001\n",
      "Epoch 46, Step: 60, Loss: 0.4662535786628723, Lr:0.0001\n",
      "Epoch 46, Step: 61, Loss: 0.3883619010448456, Lr:0.0001\n",
      "Epoch 46, Step: 62, Loss: 1.1413123607635498, Lr:0.0001\n",
      "Epoch 46, Step: 63, Loss: 0.5446205139160156, Lr:0.0001\n",
      "Epoch 46, Step: 64, Loss: 0.7179924249649048, Lr:0.0001\n",
      "Epoch 46, Step: 65, Loss: 0.5764129161834717, Lr:0.0001\n",
      "Epoch 46, Step: 66, Loss: 0.5401166081428528, Lr:0.0001\n",
      "Epoch 46, Step: 67, Loss: 0.3315008580684662, Lr:0.0001\n",
      "Epoch 46, Step: 68, Loss: 0.061629001051187515, Lr:0.0001\n",
      "Epoch 46, Step: 69, Loss: 0.6115291714668274, Lr:0.0001\n",
      "Epoch 46, Step: 70, Loss: 1.1491206884384155, Lr:0.0001\n",
      "Epoch 46, Step: 71, Loss: 0.5670496821403503, Lr:0.0001\n",
      "Epoch 46, Step: 72, Loss: 0.1657412350177765, Lr:0.0001\n",
      "Epoch 46, Step: 73, Loss: 0.48540961742401123, Lr:0.0001\n",
      "Epoch 46, Step: 74, Loss: 0.859178900718689, Lr:0.0001\n",
      "Epoch 46, Step: 75, Loss: 0.27828672528266907, Lr:0.0001\n",
      "Epoch 46, Step: 76, Loss: 0.1860055774450302, Lr:0.0001\n",
      "Epoch 46, Step: 77, Loss: 1.2292691469192505, Lr:0.0001\n",
      "Epoch 46, Step: 78, Loss: 0.37354785203933716, Lr:0.0001\n",
      "Epoch 46, Step: 79, Loss: 1.8130780458450317, Lr:0.0001\n",
      "Epoch 46, Step: 80, Loss: 0.12097037583589554, Lr:0.0001\n",
      "Epoch 46, Step: 81, Loss: 0.5795378088951111, Lr:0.0001\n",
      "Epoch 46, Step: 82, Loss: 0.9468525052070618, Lr:0.0001\n",
      "Epoch 46, Step: 83, Loss: 0.7312155961990356, Lr:0.0001\n",
      "Epoch 46, Step: 84, Loss: 0.17445777356624603, Lr:0.0001\n",
      "Epoch 46, Step: 85, Loss: 0.13017497956752777, Lr:0.0001\n",
      "Epoch 46, Step: 86, Loss: 0.04188606142997742, Lr:0.0001\n",
      "Epoch 46, Step: 87, Loss: 1.5466430187225342, Lr:0.0001\n",
      "Epoch 46, Step: 88, Loss: 0.5881150960922241, Lr:0.0001\n",
      "Epoch 46, Step: 89, Loss: 0.2558403015136719, Lr:0.0001\n",
      "Epoch 46, Step: 90, Loss: 1.241965651512146, Lr:0.0001\n",
      "Epoch 46, Step: 91, Loss: 0.08185744285583496, Lr:0.0001\n",
      "Epoch 46, Step: 92, Loss: 0.18278498947620392, Lr:0.0001\n",
      "Epoch 46, Step: 93, Loss: 0.6068063378334045, Lr:0.0001\n",
      "Epoch 46, Step: 94, Loss: 0.5668475031852722, Lr:0.0001\n",
      "Epoch 46, Step: 95, Loss: 0.6351470947265625, Lr:0.0001\n",
      "Epoch 46, Step: 96, Loss: 0.5747515559196472, Lr:0.0001\n",
      "Epoch 46, Step: 97, Loss: 0.48457449674606323, Lr:0.0001\n",
      "Epoch 46, Step: 98, Loss: 0.23708626627922058, Lr:0.0001\n",
      "Epoch 46, Step: 99, Loss: 0.8231741189956665, Lr:0.0001\n",
      "Epoch 46, Step: 100, Loss: 0.32943668961524963, Lr:0.0001\n",
      "Epoch 46, Step: 101, Loss: 0.3752169609069824, Lr:0.0001\n",
      "Epoch 46, Step: 102, Loss: 0.021870719268918037, Lr:0.0001\n",
      "Epoch 46, Step: 103, Loss: 0.34794583916664124, Lr:0.0001\n",
      "Epoch 46, Step: 104, Loss: 0.8184009790420532, Lr:0.0001\n",
      "Epoch 46, Step: 105, Loss: 0.7215680480003357, Lr:0.0001\n",
      "Epoch 46, Step: 106, Loss: 1.4172571897506714, Lr:0.0001\n",
      "Epoch 46, Step: 107, Loss: 0.24234536290168762, Lr:0.0001\n",
      "Epoch 46, Step: 108, Loss: 0.2819937765598297, Lr:0.0001\n",
      "Epoch 46, Step: 109, Loss: 1.5151426792144775, Lr:0.0001\n",
      "Epoch 46, Step: 110, Loss: 1.0912073850631714, Lr:0.0001\n",
      "Epoch 46, Step: 111, Loss: 0.17646989226341248, Lr:0.0001\n",
      "Epoch 46, Step: 112, Loss: 0.3378189504146576, Lr:0.0001\n",
      "Epoch 46, Step: 113, Loss: 0.7041730880737305, Lr:0.0001\n",
      "Epoch 46, Step: 114, Loss: 0.39273378252983093, Lr:0.0001\n",
      "Epoch 46, Step: 115, Loss: 0.6346867084503174, Lr:0.0001\n",
      "Epoch 46, Step: 116, Loss: 0.7724013924598694, Lr:0.0001\n",
      "Epoch 46, Step: 117, Loss: 0.6824487447738647, Lr:0.0001\n",
      "Epoch 46, Step: 118, Loss: 0.6172047257423401, Lr:0.0001\n",
      "Epoch 46, Step: 119, Loss: 1.0089157819747925, Lr:0.0001\n",
      "Epoch 46, Step: 120, Loss: 0.30930095911026, Lr:0.0001\n",
      "Epoch 46, Step: 121, Loss: 0.8106780052185059, Lr:0.0001\n",
      "Epoch 46, Step: 122, Loss: 0.318745881319046, Lr:0.0001\n",
      "Epoch 46, Step: 123, Loss: 0.033059339970350266, Lr:0.0001\n",
      "Epoch 46, Step: 124, Loss: 0.5878407955169678, Lr:0.0001\n",
      "Epoch 46, Step: 125, Loss: 3.3952627182006836, Lr:0.0001\n",
      "Epoch 46, Step: 126, Loss: 0.9414454698562622, Lr:0.0001\n",
      "Epoch 46, Step: 127, Loss: 0.3514420986175537, Lr:0.0001\n",
      "Epoch 46, Step: 128, Loss: 0.26515692472457886, Lr:0.0001\n",
      "Epoch 46, Step: 129, Loss: 0.5670694708824158, Lr:0.0001\n",
      "Epoch 46, Step: 130, Loss: 0.8115462064743042, Lr:0.0001\n",
      "Epoch 46, Step: 131, Loss: 0.2527804374694824, Lr:0.0001\n",
      "Epoch 46, Step: 132, Loss: 0.611514687538147, Lr:0.0001\n",
      "Epoch 46, Step: 133, Loss: 0.18940888345241547, Lr:0.0001\n",
      "Epoch 46, Step: 134, Loss: 0.6740598082542419, Lr:0.0001\n",
      "Epoch 46, Step: 135, Loss: 0.2329067587852478, Lr:0.0001\n",
      "Epoch 46, Step: 136, Loss: 0.5427980422973633, Lr:0.0001\n",
      "Epoch 46, Step: 137, Loss: 0.48949337005615234, Lr:0.0001\n",
      "Epoch 46, Step: 138, Loss: 0.8326220512390137, Lr:0.0001\n",
      "Epoch 46, Step: 139, Loss: 0.5523977279663086, Lr:0.0001\n",
      "Epoch 46, Step: 140, Loss: 0.7808395624160767, Lr:0.0001\n",
      "Epoch 46, Step: 141, Loss: 0.08746335655450821, Lr:0.0001\n",
      "Epoch 46, Step: 142, Loss: 0.5950319170951843, Lr:0.0001\n",
      "Epoch 46, Step: 143, Loss: 0.2920583486557007, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 46\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.5184 (0.5184) acc1: 75.0000 (75.0000) acc5: 100.0000 (100.0000) time: 0.0089 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.6581 (0.8481) acc1: 75.0000 (72.7273) acc5: 100.0000 (100.0000) time: 0.0067 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0069 s / it)\n",
      "* Acc@1 72.727 Acc@5 100.000 loss 0.848\n",
      "Accuracy of the network on the 33 test image: 72.7%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 47, Step: 0, Loss: 1.2205145359039307, Lr:0.0001\n",
      "Epoch 47, Step: 1, Loss: 0.5396691560745239, Lr:0.0001\n",
      "Epoch 47, Step: 2, Loss: 0.18485389649868011, Lr:0.0001\n",
      "Epoch 47, Step: 3, Loss: 0.7391715049743652, Lr:0.0001\n",
      "Epoch 47, Step: 4, Loss: 0.6443586945533752, Lr:0.0001\n",
      "Epoch 47, Step: 5, Loss: 0.19762223958969116, Lr:0.0001\n",
      "Epoch 47, Step: 6, Loss: 0.6276658177375793, Lr:0.0001\n",
      "Epoch 47, Step: 7, Loss: 0.542301595211029, Lr:0.0001\n",
      "Epoch 47, Step: 8, Loss: 0.5110365152359009, Lr:0.0001\n",
      "Epoch 47, Step: 9, Loss: 0.7055711150169373, Lr:0.0001\n",
      "Epoch 47, Step: 10, Loss: 0.44904688000679016, Lr:0.0001\n",
      "Epoch 47, Step: 11, Loss: 0.6597481966018677, Lr:0.0001\n",
      "Epoch 47, Step: 12, Loss: 0.2894291877746582, Lr:0.0001\n",
      "Epoch 47, Step: 13, Loss: 0.5005062818527222, Lr:0.0001\n",
      "Epoch 47, Step: 14, Loss: 0.5020697116851807, Lr:0.0001\n",
      "Epoch 47, Step: 15, Loss: 0.5029013752937317, Lr:0.0001\n",
      "Epoch 47, Step: 16, Loss: 0.3820358216762543, Lr:0.0001\n",
      "Epoch 47, Step: 17, Loss: 1.033392310142517, Lr:0.0001\n",
      "Epoch 47, Step: 18, Loss: 0.3623386323451996, Lr:0.0001\n",
      "Epoch 47, Step: 19, Loss: 0.8587460517883301, Lr:0.0001\n",
      "Epoch 47, Step: 20, Loss: 1.1304152011871338, Lr:0.0001\n",
      "Epoch 47, Step: 21, Loss: 0.3511781096458435, Lr:0.0001\n",
      "Epoch 47, Step: 22, Loss: 0.24786517024040222, Lr:0.0001\n",
      "Epoch 47, Step: 23, Loss: 0.6561392545700073, Lr:0.0001\n",
      "Epoch 47, Step: 24, Loss: 0.5347813367843628, Lr:0.0001\n",
      "Epoch 47, Step: 25, Loss: 1.5994857549667358, Lr:0.0001\n",
      "Epoch 47, Step: 26, Loss: 0.15677344799041748, Lr:0.0001\n",
      "Epoch 47, Step: 27, Loss: 0.2791008949279785, Lr:0.0001\n",
      "Epoch 47, Step: 28, Loss: 0.346943199634552, Lr:0.0001\n",
      "Epoch 47, Step: 29, Loss: 1.729419231414795, Lr:0.0001\n",
      "Epoch 47, Step: 30, Loss: 0.7129337787628174, Lr:0.0001\n",
      "Epoch 47, Step: 31, Loss: 0.8781538009643555, Lr:0.0001\n",
      "Epoch 47, Step: 32, Loss: 3.0728964805603027, Lr:0.0001\n",
      "Epoch 47, Step: 33, Loss: 0.8118908405303955, Lr:0.0001\n",
      "Epoch 47, Step: 34, Loss: 1.0182099342346191, Lr:0.0001\n",
      "Epoch 47, Step: 35, Loss: 0.21348781883716583, Lr:0.0001\n",
      "Epoch 47, Step: 36, Loss: 0.8257936835289001, Lr:0.0001\n",
      "Epoch 47, Step: 37, Loss: 2.9405622482299805, Lr:0.0001\n",
      "Epoch 47, Step: 38, Loss: 0.7210721969604492, Lr:0.0001\n",
      "Epoch 47, Step: 39, Loss: 0.6764305233955383, Lr:0.0001\n",
      "Epoch 47, Step: 40, Loss: 0.6126121282577515, Lr:0.0001\n",
      "Epoch 47, Step: 41, Loss: 0.9582462310791016, Lr:0.0001\n",
      "Epoch 47, Step: 42, Loss: 0.20944449305534363, Lr:0.0001\n",
      "Epoch 47, Step: 43, Loss: 0.5488858819007874, Lr:0.0001\n",
      "Epoch 47, Step: 44, Loss: 0.8411412239074707, Lr:0.0001\n",
      "Epoch 47, Step: 45, Loss: 0.358865350484848, Lr:0.0001\n",
      "Epoch 47, Step: 46, Loss: 0.3328321576118469, Lr:0.0001\n",
      "Epoch 47, Step: 47, Loss: 0.40057554841041565, Lr:0.0001\n",
      "Epoch 47, Step: 48, Loss: 0.6136695742607117, Lr:0.0001\n",
      "Epoch 47, Step: 49, Loss: 0.9744970202445984, Lr:0.0001\n",
      "Epoch 47, Step: 50, Loss: 1.2054013013839722, Lr:0.0001\n",
      "Epoch 47, Step: 51, Loss: 0.31209567189216614, Lr:0.0001\n",
      "Epoch 47, Step: 52, Loss: 0.7484207153320312, Lr:0.0001\n",
      "Epoch 47, Step: 53, Loss: 1.163252353668213, Lr:0.0001\n",
      "Epoch 47, Step: 54, Loss: 0.824962317943573, Lr:0.0001\n",
      "Epoch 47, Step: 55, Loss: 0.3637174963951111, Lr:0.0001\n",
      "Epoch 47, Step: 56, Loss: 0.5864586234092712, Lr:0.0001\n",
      "Epoch 47, Step: 57, Loss: 0.30850109457969666, Lr:0.0001\n",
      "Epoch 47, Step: 58, Loss: 0.38891416788101196, Lr:0.0001\n",
      "Epoch 47, Step: 59, Loss: 0.5326561331748962, Lr:0.0001\n",
      "Epoch 47, Step: 60, Loss: 0.47660404443740845, Lr:0.0001\n",
      "Epoch 47, Step: 61, Loss: 0.022657137364149094, Lr:0.0001\n",
      "Epoch 47, Step: 62, Loss: 0.2720555365085602, Lr:0.0001\n",
      "Epoch 47, Step: 63, Loss: 0.506834864616394, Lr:0.0001\n",
      "Epoch 47, Step: 64, Loss: 0.09517142921686172, Lr:0.0001\n",
      "Epoch 47, Step: 65, Loss: 0.019511256366968155, Lr:0.0001\n",
      "Epoch 47, Step: 66, Loss: 0.5879339575767517, Lr:0.0001\n",
      "Epoch 47, Step: 67, Loss: 0.3384477198123932, Lr:0.0001\n",
      "Epoch 47, Step: 68, Loss: 0.38666772842407227, Lr:0.0001\n",
      "Epoch 47, Step: 69, Loss: 0.2130202054977417, Lr:0.0001\n",
      "Epoch 47, Step: 70, Loss: 0.15612450242042542, Lr:0.0001\n",
      "Epoch 47, Step: 71, Loss: 0.6604422330856323, Lr:0.0001\n",
      "Epoch 47, Step: 72, Loss: 0.2838376760482788, Lr:0.0001\n",
      "Epoch 47, Step: 73, Loss: 0.6745534539222717, Lr:0.0001\n",
      "Epoch 47, Step: 74, Loss: 3.1094894409179688, Lr:0.0001\n",
      "Epoch 47, Step: 75, Loss: 0.5277231931686401, Lr:0.0001\n",
      "Epoch 47, Step: 76, Loss: 0.5426071286201477, Lr:0.0001\n",
      "Epoch 47, Step: 77, Loss: 1.6018160581588745, Lr:0.0001\n",
      "Epoch 47, Step: 78, Loss: 0.9603246450424194, Lr:0.0001\n",
      "Epoch 47, Step: 79, Loss: 0.05242745205760002, Lr:0.0001\n",
      "Epoch 47, Step: 80, Loss: 1.7151148319244385, Lr:0.0001\n",
      "Epoch 47, Step: 81, Loss: 1.6055212020874023, Lr:0.0001\n",
      "Epoch 47, Step: 82, Loss: 0.5131038427352905, Lr:0.0001\n",
      "Epoch 47, Step: 83, Loss: 0.5198056697845459, Lr:0.0001\n",
      "Epoch 47, Step: 84, Loss: 0.12848863005638123, Lr:0.0001\n",
      "Epoch 47, Step: 85, Loss: 0.32415148615837097, Lr:0.0001\n",
      "Epoch 47, Step: 86, Loss: 0.7802557349205017, Lr:0.0001\n",
      "Epoch 47, Step: 87, Loss: 0.4942083954811096, Lr:0.0001\n",
      "Epoch 47, Step: 88, Loss: 1.0531939268112183, Lr:0.0001\n",
      "Epoch 47, Step: 89, Loss: 0.13203823566436768, Lr:0.0001\n",
      "Epoch 47, Step: 90, Loss: 0.7295127511024475, Lr:0.0001\n",
      "Epoch 47, Step: 91, Loss: 0.08473946899175644, Lr:0.0001\n",
      "Epoch 47, Step: 92, Loss: 0.6914339065551758, Lr:0.0001\n",
      "Epoch 47, Step: 93, Loss: 0.2606900930404663, Lr:0.0001\n",
      "Epoch 47, Step: 94, Loss: 0.8038430213928223, Lr:0.0001\n",
      "Epoch 47, Step: 95, Loss: 0.6820879578590393, Lr:0.0001\n",
      "Epoch 47, Step: 96, Loss: 0.3855857849121094, Lr:0.0001\n",
      "Epoch 47, Step: 97, Loss: 0.28452304005622864, Lr:0.0001\n",
      "Epoch 47, Step: 98, Loss: 0.6679571866989136, Lr:0.0001\n",
      "Epoch 47, Step: 99, Loss: 0.2520056366920471, Lr:0.0001\n",
      "Epoch 47, Step: 100, Loss: 0.20795440673828125, Lr:0.0001\n",
      "Epoch 47, Step: 101, Loss: 0.11815696209669113, Lr:0.0001\n",
      "Epoch 47, Step: 102, Loss: 0.06162682920694351, Lr:0.0001\n",
      "Epoch 47, Step: 103, Loss: 0.4574925899505615, Lr:0.0001\n",
      "Epoch 47, Step: 104, Loss: 0.24293997883796692, Lr:0.0001\n",
      "Epoch 47, Step: 105, Loss: 0.27540165185928345, Lr:0.0001\n",
      "Epoch 47, Step: 106, Loss: 0.5378268957138062, Lr:0.0001\n",
      "Epoch 47, Step: 107, Loss: 0.24688765406608582, Lr:0.0001\n",
      "Epoch 47, Step: 108, Loss: 1.191909909248352, Lr:0.0001\n",
      "Epoch 47, Step: 109, Loss: 0.7443887591362, Lr:0.0001\n",
      "Epoch 47, Step: 110, Loss: 0.2081148773431778, Lr:0.0001\n",
      "Epoch 47, Step: 111, Loss: 0.7065497636795044, Lr:0.0001\n",
      "Epoch 47, Step: 112, Loss: 0.25326108932495117, Lr:0.0001\n",
      "Epoch 47, Step: 113, Loss: 0.8635927438735962, Lr:0.0001\n",
      "Epoch 47, Step: 114, Loss: 0.26216718554496765, Lr:0.0001\n",
      "Epoch 47, Step: 115, Loss: 0.3796752989292145, Lr:0.0001\n",
      "Epoch 47, Step: 116, Loss: 1.6879972219467163, Lr:0.0001\n",
      "Epoch 47, Step: 117, Loss: 0.17900505661964417, Lr:0.0001\n",
      "Epoch 47, Step: 118, Loss: 0.22070322930812836, Lr:0.0001\n",
      "Epoch 47, Step: 119, Loss: 0.38775503635406494, Lr:0.0001\n",
      "Epoch 47, Step: 120, Loss: 0.023430848494172096, Lr:0.0001\n",
      "Epoch 47, Step: 121, Loss: 0.046988435089588165, Lr:0.0001\n",
      "Epoch 47, Step: 122, Loss: 1.3122341632843018, Lr:0.0001\n",
      "Epoch 47, Step: 123, Loss: 0.23883095383644104, Lr:0.0001\n",
      "Epoch 47, Step: 124, Loss: 0.8190770745277405, Lr:0.0001\n",
      "Epoch 47, Step: 125, Loss: 0.4547977149486542, Lr:0.0001\n",
      "Epoch 47, Step: 126, Loss: 1.1481812000274658, Lr:0.0001\n",
      "Epoch 47, Step: 127, Loss: 0.25730979442596436, Lr:0.0001\n",
      "Epoch 47, Step: 128, Loss: 0.0788777619600296, Lr:0.0001\n",
      "Epoch 47, Step: 129, Loss: 0.334476500749588, Lr:0.0001\n",
      "Epoch 47, Step: 130, Loss: 0.9675285816192627, Lr:0.0001\n",
      "Epoch 47, Step: 131, Loss: 0.7218211889266968, Lr:0.0001\n",
      "Epoch 47, Step: 132, Loss: 0.11236519366502762, Lr:0.0001\n",
      "Epoch 47, Step: 133, Loss: 1.1183501482009888, Lr:0.0001\n",
      "Epoch 47, Step: 134, Loss: 0.29785940051078796, Lr:0.0001\n",
      "Epoch 47, Step: 135, Loss: 0.6200626492500305, Lr:0.0001\n",
      "Epoch 47, Step: 136, Loss: 0.5486401915550232, Lr:0.0001\n",
      "Epoch 47, Step: 137, Loss: 1.1529844999313354, Lr:0.0001\n",
      "Epoch 47, Step: 138, Loss: 0.12692223489284515, Lr:0.0001\n",
      "Epoch 47, Step: 139, Loss: 0.2630624771118164, Lr:0.0001\n",
      "Epoch 47, Step: 140, Loss: 0.43285179138183594, Lr:0.0001\n",
      "Epoch 47, Step: 141, Loss: 0.7154607176780701, Lr:0.0001\n",
      "Epoch 47, Step: 142, Loss: 0.47534725069999695, Lr:0.0001\n",
      "Epoch 47, Step: 143, Loss: 0.31416356563568115, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 47\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0011 (0.0011) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0099 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.3925 (0.5231) acc1: 75.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0069 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0070 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.523\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 48, Step: 0, Loss: 0.7702038288116455, Lr:0.0001\n",
      "Epoch 48, Step: 1, Loss: 0.5128409266471863, Lr:0.0001\n",
      "Epoch 48, Step: 2, Loss: 0.24032187461853027, Lr:0.0001\n",
      "Epoch 48, Step: 3, Loss: 0.5369490385055542, Lr:0.0001\n",
      "Epoch 48, Step: 4, Loss: 0.4807247519493103, Lr:0.0001\n",
      "Epoch 48, Step: 5, Loss: 0.7901865839958191, Lr:0.0001\n",
      "Epoch 48, Step: 6, Loss: 0.4253895878791809, Lr:0.0001\n",
      "Epoch 48, Step: 7, Loss: 0.37705591320991516, Lr:0.0001\n",
      "Epoch 48, Step: 8, Loss: 0.7441316843032837, Lr:0.0001\n",
      "Epoch 48, Step: 9, Loss: 0.3385927081108093, Lr:0.0001\n",
      "Epoch 48, Step: 10, Loss: 2.181654930114746, Lr:0.0001\n",
      "Epoch 48, Step: 11, Loss: 0.14537125825881958, Lr:0.0001\n",
      "Epoch 48, Step: 12, Loss: 0.9756255149841309, Lr:0.0001\n",
      "Epoch 48, Step: 13, Loss: 0.4728841185569763, Lr:0.0001\n",
      "Epoch 48, Step: 14, Loss: 0.48318833112716675, Lr:0.0001\n",
      "Epoch 48, Step: 15, Loss: 0.1706453114748001, Lr:0.0001\n",
      "Epoch 48, Step: 16, Loss: 0.01406870223581791, Lr:0.0001\n",
      "Epoch 48, Step: 17, Loss: 0.7588976621627808, Lr:0.0001\n",
      "Epoch 48, Step: 18, Loss: 0.5913024544715881, Lr:0.0001\n",
      "Epoch 48, Step: 19, Loss: 0.20591191947460175, Lr:0.0001\n",
      "Epoch 48, Step: 20, Loss: 0.352765291929245, Lr:0.0001\n",
      "Epoch 48, Step: 21, Loss: 0.1296866238117218, Lr:0.0001\n",
      "Epoch 48, Step: 22, Loss: 0.3106482923030853, Lr:0.0001\n",
      "Epoch 48, Step: 23, Loss: 0.5311923027038574, Lr:0.0001\n",
      "Epoch 48, Step: 24, Loss: 2.0529098510742188, Lr:0.0001\n",
      "Epoch 48, Step: 25, Loss: 1.6492595672607422, Lr:0.0001\n",
      "Epoch 48, Step: 26, Loss: 0.867673397064209, Lr:0.0001\n",
      "Epoch 48, Step: 27, Loss: 1.8612308502197266, Lr:0.0001\n",
      "Epoch 48, Step: 28, Loss: 0.8513026833534241, Lr:0.0001\n",
      "Epoch 48, Step: 29, Loss: 0.6142771244049072, Lr:0.0001\n",
      "Epoch 48, Step: 30, Loss: 1.413488745689392, Lr:0.0001\n",
      "Epoch 48, Step: 31, Loss: 1.195206642150879, Lr:0.0001\n",
      "Epoch 48, Step: 32, Loss: 1.2832709550857544, Lr:0.0001\n",
      "Epoch 48, Step: 33, Loss: 0.43472737073898315, Lr:0.0001\n",
      "Epoch 48, Step: 34, Loss: 2.2575955390930176, Lr:0.0001\n",
      "Epoch 48, Step: 35, Loss: 0.1662093997001648, Lr:0.0001\n",
      "Epoch 48, Step: 36, Loss: 0.4414175748825073, Lr:0.0001\n",
      "Epoch 48, Step: 37, Loss: 0.14502361416816711, Lr:0.0001\n",
      "Epoch 48, Step: 38, Loss: 0.2072361409664154, Lr:0.0001\n",
      "Epoch 48, Step: 39, Loss: 0.2902105748653412, Lr:0.0001\n",
      "Epoch 48, Step: 40, Loss: 0.5856366157531738, Lr:0.0001\n",
      "Epoch 48, Step: 41, Loss: 1.6638630628585815, Lr:0.0001\n",
      "Epoch 48, Step: 42, Loss: 0.26802685856819153, Lr:0.0001\n",
      "Epoch 48, Step: 43, Loss: 0.8065477013587952, Lr:0.0001\n",
      "Epoch 48, Step: 44, Loss: 0.2896762192249298, Lr:0.0001\n",
      "Epoch 48, Step: 45, Loss: 0.6693359017372131, Lr:0.0001\n",
      "Epoch 48, Step: 46, Loss: 0.4128454327583313, Lr:0.0001\n",
      "Epoch 48, Step: 47, Loss: 0.13558214902877808, Lr:0.0001\n",
      "Epoch 48, Step: 48, Loss: 0.35890546441078186, Lr:0.0001\n",
      "Epoch 48, Step: 49, Loss: 0.482746422290802, Lr:0.0001\n",
      "Epoch 48, Step: 50, Loss: 0.5496492981910706, Lr:0.0001\n",
      "Epoch 48, Step: 51, Loss: 0.102442167699337, Lr:0.0001\n",
      "Epoch 48, Step: 52, Loss: 0.14304736256599426, Lr:0.0001\n",
      "Epoch 48, Step: 53, Loss: 0.27612197399139404, Lr:0.0001\n",
      "Epoch 48, Step: 54, Loss: 0.2725716829299927, Lr:0.0001\n",
      "Epoch 48, Step: 55, Loss: 0.02362367883324623, Lr:0.0001\n",
      "Epoch 48, Step: 56, Loss: 1.6826395988464355, Lr:0.0001\n",
      "Epoch 48, Step: 57, Loss: 1.3136800527572632, Lr:0.0001\n",
      "Epoch 48, Step: 58, Loss: 1.4098830223083496, Lr:0.0001\n",
      "Epoch 48, Step: 59, Loss: 0.8765812516212463, Lr:0.0001\n",
      "Epoch 48, Step: 60, Loss: 0.3830794394016266, Lr:0.0001\n",
      "Epoch 48, Step: 61, Loss: 0.18338757753372192, Lr:0.0001\n",
      "Epoch 48, Step: 62, Loss: 0.3505879044532776, Lr:0.0001\n",
      "Epoch 48, Step: 63, Loss: 0.8383966684341431, Lr:0.0001\n",
      "Epoch 48, Step: 64, Loss: 0.40672844648361206, Lr:0.0001\n",
      "Epoch 48, Step: 65, Loss: 2.267035484313965, Lr:0.0001\n",
      "Epoch 48, Step: 66, Loss: 1.7460172176361084, Lr:0.0001\n",
      "Epoch 48, Step: 67, Loss: 0.8082258701324463, Lr:0.0001\n",
      "Epoch 48, Step: 68, Loss: 0.5490661263465881, Lr:0.0001\n",
      "Epoch 48, Step: 69, Loss: 0.40250131487846375, Lr:0.0001\n",
      "Epoch 48, Step: 70, Loss: 0.8503422141075134, Lr:0.0001\n",
      "Epoch 48, Step: 71, Loss: 0.0595509372651577, Lr:0.0001\n",
      "Epoch 48, Step: 72, Loss: 0.14576147496700287, Lr:0.0001\n",
      "Epoch 48, Step: 73, Loss: 0.3377343714237213, Lr:0.0001\n",
      "Epoch 48, Step: 74, Loss: 0.6986819505691528, Lr:0.0001\n",
      "Epoch 48, Step: 75, Loss: 0.45138734579086304, Lr:0.0001\n",
      "Epoch 48, Step: 76, Loss: 0.5952956676483154, Lr:0.0001\n",
      "Epoch 48, Step: 77, Loss: 0.8590916991233826, Lr:0.0001\n",
      "Epoch 48, Step: 78, Loss: 1.5365509986877441, Lr:0.0001\n",
      "Epoch 48, Step: 79, Loss: 0.578815758228302, Lr:0.0001\n",
      "Epoch 48, Step: 80, Loss: 0.6303151845932007, Lr:0.0001\n",
      "Epoch 48, Step: 81, Loss: 0.708127498626709, Lr:0.0001\n",
      "Epoch 48, Step: 82, Loss: 0.39620542526245117, Lr:0.0001\n",
      "Epoch 48, Step: 83, Loss: 0.7428041696548462, Lr:0.0001\n",
      "Epoch 48, Step: 84, Loss: 0.5678439736366272, Lr:0.0001\n",
      "Epoch 48, Step: 85, Loss: 0.64096999168396, Lr:0.0001\n",
      "Epoch 48, Step: 86, Loss: 0.6690253615379333, Lr:0.0001\n",
      "Epoch 48, Step: 87, Loss: 0.7408151626586914, Lr:0.0001\n",
      "Epoch 48, Step: 88, Loss: 0.5601915121078491, Lr:0.0001\n",
      "Epoch 48, Step: 89, Loss: 0.3027673065662384, Lr:0.0001\n",
      "Epoch 48, Step: 90, Loss: 0.39652955532073975, Lr:0.0001\n",
      "Epoch 48, Step: 91, Loss: 0.3539958596229553, Lr:0.0001\n",
      "Epoch 48, Step: 92, Loss: 0.20531342923641205, Lr:0.0001\n",
      "Epoch 48, Step: 93, Loss: 0.33833253383636475, Lr:0.0001\n",
      "Epoch 48, Step: 94, Loss: 0.706039547920227, Lr:0.0001\n",
      "Epoch 48, Step: 95, Loss: 0.7202460765838623, Lr:0.0001\n",
      "Epoch 48, Step: 96, Loss: 0.08512613922357559, Lr:0.0001\n",
      "Epoch 48, Step: 97, Loss: 0.07118088006973267, Lr:0.0001\n",
      "Epoch 48, Step: 98, Loss: 0.36025726795196533, Lr:0.0001\n",
      "Epoch 48, Step: 99, Loss: 0.42800891399383545, Lr:0.0001\n",
      "Epoch 48, Step: 100, Loss: 0.2417651116847992, Lr:0.0001\n",
      "Epoch 48, Step: 101, Loss: 0.09109987318515778, Lr:0.0001\n",
      "Epoch 48, Step: 102, Loss: 0.8448140025138855, Lr:0.0001\n",
      "Epoch 48, Step: 103, Loss: 0.4264359474182129, Lr:0.0001\n",
      "Epoch 48, Step: 104, Loss: 0.3637298047542572, Lr:0.0001\n",
      "Epoch 48, Step: 105, Loss: 0.5995553731918335, Lr:0.0001\n",
      "Epoch 48, Step: 106, Loss: 0.35935667157173157, Lr:0.0001\n",
      "Epoch 48, Step: 107, Loss: 0.15982960164546967, Lr:0.0001\n",
      "Epoch 48, Step: 108, Loss: 0.6058530807495117, Lr:0.0001\n",
      "Epoch 48, Step: 109, Loss: 0.616398811340332, Lr:0.0001\n",
      "Epoch 48, Step: 110, Loss: 0.14562810957431793, Lr:0.0001\n",
      "Epoch 48, Step: 111, Loss: 0.3928850293159485, Lr:0.0001\n",
      "Epoch 48, Step: 112, Loss: 0.2924771010875702, Lr:0.0001\n",
      "Epoch 48, Step: 113, Loss: 0.4078841209411621, Lr:0.0001\n",
      "Epoch 48, Step: 114, Loss: 0.8242971301078796, Lr:0.0001\n",
      "Epoch 48, Step: 115, Loss: 0.5262783169746399, Lr:0.0001\n",
      "Epoch 48, Step: 116, Loss: 2.111295223236084, Lr:0.0001\n",
      "Epoch 48, Step: 117, Loss: 0.4422646462917328, Lr:0.0001\n",
      "Epoch 48, Step: 118, Loss: 1.1110471487045288, Lr:0.0001\n",
      "Epoch 48, Step: 119, Loss: 0.296178936958313, Lr:0.0001\n",
      "Epoch 48, Step: 120, Loss: 0.7804615497589111, Lr:0.0001\n",
      "Epoch 48, Step: 121, Loss: 0.3881130516529083, Lr:0.0001\n",
      "Epoch 48, Step: 122, Loss: 0.8255150318145752, Lr:0.0001\n",
      "Epoch 48, Step: 123, Loss: 0.12817519903182983, Lr:0.0001\n",
      "Epoch 48, Step: 124, Loss: 0.5484426021575928, Lr:0.0001\n",
      "Epoch 48, Step: 125, Loss: 0.37446078658103943, Lr:0.0001\n",
      "Epoch 48, Step: 126, Loss: 1.706435203552246, Lr:0.0001\n",
      "Epoch 48, Step: 127, Loss: 0.01798204891383648, Lr:0.0001\n",
      "Epoch 48, Step: 128, Loss: 0.25517359375953674, Lr:0.0001\n",
      "Epoch 48, Step: 129, Loss: 1.384399652481079, Lr:0.0001\n",
      "Epoch 48, Step: 130, Loss: 0.3359748423099518, Lr:0.0001\n",
      "Epoch 48, Step: 131, Loss: 0.07069317996501923, Lr:0.0001\n",
      "Epoch 48, Step: 132, Loss: 0.932961642742157, Lr:0.0001\n",
      "Epoch 48, Step: 133, Loss: 0.9693547487258911, Lr:0.0001\n",
      "Epoch 48, Step: 134, Loss: 1.4098306894302368, Lr:0.0001\n",
      "Epoch 48, Step: 135, Loss: 0.38001856207847595, Lr:0.0001\n",
      "Epoch 48, Step: 136, Loss: 1.4340603351593018, Lr:0.0001\n",
      "Epoch 48, Step: 137, Loss: 0.8037651777267456, Lr:0.0001\n",
      "Epoch 48, Step: 138, Loss: 0.09971156716346741, Lr:0.0001\n",
      "Epoch 48, Step: 139, Loss: 0.7470545768737793, Lr:0.0001\n",
      "Epoch 48, Step: 140, Loss: 0.41050371527671814, Lr:0.0001\n",
      "Epoch 48, Step: 141, Loss: 0.8935762643814087, Lr:0.0001\n",
      "Epoch 48, Step: 142, Loss: 0.393252432346344, Lr:0.0001\n",
      "Epoch 48, Step: 143, Loss: 0.24393385648727417, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 48\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.0263 (0.0263) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0109 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.3647 (0.6571) acc1: 75.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0077 data: 0.0036 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0078 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.657\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 49, Step: 0, Loss: 0.29441410303115845, Lr:0.0001\n",
      "Epoch 49, Step: 1, Loss: 1.3939690589904785, Lr:0.0001\n",
      "Epoch 49, Step: 2, Loss: 0.7342493534088135, Lr:0.0001\n",
      "Epoch 49, Step: 3, Loss: 0.06154142692685127, Lr:0.0001\n",
      "Epoch 49, Step: 4, Loss: 0.5214861035346985, Lr:0.0001\n",
      "Epoch 49, Step: 5, Loss: 0.5291962027549744, Lr:0.0001\n",
      "Epoch 49, Step: 6, Loss: 0.25062984228134155, Lr:0.0001\n",
      "Epoch 49, Step: 7, Loss: 0.11768104881048203, Lr:0.0001\n",
      "Epoch 49, Step: 8, Loss: 0.2976737916469574, Lr:0.0001\n",
      "Epoch 49, Step: 9, Loss: 0.8655695915222168, Lr:0.0001\n",
      "Epoch 49, Step: 10, Loss: 0.8200811147689819, Lr:0.0001\n",
      "Epoch 49, Step: 11, Loss: 0.18089154362678528, Lr:0.0001\n",
      "Epoch 49, Step: 12, Loss: 0.06350655853748322, Lr:0.0001\n",
      "Epoch 49, Step: 13, Loss: 0.32197654247283936, Lr:0.0001\n",
      "Epoch 49, Step: 14, Loss: 0.6352533102035522, Lr:0.0001\n",
      "Epoch 49, Step: 15, Loss: 0.4686849117279053, Lr:0.0001\n",
      "Epoch 49, Step: 16, Loss: 0.5543829202651978, Lr:0.0001\n",
      "Epoch 49, Step: 17, Loss: 0.2374776303768158, Lr:0.0001\n",
      "Epoch 49, Step: 18, Loss: 0.2215176373720169, Lr:0.0001\n",
      "Epoch 49, Step: 19, Loss: 0.810113251209259, Lr:0.0001\n",
      "Epoch 49, Step: 20, Loss: 0.5653404593467712, Lr:0.0001\n",
      "Epoch 49, Step: 21, Loss: 0.3778685927391052, Lr:0.0001\n",
      "Epoch 49, Step: 22, Loss: 0.6067484021186829, Lr:0.0001\n",
      "Epoch 49, Step: 23, Loss: 0.6297621130943298, Lr:0.0001\n",
      "Epoch 49, Step: 24, Loss: 0.3624423146247864, Lr:0.0001\n",
      "Epoch 49, Step: 25, Loss: 0.34609881043434143, Lr:0.0001\n",
      "Epoch 49, Step: 26, Loss: 0.9562005400657654, Lr:0.0001\n",
      "Epoch 49, Step: 27, Loss: 0.6099492907524109, Lr:0.0001\n",
      "Epoch 49, Step: 28, Loss: 0.42822784185409546, Lr:0.0001\n",
      "Epoch 49, Step: 29, Loss: 0.2146034836769104, Lr:0.0001\n",
      "Epoch 49, Step: 30, Loss: 0.4334281086921692, Lr:0.0001\n",
      "Epoch 49, Step: 31, Loss: 0.38808363676071167, Lr:0.0001\n",
      "Epoch 49, Step: 32, Loss: 0.45521044731140137, Lr:0.0001\n",
      "Epoch 49, Step: 33, Loss: 0.37628620862960815, Lr:0.0001\n",
      "Epoch 49, Step: 34, Loss: 0.577960193157196, Lr:0.0001\n",
      "Epoch 49, Step: 35, Loss: 1.043243169784546, Lr:0.0001\n",
      "Epoch 49, Step: 36, Loss: 0.10122917592525482, Lr:0.0001\n",
      "Epoch 49, Step: 37, Loss: 0.16676664352416992, Lr:0.0001\n",
      "Epoch 49, Step: 38, Loss: 1.100204586982727, Lr:0.0001\n",
      "Epoch 49, Step: 39, Loss: 0.20441260933876038, Lr:0.0001\n",
      "Epoch 49, Step: 40, Loss: 0.7005041837692261, Lr:0.0001\n",
      "Epoch 49, Step: 41, Loss: 0.060575250536203384, Lr:0.0001\n",
      "Epoch 49, Step: 42, Loss: 2.5559253692626953, Lr:0.0001\n",
      "Epoch 49, Step: 43, Loss: 0.3660131096839905, Lr:0.0001\n",
      "Epoch 49, Step: 44, Loss: 1.6583176851272583, Lr:0.0001\n",
      "Epoch 49, Step: 45, Loss: 0.6436367630958557, Lr:0.0001\n",
      "Epoch 49, Step: 46, Loss: 0.34998267889022827, Lr:0.0001\n",
      "Epoch 49, Step: 47, Loss: 0.6499478220939636, Lr:0.0001\n",
      "Epoch 49, Step: 48, Loss: 0.27866697311401367, Lr:0.0001\n",
      "Epoch 49, Step: 49, Loss: 0.7808488011360168, Lr:0.0001\n",
      "Epoch 49, Step: 50, Loss: 0.6893286108970642, Lr:0.0001\n",
      "Epoch 49, Step: 51, Loss: 1.5856480598449707, Lr:0.0001\n",
      "Epoch 49, Step: 52, Loss: 1.3175162076950073, Lr:0.0001\n",
      "Epoch 49, Step: 53, Loss: 0.8653756380081177, Lr:0.0001\n",
      "Epoch 49, Step: 54, Loss: 0.4016919434070587, Lr:0.0001\n",
      "Epoch 49, Step: 55, Loss: 0.9500023722648621, Lr:0.0001\n",
      "Epoch 49, Step: 56, Loss: 0.5591046214103699, Lr:0.0001\n",
      "Epoch 49, Step: 57, Loss: 0.39885449409484863, Lr:0.0001\n",
      "Epoch 49, Step: 58, Loss: 0.6377415657043457, Lr:0.0001\n",
      "Epoch 49, Step: 59, Loss: 0.5057281255722046, Lr:0.0001\n",
      "Epoch 49, Step: 60, Loss: 0.1434912383556366, Lr:0.0001\n",
      "Epoch 49, Step: 61, Loss: 0.8426117300987244, Lr:0.0001\n",
      "Epoch 49, Step: 62, Loss: 0.5049952268600464, Lr:0.0001\n",
      "Epoch 49, Step: 63, Loss: 0.31024619936943054, Lr:0.0001\n",
      "Epoch 49, Step: 64, Loss: 0.22246479988098145, Lr:0.0001\n",
      "Epoch 49, Step: 65, Loss: 0.29791995882987976, Lr:0.0001\n",
      "Epoch 49, Step: 66, Loss: 0.026960814371705055, Lr:0.0001\n",
      "Epoch 49, Step: 67, Loss: 0.9464470148086548, Lr:0.0001\n",
      "Epoch 49, Step: 68, Loss: 0.7593417763710022, Lr:0.0001\n",
      "Epoch 49, Step: 69, Loss: 0.420335590839386, Lr:0.0001\n",
      "Epoch 49, Step: 70, Loss: 0.11476965993642807, Lr:0.0001\n",
      "Epoch 49, Step: 71, Loss: 0.1592218279838562, Lr:0.0001\n",
      "Epoch 49, Step: 72, Loss: 0.39800724387168884, Lr:0.0001\n",
      "Epoch 49, Step: 73, Loss: 0.6027443408966064, Lr:0.0001\n",
      "Epoch 49, Step: 74, Loss: 0.4024331867694855, Lr:0.0001\n",
      "Epoch 49, Step: 75, Loss: 0.2524687647819519, Lr:0.0001\n",
      "Epoch 49, Step: 76, Loss: 0.02573341690003872, Lr:0.0001\n",
      "Epoch 49, Step: 77, Loss: 0.048799335956573486, Lr:0.0001\n",
      "Epoch 49, Step: 78, Loss: 0.37091436982154846, Lr:0.0001\n",
      "Epoch 49, Step: 79, Loss: 0.46489444375038147, Lr:0.0001\n",
      "Epoch 49, Step: 80, Loss: 0.5560933947563171, Lr:0.0001\n",
      "Epoch 49, Step: 81, Loss: 0.0963059663772583, Lr:0.0001\n",
      "Epoch 49, Step: 82, Loss: 1.3736165761947632, Lr:0.0001\n",
      "Epoch 49, Step: 83, Loss: 0.5312589406967163, Lr:0.0001\n",
      "Epoch 49, Step: 84, Loss: 1.5570627450942993, Lr:0.0001\n",
      "Epoch 49, Step: 85, Loss: 0.5472321510314941, Lr:0.0001\n",
      "Epoch 49, Step: 86, Loss: 0.7333395481109619, Lr:0.0001\n",
      "Epoch 49, Step: 87, Loss: 1.6809237003326416, Lr:0.0001\n",
      "Epoch 49, Step: 88, Loss: 0.5398499369621277, Lr:0.0001\n",
      "Epoch 49, Step: 89, Loss: 0.22051024436950684, Lr:0.0001\n",
      "Epoch 49, Step: 90, Loss: 0.5559346675872803, Lr:0.0001\n",
      "Epoch 49, Step: 91, Loss: 0.9026115536689758, Lr:0.0001\n",
      "Epoch 49, Step: 92, Loss: 0.5275832414627075, Lr:0.0001\n",
      "Epoch 49, Step: 93, Loss: 0.08853618800640106, Lr:0.0001\n",
      "Epoch 49, Step: 94, Loss: 0.0759500190615654, Lr:0.0001\n",
      "Epoch 49, Step: 95, Loss: 1.5583409070968628, Lr:0.0001\n",
      "Epoch 49, Step: 96, Loss: 0.24616461992263794, Lr:0.0001\n",
      "Epoch 49, Step: 97, Loss: 0.4694942533969879, Lr:0.0001\n",
      "Epoch 49, Step: 98, Loss: 0.31148436665534973, Lr:0.0001\n",
      "Epoch 49, Step: 99, Loss: 2.3214728832244873, Lr:0.0001\n",
      "Epoch 49, Step: 100, Loss: 0.509223997592926, Lr:0.0001\n",
      "Epoch 49, Step: 101, Loss: 0.2957606613636017, Lr:0.0001\n",
      "Epoch 49, Step: 102, Loss: 0.5437626838684082, Lr:0.0001\n",
      "Epoch 49, Step: 103, Loss: 0.42807134985923767, Lr:0.0001\n",
      "Epoch 49, Step: 104, Loss: 0.10938556492328644, Lr:0.0001\n",
      "Epoch 49, Step: 105, Loss: 0.03017737902700901, Lr:0.0001\n",
      "Epoch 49, Step: 106, Loss: 0.67659592628479, Lr:0.0001\n",
      "Epoch 49, Step: 107, Loss: 0.05731024220585823, Lr:0.0001\n",
      "Epoch 49, Step: 108, Loss: 1.2742537260055542, Lr:0.0001\n",
      "Epoch 49, Step: 109, Loss: 0.08150514960289001, Lr:0.0001\n",
      "Epoch 49, Step: 110, Loss: 0.4509512186050415, Lr:0.0001\n",
      "Epoch 49, Step: 111, Loss: 0.5228570699691772, Lr:0.0001\n",
      "Epoch 49, Step: 112, Loss: 1.0976762771606445, Lr:0.0001\n",
      "Epoch 49, Step: 113, Loss: 0.5322130918502808, Lr:0.0001\n",
      "Epoch 49, Step: 114, Loss: 0.04633009061217308, Lr:0.0001\n",
      "Epoch 49, Step: 115, Loss: 0.5533515214920044, Lr:0.0001\n",
      "Epoch 49, Step: 116, Loss: 0.24181431531906128, Lr:0.0001\n",
      "Epoch 49, Step: 117, Loss: 1.0306719541549683, Lr:0.0001\n",
      "Epoch 49, Step: 118, Loss: 1.5884113311767578, Lr:0.0001\n",
      "Epoch 49, Step: 119, Loss: 0.598480224609375, Lr:0.0001\n",
      "Epoch 49, Step: 120, Loss: 0.39022788405418396, Lr:0.0001\n",
      "Epoch 49, Step: 121, Loss: 1.4477465152740479, Lr:0.0001\n",
      "Epoch 49, Step: 122, Loss: 0.4186176657676697, Lr:0.0001\n",
      "Epoch 49, Step: 123, Loss: 0.05017350614070892, Lr:0.0001\n",
      "Epoch 49, Step: 124, Loss: 0.26190462708473206, Lr:0.0001\n",
      "Epoch 49, Step: 125, Loss: 0.9589855670928955, Lr:0.0001\n",
      "Epoch 49, Step: 126, Loss: 0.4767000079154968, Lr:0.0001\n",
      "Epoch 49, Step: 127, Loss: 0.21262533962726593, Lr:0.0001\n",
      "Epoch 49, Step: 128, Loss: 0.5979997515678406, Lr:0.0001\n",
      "Epoch 49, Step: 129, Loss: 0.49766767024993896, Lr:0.0001\n",
      "Epoch 49, Step: 130, Loss: 1.3308591842651367, Lr:0.0001\n",
      "Epoch 49, Step: 131, Loss: 0.33512675762176514, Lr:0.0001\n",
      "Epoch 49, Step: 132, Loss: 0.5111919045448303, Lr:0.0001\n",
      "Epoch 49, Step: 133, Loss: 0.4171118140220642, Lr:0.0001\n",
      "Epoch 49, Step: 134, Loss: 0.6055850982666016, Lr:0.0001\n",
      "Epoch 49, Step: 135, Loss: 0.510671854019165, Lr:0.0001\n",
      "Epoch 49, Step: 136, Loss: 0.6063477396965027, Lr:0.0001\n",
      "Epoch 49, Step: 137, Loss: 1.4676107168197632, Lr:0.0001\n",
      "Epoch 49, Step: 138, Loss: 0.02420041151344776, Lr:0.0001\n",
      "Epoch 49, Step: 139, Loss: 0.4302311837673187, Lr:0.0001\n",
      "Epoch 49, Step: 140, Loss: 0.5792225003242493, Lr:0.0001\n",
      "Epoch 49, Step: 141, Loss: 0.015525774098932743, Lr:0.0001\n",
      "Epoch 49, Step: 142, Loss: 0.920587420463562, Lr:0.0001\n",
      "Epoch 49, Step: 143, Loss: 0.4573505222797394, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "Epoch 49\n",
      "length of data_loader_train is 144\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:00 loss: 0.1152 (0.1152) acc1: 100.0000 (100.0000) acc5: 100.0000 (100.0000) time: 0.0120 data: 0.0059 max mem: 220\n",
      "Test: [8/9] eta: 0:00:00 loss: 0.4530 (0.5770) acc1: 75.0000 (78.7879) acc5: 100.0000 (100.0000) time: 0.0084 data: 0.0038 max mem: 220\n",
      "Test: Total time: 0:00:00 (0.0084 s / it)\n",
      "* Acc@1 78.788 Acc@5 100.000 loss 0.577\n",
      "Accuracy of the network on the 33 test image: 78.8%\n",
      "Training...\n",
      "log_dir: ./resnet_output_dir\n",
      "Epoch 50, Step: 0, Loss: 0.10387329012155533, Lr:0.0001\n",
      "Epoch 50, Step: 1, Loss: 0.7030757665634155, Lr:0.0001\n",
      "Epoch 50, Step: 2, Loss: 0.5100511312484741, Lr:0.0001\n",
      "Epoch 50, Step: 3, Loss: 0.7867148518562317, Lr:0.0001\n",
      "Epoch 50, Step: 4, Loss: 0.738059937953949, Lr:0.0001\n",
      "Epoch 50, Step: 5, Loss: 0.3954019546508789, Lr:0.0001\n",
      "Epoch 50, Step: 6, Loss: 0.5600893497467041, Lr:0.0001\n",
      "Epoch 50, Step: 7, Loss: 0.45419251918792725, Lr:0.0001\n",
      "Epoch 50, Step: 8, Loss: 0.4683518409729004, Lr:0.0001\n",
      "Epoch 50, Step: 9, Loss: 0.4187747836112976, Lr:0.0001\n",
      "Epoch 50, Step: 10, Loss: 0.4447667598724365, Lr:0.0001\n",
      "Epoch 50, Step: 11, Loss: 0.5465113520622253, Lr:0.0001\n",
      "Epoch 50, Step: 12, Loss: 0.5804444551467896, Lr:0.0001\n",
      "Epoch 50, Step: 13, Loss: 0.4124581515789032, Lr:0.0001\n",
      "Epoch 50, Step: 14, Loss: 1.2954158782958984, Lr:0.0001\n",
      "Epoch 50, Step: 15, Loss: 0.005952559411525726, Lr:0.0001\n",
      "Epoch 50, Step: 16, Loss: 2.3428685665130615, Lr:0.0001\n",
      "Epoch 50, Step: 17, Loss: 0.2708514928817749, Lr:0.0001\n",
      "Epoch 50, Step: 18, Loss: 0.6555326581001282, Lr:0.0001\n",
      "Epoch 50, Step: 19, Loss: 0.3079807162284851, Lr:0.0001\n",
      "Epoch 50, Step: 20, Loss: 0.3714284300804138, Lr:0.0001\n",
      "Epoch 50, Step: 21, Loss: 1.7482322454452515, Lr:0.0001\n",
      "Epoch 50, Step: 22, Loss: 0.6549866795539856, Lr:0.0001\n",
      "Epoch 50, Step: 23, Loss: 0.0897645428776741, Lr:0.0001\n",
      "Epoch 50, Step: 24, Loss: 0.40894705057144165, Lr:0.0001\n",
      "Epoch 50, Step: 25, Loss: 0.46425941586494446, Lr:0.0001\n",
      "Epoch 50, Step: 26, Loss: 0.33662912249565125, Lr:0.0001\n",
      "Epoch 50, Step: 27, Loss: 0.3644494116306305, Lr:0.0001\n",
      "Epoch 50, Step: 28, Loss: 1.9916467666625977, Lr:0.0001\n",
      "Epoch 50, Step: 29, Loss: 0.38195472955703735, Lr:0.0001\n",
      "Epoch 50, Step: 30, Loss: 0.2626664340496063, Lr:0.0001\n",
      "Epoch 50, Step: 31, Loss: 0.6088663935661316, Lr:0.0001\n",
      "Epoch 50, Step: 32, Loss: 1.0717450380325317, Lr:0.0001\n",
      "Epoch 50, Step: 33, Loss: 0.24874913692474365, Lr:0.0001\n",
      "Epoch 50, Step: 34, Loss: 0.3694588541984558, Lr:0.0001\n",
      "Epoch 50, Step: 35, Loss: 0.005427194759249687, Lr:0.0001\n",
      "Epoch 50, Step: 36, Loss: 1.228367805480957, Lr:0.0001\n",
      "Epoch 50, Step: 37, Loss: 0.07910032570362091, Lr:0.0001\n",
      "Epoch 50, Step: 38, Loss: 0.2984898090362549, Lr:0.0001\n",
      "Epoch 50, Step: 39, Loss: 0.451666921377182, Lr:0.0001\n",
      "Epoch 50, Step: 40, Loss: 1.3878976106643677, Lr:0.0001\n",
      "Epoch 50, Step: 41, Loss: 0.26647084951400757, Lr:0.0001\n",
      "Epoch 50, Step: 42, Loss: 0.9006365537643433, Lr:0.0001\n",
      "Epoch 50, Step: 43, Loss: 0.29092973470687866, Lr:0.0001\n",
      "Epoch 50, Step: 44, Loss: 1.3392016887664795, Lr:0.0001\n",
      "Epoch 50, Step: 45, Loss: 0.3963247835636139, Lr:0.0001\n",
      "Epoch 50, Step: 46, Loss: 1.109692096710205, Lr:0.0001\n",
      "Epoch 50, Step: 47, Loss: 0.4967321753501892, Lr:0.0001\n",
      "Epoch 50, Step: 48, Loss: 0.1050226092338562, Lr:0.0001\n",
      "Epoch 50, Step: 49, Loss: 0.44351765513420105, Lr:0.0001\n",
      "Epoch 50, Step: 50, Loss: 0.2833161950111389, Lr:0.0001\n",
      "Epoch 50, Step: 51, Loss: 0.22216476500034332, Lr:0.0001\n",
      "Epoch 50, Step: 52, Loss: 1.0066930055618286, Lr:0.0001\n",
      "Epoch 50, Step: 53, Loss: 0.9608558416366577, Lr:0.0001\n",
      "Epoch 50, Step: 54, Loss: 0.22793683409690857, Lr:0.0001\n",
      "Epoch 50, Step: 55, Loss: 0.2427239716053009, Lr:0.0001\n",
      "Epoch 50, Step: 56, Loss: 0.03607138618826866, Lr:0.0001\n",
      "Epoch 50, Step: 57, Loss: 0.4396285116672516, Lr:0.0001\n",
      "Epoch 50, Step: 58, Loss: 0.3459106683731079, Lr:0.0001\n",
      "Epoch 50, Step: 59, Loss: 2.3436880111694336, Lr:0.0001\n",
      "Epoch 50, Step: 60, Loss: 1.0493667125701904, Lr:0.0001\n",
      "Epoch 50, Step: 61, Loss: 0.5385748744010925, Lr:0.0001\n",
      "Epoch 50, Step: 62, Loss: 0.4991661608219147, Lr:0.0001\n",
      "Epoch 50, Step: 63, Loss: 0.14164240658283234, Lr:0.0001\n",
      "Epoch 50, Step: 64, Loss: 1.0474131107330322, Lr:0.0001\n",
      "Epoch 50, Step: 65, Loss: 0.10575378686189651, Lr:0.0001\n",
      "Epoch 50, Step: 66, Loss: 0.39411938190460205, Lr:0.0001\n",
      "Epoch 50, Step: 67, Loss: 0.4655667543411255, Lr:0.0001\n",
      "Epoch 50, Step: 68, Loss: 0.4400162100791931, Lr:0.0001\n",
      "Epoch 50, Step: 69, Loss: 0.3385869264602661, Lr:0.0001\n",
      "Epoch 50, Step: 70, Loss: 0.004758537281304598, Lr:0.0001\n",
      "Epoch 50, Step: 71, Loss: 0.7483716011047363, Lr:0.0001\n",
      "Epoch 50, Step: 72, Loss: 0.32705363631248474, Lr:0.0001\n",
      "Epoch 50, Step: 73, Loss: 0.06736601144075394, Lr:0.0001\n",
      "Epoch 50, Step: 74, Loss: 0.19417572021484375, Lr:0.0001\n",
      "Epoch 50, Step: 75, Loss: 0.8085566759109497, Lr:0.0001\n",
      "Epoch 50, Step: 76, Loss: 0.4415796399116516, Lr:0.0001\n",
      "Epoch 50, Step: 77, Loss: 0.4522475004196167, Lr:0.0001\n",
      "Epoch 50, Step: 78, Loss: 0.5158225893974304, Lr:0.0001\n",
      "Epoch 50, Step: 79, Loss: 0.08320293575525284, Lr:0.0001\n",
      "Epoch 50, Step: 80, Loss: 0.514756977558136, Lr:0.0001\n",
      "Epoch 50, Step: 81, Loss: 0.24850812554359436, Lr:0.0001\n",
      "Epoch 50, Step: 82, Loss: 0.33986666798591614, Lr:0.0001\n",
      "Epoch 50, Step: 83, Loss: 1.9285242557525635, Lr:0.0001\n",
      "Epoch 50, Step: 84, Loss: 0.3763582408428192, Lr:0.0001\n",
      "Epoch 50, Step: 85, Loss: 0.4038141667842865, Lr:0.0001\n",
      "Epoch 50, Step: 86, Loss: 0.5196707844734192, Lr:0.0001\n",
      "Epoch 50, Step: 87, Loss: 0.2292923629283905, Lr:0.0001\n",
      "Epoch 50, Step: 88, Loss: 0.17356568574905396, Lr:0.0001\n",
      "Epoch 50, Step: 89, Loss: 1.011579155921936, Lr:0.0001\n",
      "Epoch 50, Step: 90, Loss: 0.46488669514656067, Lr:0.0001\n",
      "Epoch 50, Step: 91, Loss: 0.3258373737335205, Lr:0.0001\n",
      "Epoch 50, Step: 92, Loss: 0.6202975511550903, Lr:0.0001\n",
      "Epoch 50, Step: 93, Loss: 1.2348992824554443, Lr:0.0001\n",
      "Epoch 50, Step: 94, Loss: 1.6278014183044434, Lr:0.0001\n",
      "Epoch 50, Step: 95, Loss: 1.1093193292617798, Lr:0.0001\n",
      "Epoch 50, Step: 96, Loss: 0.13544628024101257, Lr:0.0001\n",
      "Epoch 50, Step: 97, Loss: 0.34046614170074463, Lr:0.0001\n",
      "Epoch 50, Step: 98, Loss: 2.329136371612549, Lr:0.0001\n",
      "Epoch 50, Step: 99, Loss: 0.13930769264698029, Lr:0.0001\n",
      "Epoch 50, Step: 100, Loss: 0.09461208432912827, Lr:0.0001\n",
      "Epoch 50, Step: 101, Loss: 0.31413793563842773, Lr:0.0001\n",
      "Epoch 50, Step: 102, Loss: 0.3542889356613159, Lr:0.0001\n",
      "Epoch 50, Step: 103, Loss: 0.5220906734466553, Lr:0.0001\n",
      "Epoch 50, Step: 104, Loss: 0.06987830251455307, Lr:0.0001\n",
      "Epoch 50, Step: 105, Loss: 0.886524498462677, Lr:0.0001\n",
      "Epoch 50, Step: 106, Loss: 0.12515851855278015, Lr:0.0001\n",
      "Epoch 50, Step: 107, Loss: 0.2705860733985901, Lr:0.0001\n",
      "Epoch 50, Step: 108, Loss: 0.8239631056785583, Lr:0.0001\n",
      "Epoch 50, Step: 109, Loss: 0.7235732674598694, Lr:0.0001\n",
      "Epoch 50, Step: 110, Loss: 1.1570701599121094, Lr:0.0001\n",
      "Epoch 50, Step: 111, Loss: 0.17746073007583618, Lr:0.0001\n",
      "Epoch 50, Step: 112, Loss: 0.45813751220703125, Lr:0.0001\n",
      "Epoch 50, Step: 113, Loss: 0.10727660357952118, Lr:0.0001\n",
      "Epoch 50, Step: 114, Loss: 0.3964868187904358, Lr:0.0001\n",
      "Epoch 50, Step: 115, Loss: 0.3137212097644806, Lr:0.0001\n",
      "Epoch 50, Step: 116, Loss: 0.330581933259964, Lr:0.0001\n",
      "Epoch 50, Step: 117, Loss: 0.7612432837486267, Lr:0.0001\n",
      "Epoch 50, Step: 118, Loss: 0.3256922662258148, Lr:0.0001\n",
      "Epoch 50, Step: 119, Loss: 0.8126343488693237, Lr:0.0001\n",
      "Epoch 50, Step: 120, Loss: 0.28857383131980896, Lr:0.0001\n",
      "Epoch 50, Step: 121, Loss: 0.32418307662010193, Lr:0.0001\n",
      "Epoch 50, Step: 122, Loss: 0.5772223472595215, Lr:0.0001\n",
      "Epoch 50, Step: 123, Loss: 0.41478171944618225, Lr:0.0001\n",
      "Epoch 50, Step: 124, Loss: 0.07125832140445709, Lr:0.0001\n",
      "Epoch 50, Step: 125, Loss: 1.2212879657745361, Lr:0.0001\n",
      "Epoch 50, Step: 126, Loss: 0.07473868876695633, Lr:0.0001\n",
      "Epoch 50, Step: 127, Loss: 0.9095647931098938, Lr:0.0001\n",
      "Epoch 50, Step: 128, Loss: 0.05141595005989075, Lr:0.0001\n",
      "Epoch 50, Step: 129, Loss: 0.2558647096157074, Lr:0.0001\n",
      "Epoch 50, Step: 130, Loss: 1.385026454925537, Lr:0.0001\n",
      "Epoch 50, Step: 131, Loss: 0.24156467616558075, Lr:0.0001\n",
      "Epoch 50, Step: 132, Loss: 0.4034911096096039, Lr:0.0001\n",
      "Epoch 50, Step: 133, Loss: 0.9673057794570923, Lr:0.0001\n",
      "Epoch 50, Step: 134, Loss: 0.1288176029920578, Lr:0.0001\n",
      "Epoch 50, Step: 135, Loss: 0.017380930483341217, Lr:0.0001\n",
      "Epoch 50, Step: 136, Loss: 0.33359295129776, Lr:0.0001\n",
      "Epoch 50, Step: 137, Loss: 0.8078018426895142, Lr:0.0001\n",
      "Epoch 50, Step: 138, Loss: 0.19419144093990326, Lr:0.0001\n",
      "Epoch 50, Step: 139, Loss: 0.7367035746574402, Lr:0.0001\n",
      "Epoch 50, Step: 140, Loss: 1.3415027856826782, Lr:0.0001\n",
      "Epoch 50, Step: 141, Loss: 0.041632030159235, Lr:0.0001\n",
      "Epoch 50, Step: 142, Loss: 0.9778302907943726, Lr:0.0001\n",
      "Epoch 50, Step: 143, Loss: 0.7970306873321533, Lr:0.0001\n",
      "Saving checkpoints...\n",
      "best acc is 84.84848484848484 at 13 epoch\n"
     ]
    }
   ],
   "source": [
    "%run resnet_baseline/train.py --mode train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c72039-fdb5-4553-89c3-6f00ab3c7579",
   "metadata": {},
   "source": [
    "# 使用tensorboard查看训练结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610d80c-c3c7-4fa6-9617-5493bec753d1",
   "metadata": {},
   "source": [
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfd9eb6-12b4-4e3d-9554-19b8d412d531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./resnet_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4493fc4e-7aaf-4db0-9b6f-3d584b87dc17",
   "metadata": {},
   "source": [
    "# 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73606596-cc45-42c2-844e-4f24c62305ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\1y9UDX_Annotation0.jpg\n",
      "score is 0.8604089021682739, class id is 0, class name is T0\n",
      "1y9UDX_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\4mF7tL_Annotation0.jpg\n",
      "score is 0.8026546835899353, class id is 2, class name is T2\n",
      "4mF7tL_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\4mF7tL_Annotation1.jpg\n",
      "score is 0.7501621246337891, class id is 2, class name is T2\n",
      "4mF7tL_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\4mF7tL_Annotation2.jpg\n",
      "score is 0.6778193116188049, class id is 2, class name is T2\n",
      "4mF7tL_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\4mF7tL_Annotation3.jpg\n",
      "score is 0.6102361083030701, class id is 2, class name is T2\n",
      "4mF7tL_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\B5MAwD_Annotation0.jpg\n",
      "score is 0.3408605754375458, class id is 0, class name is T0\n",
      "B5MAwD_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\B5MAwD_Annotation1.jpg\n",
      "score is 0.29286059737205505, class id is 0, class name is T0\n",
      "B5MAwD_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\B5MAwD_Annotation2.jpg\n",
      "score is 0.4405730962753296, class id is 0, class name is T0\n",
      "B5MAwD_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\B5MAwD_Annotation3.jpg\n",
      "score is 0.4505935311317444, class id is 2, class name is T2\n",
      "B5MAwD_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\cRnBLx_Annotation0.jpg\n",
      "score is 0.41999393701553345, class id is 2, class name is T2\n",
      "cRnBLx_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\cRnBLx_Annotation1.jpg\n",
      "score is 0.46950244903564453, class id is 2, class name is T2\n",
      "cRnBLx_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\cRnBLx_Annotation2.jpg\n",
      "score is 0.3698562681674957, class id is 2, class name is T2\n",
      "cRnBLx_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\cRnBLx_Annotation3.jpg\n",
      "score is 0.3506837487220764, class id is 1, class name is T1\n",
      "cRnBLx_Annotation3\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\gfozad_Annotation0.jpg\n",
      "score is 0.600368082523346, class id is 2, class name is T2\n",
      "gfozad_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\i0v7lq_Annotation0.jpg\n",
      "score is 0.3602027893066406, class id is 2, class name is T2\n",
      "i0v7lq_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\i0v7lq_Annotation1.jpg\n",
      "score is 0.4654320776462555, class id is 0, class name is T0\n",
      "i0v7lq_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\iQUs4h_Annotation0.jpg\n",
      "score is 0.6745477318763733, class id is 2, class name is T2\n",
      "iQUs4h_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\iQUs4h_Annotation1.jpg\n",
      "score is 0.4244392216205597, class id is 2, class name is T2\n",
      "iQUs4h_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Jw1GFk_Annotation0.jpg\n",
      "score is 0.766180157661438, class id is 0, class name is T0\n",
      "Jw1GFk_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Jw1GFk_Annotation1.jpg\n",
      "score is 0.522878110408783, class id is 2, class name is T2\n",
      "Jw1GFk_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Jw1GFk_Annotation2.jpg\n",
      "score is 0.6241782903671265, class id is 2, class name is T2\n",
      "Jw1GFk_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\K2f9Nb_Annotation0.jpg\n",
      "score is 0.6826247572898865, class id is 2, class name is T2\n",
      "K2f9Nb_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\K2f9Nb_Annotation1.jpg\n",
      "score is 0.5125839710235596, class id is 2, class name is T2\n",
      "K2f9Nb_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kOUcud_Annotation0.jpg\n",
      "score is 0.5212225317955017, class id is 2, class name is T2\n",
      "kOUcud_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kOUcud_Annotation1.jpg\n",
      "score is 0.48137366771698, class id is 2, class name is T2\n",
      "kOUcud_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kOUcud_Annotation2.jpg\n",
      "score is 0.3161926865577698, class id is 2, class name is T2\n",
      "kOUcud_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kOUcud_Annotation3.jpg\n",
      "score is 0.5002437829971313, class id is 2, class name is T2\n",
      "kOUcud_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kQPIhs_Annotation0.jpg\n",
      "score is 0.5281005501747131, class id is 2, class name is T2\n",
      "kQPIhs_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kQPIhs_Annotation1.jpg\n",
      "score is 0.7598811984062195, class id is 2, class name is T2\n",
      "kQPIhs_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kQPIhs_Annotation2.jpg\n",
      "score is 0.502395510673523, class id is 2, class name is T2\n",
      "kQPIhs_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\kQPIhs_Annotation3.jpg\n",
      "score is 0.4416569173336029, class id is 2, class name is T2\n",
      "kQPIhs_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\KRuNrD_Annotation0.jpg\n",
      "score is 0.5360820293426514, class id is 2, class name is T2\n",
      "KRuNrD_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\KRuNrD_Annotation1.jpg\n",
      "score is 0.7535290718078613, class id is 2, class name is T2\n",
      "KRuNrD_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\KRuNrD_Annotation2.jpg\n",
      "score is 0.38305896520614624, class id is 0, class name is T0\n",
      "KRuNrD_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\KRuNrD_Annotation3.jpg\n",
      "score is 0.3704889714717865, class id is 2, class name is T2\n",
      "KRuNrD_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lc3OZp_Annotation0.jpg\n",
      "score is 0.34509244561195374, class id is 0, class name is T0\n",
      "lc3OZp_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lc3OZp_Annotation1.jpg\n",
      "score is 0.5454636216163635, class id is 0, class name is T0\n",
      "lc3OZp_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lg0CwS_Annotation0.jpg\n",
      "score is 0.4771900177001953, class id is 2, class name is T2\n",
      "lg0CwS_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lg0CwS_Annotation1.jpg\n",
      "score is 0.32298049330711365, class id is 0, class name is T0\n",
      "lg0CwS_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lg0CwS_Annotation2.jpg\n",
      "score is 0.3883880078792572, class id is 0, class name is T0\n",
      "lg0CwS_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lg0CwS_Annotation3.jpg\n",
      "score is 0.3150528371334076, class id is 2, class name is T2\n",
      "lg0CwS_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lO9MmB_Annotation0.jpg\n",
      "score is 0.6929218769073486, class id is 2, class name is T2\n",
      "lO9MmB_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lO9MmB_Annotation1.jpg\n",
      "score is 0.7194849252700806, class id is 2, class name is T2\n",
      "lO9MmB_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\lO9MmB_Annotation2.jpg\n",
      "score is 0.5451630353927612, class id is 0, class name is T0\n",
      "lO9MmB_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LRJwmg_Annotation0.jpg\n",
      "score is 0.43846312165260315, class id is 2, class name is T2\n",
      "LRJwmg_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LRJwmg_Annotation1.jpg\n",
      "score is 0.5368027687072754, class id is 0, class name is T0\n",
      "LRJwmg_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LRJwmg_Annotation2.jpg\n",
      "score is 0.5569707155227661, class id is 2, class name is T2\n",
      "LRJwmg_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LrKXU4_Annotation0.jpg\n",
      "score is 0.3643089830875397, class id is 2, class name is T2\n",
      "LrKXU4_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LrKXU4_Annotation1.jpg\n",
      "score is 0.4560556411743164, class id is 2, class name is T2\n",
      "LrKXU4_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LrKXU4_Annotation2.jpg\n",
      "score is 0.6014336943626404, class id is 2, class name is T2\n",
      "LrKXU4_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\LrKXU4_Annotation3.jpg\n",
      "score is 0.4822564423084259, class id is 0, class name is T0\n",
      "LrKXU4_Annotation3\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\m14lJk_Annotation0.jpg\n",
      "score is 0.5784846544265747, class id is 2, class name is T2\n",
      "m14lJk_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\MHNQ4e_Annotation0.jpg\n",
      "score is 0.3096276521682739, class id is 0, class name is T0\n",
      "MHNQ4e_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\MHNQ4e_Annotation1.jpg\n",
      "score is 0.44627851247787476, class id is 2, class name is T2\n",
      "MHNQ4e_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\miuArM_Annotation0.jpg\n",
      "score is 0.42945948243141174, class id is 0, class name is T0\n",
      "miuArM_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\miuArM_Annotation1.jpg\n",
      "score is 0.3377687633037567, class id is 2, class name is T2\n",
      "miuArM_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\miuArM_Annotation2.jpg\n",
      "score is 0.659576952457428, class id is 2, class name is T2\n",
      "miuArM_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\N4HDTG_Annotation0.jpg\n",
      "score is 0.6283574104309082, class id is 2, class name is T2\n",
      "N4HDTG_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\N4HDTG_Annotation1.jpg\n",
      "score is 0.36873215436935425, class id is 0, class name is T0\n",
      "N4HDTG_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\N4HDTG_Annotation2.jpg\n",
      "score is 0.40767040848731995, class id is 2, class name is T2\n",
      "N4HDTG_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\NGynsT_Annotation0.jpg\n",
      "score is 0.630275547504425, class id is 2, class name is T2\n",
      "NGynsT_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\NGynsT_Annotation1.jpg\n",
      "score is 0.4795374870300293, class id is 2, class name is T2\n",
      "NGynsT_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\NGynsT_Annotation2.jpg\n",
      "score is 0.34690696001052856, class id is 1, class name is T1\n",
      "NGynsT_Annotation2\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\OyS7Ax_Annotation0.jpg\n",
      "score is 0.4455471634864807, class id is 0, class name is T0\n",
      "OyS7Ax_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\OyS7Ax_Annotation1.jpg\n",
      "score is 0.5565802454948425, class id is 0, class name is T0\n",
      "OyS7Ax_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\OyS7Ax_Annotation2.jpg\n",
      "score is 0.47504663467407227, class id is 2, class name is T2\n",
      "OyS7Ax_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\OyS7Ax_Annotation3.jpg\n",
      "score is 0.6258467435836792, class id is 2, class name is T2\n",
      "OyS7Ax_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Pdaqkr_Annotation0.jpg\n",
      "score is 0.35505208373069763, class id is 0, class name is T0\n",
      "Pdaqkr_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Pdaqkr_Annotation1.jpg\n",
      "score is 0.4031849205493927, class id is 0, class name is T0\n",
      "Pdaqkr_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Pdaqkr_Annotation2.jpg\n",
      "score is 0.34828653931617737, class id is 0, class name is T0\n",
      "Pdaqkr_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Pdaqkr_Annotation3.jpg\n",
      "score is 0.4057838022708893, class id is 1, class name is T1\n",
      "Pdaqkr_Annotation3\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\PDmefU_Annotation0.jpg\n",
      "score is 0.4157501757144928, class id is 1, class name is T1\n",
      "PDmefU_Annotation0\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\PDmefU_Annotation1.jpg\n",
      "score is 0.6707400679588318, class id is 0, class name is T0\n",
      "PDmefU_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\PhoFHq_Annotation0.jpg\n",
      "score is 0.5711960196495056, class id is 2, class name is T2\n",
      "PhoFHq_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\PhoFHq_Annotation1.jpg\n",
      "score is 0.43898481130599976, class id is 2, class name is T2\n",
      "PhoFHq_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\PhoFHq_Annotation2.jpg\n",
      "score is 0.3666101396083832, class id is 2, class name is T2\n",
      "PhoFHq_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\s7iuvj_Annotation0.jpg\n",
      "score is 0.6766595244407654, class id is 2, class name is T2\n",
      "s7iuvj_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\s7iuvj_Annotation1.jpg\n",
      "score is 0.362160861492157, class id is 0, class name is T0\n",
      "s7iuvj_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\s7iuvj_Annotation2.jpg\n",
      "score is 0.4046148359775543, class id is 2, class name is T2\n",
      "s7iuvj_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\s7iuvj_Annotation3.jpg\n",
      "score is 0.40620777010917664, class id is 1, class name is T1\n",
      "s7iuvj_Annotation3\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\sPBTxE_Annotation0.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "sPBTxE_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\sPBTxE_Annotation1.jpg\n",
      "score is 0.2968882918357849, class id is 0, class name is T0\n",
      "sPBTxE_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\tePz9r_Annotation0.jpg\n",
      "score is 0.9069504737854004, class id is 2, class name is T2\n",
      "tePz9r_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\trx9ej_Annotation0.jpg\n",
      "score is 0.3569689691066742, class id is 2, class name is T2\n",
      "trx9ej_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\trx9ej_Annotation1.jpg\n",
      "score is 0.38232702016830444, class id is 0, class name is T0\n",
      "trx9ej_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\trx9ej_Annotation2.jpg\n",
      "score is 0.5371609926223755, class id is 2, class name is T2\n",
      "trx9ej_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\tZl5k7_Annotation0.jpg\n",
      "score is 0.5334591865539551, class id is 2, class name is T2\n",
      "tZl5k7_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\tZl5k7_Annotation1.jpg\n",
      "score is 0.34855490922927856, class id is 0, class name is T0\n",
      "tZl5k7_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\tZl5k7_Annotation2.jpg\n",
      "score is 0.32436057925224304, class id is 0, class name is T0\n",
      "tZl5k7_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uBGVp5_Annotation0.jpg\n",
      "score is 0.6483032703399658, class id is 2, class name is T2\n",
      "uBGVp5_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation0.jpg\n",
      "score is 0.32367298007011414, class id is 0, class name is T0\n",
      "uj56hr_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation1.jpg\n",
      "score is 0.45474591851234436, class id is 0, class name is T0\n",
      "uj56hr_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation2.jpg\n",
      "score is 0.4942343533039093, class id is 0, class name is T0\n",
      "uj56hr_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation3.jpg\n",
      "score is 0.4039514660835266, class id is 2, class name is T2\n",
      "uj56hr_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation4.jpg\n",
      "score is 0.3957376778125763, class id is 2, class name is T2\n",
      "uj56hr_Annotation4\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation5.jpg\n",
      "score is 0.36552613973617554, class id is 2, class name is T2\n",
      "uj56hr_Annotation5\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation6.jpg\n",
      "score is 0.32491737604141235, class id is 2, class name is T2\n",
      "uj56hr_Annotation6\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\uj56hr_Annotation7.jpg\n",
      "score is 0.44231978058815, class id is 1, class name is T1\n",
      "uj56hr_Annotation7\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vLZGD6_Annotation0.jpg\n",
      "score is 0.49343666434288025, class id is 2, class name is T2\n",
      "vLZGD6_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vLZGD6_Annotation1.jpg\n",
      "score is 0.40625903010368347, class id is 2, class name is T2\n",
      "vLZGD6_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vLZGD6_Annotation2.jpg\n",
      "score is 0.7198614478111267, class id is 2, class name is T2\n",
      "vLZGD6_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vNxSRB_Annotation0.jpg\n",
      "score is 0.40606772899627686, class id is 0, class name is T0\n",
      "vNxSRB_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vNxSRB_Annotation1.jpg\n",
      "score is 0.5647266507148743, class id is 2, class name is T2\n",
      "vNxSRB_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vNxSRB_Annotation2.jpg\n",
      "score is 0.36883044242858887, class id is 2, class name is T2\n",
      "vNxSRB_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vPE2HD_Annotation0.jpg\n",
      "score is 0.379301518201828, class id is 2, class name is T2\n",
      "vPE2HD_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vPE2HD_Annotation1.jpg\n",
      "score is 0.3819192945957184, class id is 0, class name is T0\n",
      "vPE2HD_Annotation1\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vPE2HD_Annotation2.jpg\n",
      "score is 0.4928768277168274, class id is 2, class name is T2\n",
      "vPE2HD_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\vPE2HD_Annotation3.jpg\n",
      "score is 0.7138398289680481, class id is 0, class name is T0\n",
      "vPE2HD_Annotation3\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\w2Qbz8_Annotation0.jpg\n",
      "score is 0.6227433681488037, class id is 2, class name is T2\n",
      "w2Qbz8_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\w2Qbz8_Annotation1.jpg\n",
      "score is 0.5305296182632446, class id is 2, class name is T2\n",
      "w2Qbz8_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\w2Qbz8_Annotation2.jpg\n",
      "score is 0.6108033657073975, class id is 2, class name is T2\n",
      "w2Qbz8_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\w2Qbz8_Annotation3.jpg\n",
      "score is 0.39995431900024414, class id is 1, class name is T1\n",
      "w2Qbz8_Annotation3\n",
      "1\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Wp3Lv9_Annotation0.jpg\n",
      "score is 0.34955376386642456, class id is 0, class name is T0\n",
      "Wp3Lv9_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Wp3Lv9_Annotation1.jpg\n",
      "score is 0.825441837310791, class id is 2, class name is T2\n",
      "Wp3Lv9_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Wp3Lv9_Annotation2.jpg\n",
      "score is 0.3351043462753296, class id is 0, class name is T0\n",
      "Wp3Lv9_Annotation2\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation0.jpg\n",
      "score is 0.33512207865715027, class id is 0, class name is T0\n",
      "wuCWgz_Annotation0\n",
      "0\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation1.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "wuCWgz_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation2.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "wuCWgz_Annotation2\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation3.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "wuCWgz_Annotation3\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation4.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "wuCWgz_Annotation4\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation5.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "wuCWgz_Annotation5\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\wuCWgz_Annotation6.jpg\n",
      "score is 0.2612455189228058, class id is 2, class name is T2\n",
      "wuCWgz_Annotation6\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\Y1gva6_Annotation0.jpg\n",
      "score is 0.8410396575927734, class id is 2, class name is T2\n",
      "Y1gva6_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\yuBHAm_Annotation0.jpg\n",
      "score is 0.46424540877342224, class id is 2, class name is T2\n",
      "yuBHAm_Annotation0\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\yuBHAm_Annotation1.jpg\n",
      "score is 0.7021903395652771, class id is 2, class name is T2\n",
      "yuBHAm_Annotation1\n",
      "2\n",
      "\n",
      "\n",
      "infer mode...\n",
      "number of trainable params (M): 11.18\n",
      "Resume checkpoint ./resnet_output_dir/checkpoint-13.pth\n",
      "With optim & sched!\n",
      "image path is ./Test\\zIGPHq_Annotation0.jpg\n",
      "score is 0.43183520436286926, class id is 1, class name is T1\n",
      "zIGPHq_Annotation0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%run resnet_baseline/train.py --mode infer --resume ./resnet_output_dir/checkpoint-13.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb98c6a-6108-48ab-897d-d9a2b4043cfc",
   "metadata": {},
   "source": [
    "# 重排 result 顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5739425c-8700-4b66-8579-43035c8a1205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Jw1GFk_Annotation0', '0'], ['Jw1GFk_Annotation1', '2'], ['Jw1GFk_Annotation2', '2'], ['kOUcud_Annotation0', '2'], ['kOUcud_Annotation1', '2'], ['kOUcud_Annotation2', '2'], ['kOUcud_Annotation3', '2'], ['4mF7tL_Annotation0', '2'], ['4mF7tL_Annotation1', '2'], ['4mF7tL_Annotation2', '2'], ['4mF7tL_Annotation3', '2'], ['lc3OZp_Annotation0', '0'], ['lc3OZp_Annotation1', '0'], ['lg0CwS_Annotation0', '2'], ['lg0CwS_Annotation1', '0'], ['lg0CwS_Annotation2', '0'], ['lg0CwS_Annotation3', '2'], ['wuCWgz_Annotation0', '0'], ['wuCWgz_Annotation1', '2'], ['wuCWgz_Annotation2', '2'], ['wuCWgz_Annotation3', '2'], ['wuCWgz_Annotation4', '2'], ['wuCWgz_Annotation5', '2'], ['wuCWgz_Annotation6', '2'], ['kQPIhs_Annotation0', '2'], ['kQPIhs_Annotation1', '2'], ['kQPIhs_Annotation2', '2'], ['kQPIhs_Annotation3', '2'], ['KRuNrD_Annotation0', '2'], ['KRuNrD_Annotation1', '2'], ['KRuNrD_Annotation2', '0'], ['KRuNrD_Annotation3', '2'], ['gfozad_Annotation0', '2'], ['i0v7lq_Annotation0', '2'], ['i0v7lq_Annotation1', '0'], ['K2f9Nb_Annotation0', '2'], ['K2f9Nb_Annotation1', '2'], ['zIGPHq_Annotation0', '1'], ['yuBHAm_Annotation0', '2'], ['yuBHAm_Annotation1', '2'], ['lO9MmB_Annotation0', '2'], ['lO9MmB_Annotation1', '2'], ['lO9MmB_Annotation2', '0'], ['LRJwmg_Annotation0', '2'], ['LRJwmg_Annotation1', '0'], ['LRJwmg_Annotation2', '2'], ['LrKXU4_Annotation0', '2'], ['LrKXU4_Annotation1', '2'], ['LrKXU4_Annotation2', '2'], ['LrKXU4_Annotation3', '0'], ['MHNQ4e_Annotation0', '0'], ['MHNQ4e_Annotation1', '2'], ['miuArM_Annotation0', '0'], ['miuArM_Annotation1', '2'], ['miuArM_Annotation2', '2'], ['N4HDTG_Annotation0', '2'], ['N4HDTG_Annotation1', '0'], ['N4HDTG_Annotation2', '2'], ['NGynsT_Annotation0', '2'], ['NGynsT_Annotation1', '2'], ['NGynsT_Annotation2', '1'], ['Y1gva6_Annotation0', '2'], ['1y9UDX_Annotation0', '0'], ['Wp3Lv9_Annotation0', '0'], ['Wp3Lv9_Annotation1', '2'], ['Wp3Lv9_Annotation2', '0'], ['uj56hr_Annotation0', '0'], ['uj56hr_Annotation1', '0'], ['uj56hr_Annotation2', '0'], ['uj56hr_Annotation3', '2'], ['uj56hr_Annotation4', '2'], ['uj56hr_Annotation5', '2'], ['uj56hr_Annotation6', '2'], ['uj56hr_Annotation7', '1'], ['OyS7Ax_Annotation0', '0'], ['OyS7Ax_Annotation1', '0'], ['OyS7Ax_Annotation2', '2'], ['OyS7Ax_Annotation3', '2'], ['Pdaqkr_Annotation0', '0'], ['Pdaqkr_Annotation1', '0'], ['Pdaqkr_Annotation2', '0'], ['Pdaqkr_Annotation3', '1'], ['B5MAwD_Annotation0', '0'], ['B5MAwD_Annotation1', '0'], ['B5MAwD_Annotation2', '0'], ['B5MAwD_Annotation3', '2'], ['vPE2HD_Annotation0', '2'], ['vPE2HD_Annotation1', '0'], ['vPE2HD_Annotation2', '2'], ['vPE2HD_Annotation3', '0'], ['w2Qbz8_Annotation0', '2'], ['w2Qbz8_Annotation1', '2'], ['w2Qbz8_Annotation2', '2'], ['w2Qbz8_Annotation3', '1'], ['cRnBLx_Annotation0', '2'], ['cRnBLx_Annotation1', '2'], ['cRnBLx_Annotation2', '2'], ['cRnBLx_Annotation3', '1'], ['PDmefU_Annotation0', '1'], ['PDmefU_Annotation1', '0'], ['PhoFHq_Annotation0', '2'], ['PhoFHq_Annotation1', '2'], ['PhoFHq_Annotation2', '2'], ['iQUs4h_Annotation0', '2'], ['iQUs4h_Annotation1', '2'], ['s7iuvj_Annotation0', '2'], ['s7iuvj_Annotation1', '0'], ['s7iuvj_Annotation2', '2'], ['s7iuvj_Annotation3', '1'], ['trx9ej_Annotation0', '2'], ['trx9ej_Annotation1', '0'], ['trx9ej_Annotation2', '2'], ['m14lJk_Annotation0', '2'], ['sPBTxE_Annotation0', '2'], ['sPBTxE_Annotation1', '0'], ['tZl5k7_Annotation0', '2'], ['tZl5k7_Annotation1', '0'], ['tZl5k7_Annotation2', '0'], ['vLZGD6_Annotation0', '2'], ['vLZGD6_Annotation1', '2'], ['vLZGD6_Annotation2', '2'], ['vNxSRB_Annotation0', '0'], ['vNxSRB_Annotation1', '2'], ['vNxSRB_Annotation2', '2'], ['tePz9r_Annotation0', '2'], ['uBGVp5_Annotation0', '2']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "example_path = 'E:/3rdSEED/test/test/提交示例.csv'\n",
    "result_path = 'D:/AI/3rdSEED/SEED2022_gastric_cancer_classification/result.csv'\n",
    "\n",
    "result = {}\n",
    "with open(result_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        result[row[0]] = row[1]\n",
    "\n",
    "new_result = []\n",
    "with open(example_path, 'r') as f1:\n",
    "    reader = csv.reader(f1)\n",
    "    for row in reader:\n",
    "        line = []\n",
    "        line.append(row[0])\n",
    "        line.append(result[row[0]])\n",
    "        new_result.append(line)\n",
    "\n",
    "print(new_result)\n",
    "\n",
    "f = open('result1.csv','w',newline='')\n",
    "with f:\n",
    "    w = csv.writer(f,dialect=\"excel\") \n",
    "    for line in new_result:\n",
    "        w.writerow(line) #按行写入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0a752-ed3d-4613-bf68-b44ed32738b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
